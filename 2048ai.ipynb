{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make env parent class\n",
    "class env2048:  \n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Firefox()\n",
    "        self.driver.implicitly_wait(3)\n",
    "        self.driver.get('https://play2048.co')\n",
    "        self.action_map = {0:'up', 1:'right', 2:'down', 3:'left'}\n",
    "        \n",
    "        # setup score memory for finding rewards\n",
    "        self.score = 0\n",
    "    \n",
    "    # for this project, input will never be of magnitude 0 so I don't do a divide by 0 check\n",
    "    def normalize(self, data):\n",
    "        norm = np.linalg.norm(data)\n",
    "        return data/norm\n",
    "    \n",
    "    def get_state(self, normalized = True):\n",
    "        successful = False\n",
    "        while not successful:\n",
    "            successful = True\n",
    "            tiles = self.driver.find_element_by_class_name('tile-container')\n",
    "            values = [0]*16\n",
    "            for tile in tiles.find_elements_by_tag_name('div'):\n",
    "                className = tile.get_attribute('class')\n",
    "                # lazy method of avoiding the tile-inner class\n",
    "                if len(className) > 20:\n",
    "                    # must do this to isolate the string w/ pos information\n",
    "                    tile_pos = className.split(' ')[2]\n",
    "                    # use indexes since size of string won't change so this is fast\n",
    "                    x = int(tile_pos[14]) - 1\n",
    "                    y = int(tile_pos[16]) - 1\n",
    "                    index = 4*y + x\n",
    "                    if tile.text != '':\n",
    "                        values[index] = int(tile.text)\n",
    "                    else:\n",
    "                        # if here that means page hadn't fully updated\n",
    "                        successful = False\n",
    "        if normalized:\n",
    "            return self.normalize(np.array([values]))\n",
    "        else:\n",
    "            return np.array([values])\n",
    "    \n",
    "    # while there is a score-addition tag that could make this more efficient\n",
    "    # there were some issues where at times the text wouldn't be read properly\n",
    "    # therefore I just manually calculate score addition for more consistent data\n",
    "    def get_reward(self):\n",
    "        useHighestBlock = False\n",
    "        self.board_sum = True\n",
    "        if useHighestBlock:\n",
    "            return np.max(self.get_state(normalized = False))\n",
    "        elif self.board_sum:\n",
    "            return np.sum(self.get_state(normalized = False))\n",
    "        else:\n",
    "            r = self.driver.find_element_by_class_name('score-container')\n",
    "            # do the split in case the score addition text is in the element\n",
    "            score = int(r.text.split('\\n')[0])\n",
    "            r = score - self.score\n",
    "            self.score = score\n",
    "            return r\n",
    "    \n",
    "    def get_done(self):\n",
    "        status = self.driver.find_element_by_class_name('game-message')\n",
    "        return status.text != \"\"\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        body = self.driver.find_element_by_tag_name('body')\n",
    "        if action == 0:\n",
    "            body.send_keys(u'\\ue013')\n",
    "        elif action == 1:\n",
    "            body.send_keys(u'\\ue014')\n",
    "        elif action == 2:\n",
    "            body.send_keys(u'\\ue015')\n",
    "        elif action == 3:\n",
    "            body.send_keys(u'\\ue012')\n",
    "        \n",
    "        time.sleep(.1)\n",
    "        \n",
    "        return self.get_state(), self.get_reward(), self.get_done()\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DqnAgent(tf.keras.Model):\n",
    "    def __init__(self, input_space_n, action_space_n):\n",
    "        # setup model\n",
    "        super(DqnAgent, self).__init__()\n",
    "        self.INPUT_N = input_space_n\n",
    "        self.ACTION_N = action_space_n\n",
    "        self.h1 = tf.keras.layers.Dense(64, input_shape = (input_space_n,))\n",
    "        self.prediction = tf.keras.layers.Dense(self.ACTION_N,  activation = tf.keras.activations.linear)\n",
    "        \n",
    "        # setup loss function\n",
    "        self.loss = tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=.01)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # forward propogation\n",
    "        x = self.h1(x)\n",
    "        return self.prediction(x)\n",
    "    \n",
    "    \n",
    "# experiences is numpy list s.t.\n",
    "# 0 = state\n",
    "# 1 = action\n",
    "# 2 = next_state\n",
    "# 3 = reward\n",
    "# 4 = done\n",
    "\n",
    "\n",
    "def train_step(model, experiences, repeat = 3, debug = False):\n",
    "    DISCOUNT = .9\n",
    "    if debug:\n",
    "        print('experiences shape: ', experiences.shape)\n",
    "    # get shapes to right shape\n",
    "    states = np.array([e[0].reshape(16,) for e in experiences])\n",
    "    \n",
    "    # get q_vals/rewards setup and correct shape\n",
    "    rewards = np.zeros((len(states) , 4))\n",
    "    for i in range(states.shape[0]):\n",
    "        e = experiences[i]\n",
    "        if e[3] > 0:\n",
    "            rewards[i,e[1]] = e[3]\n",
    "        else:\n",
    "            rewards[i,e[1]] = 0\n",
    "        if not e[4] == True:\n",
    "            rewards[i] += DISCOUNT * model(np.array([states[i]]).reshape(-1, 16))\n",
    "    \n",
    "    # normalize the rewards\n",
    "    rewards = np.array(rewards)\n",
    "    rewards = (rewards - np.mean(rewards, axis=0))/np.std(rewards, axis=0)\n",
    "    print('mean of rewards, ', np.mean(rewards))\n",
    "    \n",
    "    # get sample\n",
    "    batch_ratio = .1\n",
    "    for r in range(repeat):\n",
    "        if r % 10 == 0:\n",
    "            print('iteration: ', r)\n",
    "        batch_size = int(experiences.shape[0]*batch_ratio)\n",
    "        select = np.random.randint(experiences.shape[0], size = batch_size)\n",
    "        trainX = np.array(states[select]).reshape(-1, 16,)\n",
    "        trainY = np.array(rewards[select]).reshape(-1, 4,)\n",
    "        if debug:\n",
    "            print('TrainX shape: ', trainX.shape)\n",
    "            print('TrainY mean', np.mean(trainY))\n",
    "            print('TrainY shape: ', trainY.shape)\n",
    "            print(select[:10])\n",
    "        with tf.GradientTape() as tape:\n",
    "            predict = model(trainX)\n",
    "            loss = model.loss(trainY, predict)\n",
    "            if r % 10 == 0:\n",
    "                print('Iteration', r, ', the loss is currently', loss.numpy())\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "model = DqnAgent(16, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_greedy = False\n",
    "def get_action(state):\n",
    "    q_vals = model(state)[0]\n",
    "    exp = np.exp(q_vals)\n",
    "    softmax_vals = exp/np.sum(exp)\n",
    "    #print(softmax_vals)\n",
    "    EPSILON = .15\n",
    "    if e_greedy:\n",
    "        choice = np.random.choice([0,1], p=[EPSILON, 1- EPSILON])\n",
    "        if choice == 1:\n",
    "            try:\n",
    "                return np.argmax(q_vals, axis = 0)\n",
    "            except:\n",
    "                print(softmax_vals)\n",
    "                print(q_vals)\n",
    "        else:\n",
    "            return np.random.choice(4, p = [.25]*4)\n",
    "    else:\n",
    "        return np.random.choice(4, p= softmax_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of rewards,  2.5386270159162802e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0663928985595703\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0371688604354858\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.032721757888794\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0566681623458862\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0347036123275757\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.035218358039856\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0203419923782349\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0382144451141357\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0065908432006836\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.017730951309204\n",
      "4 th training episode\n",
      "mean of rewards,  -4.458467550436599e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0259482860565186\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0323914289474487\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.027206301689148\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0251988172531128\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0334606170654297\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.033983826637268\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0214450359344482\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0082652568817139\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0150136947631836\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0259970426559448\n",
      "9 th training episode\n",
      "mean of rewards,  -4.727689846300465e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.054793357849121\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0164856910705566\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0147969722747803\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.01059091091156\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.019256830215454\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0039571523666382\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0380738973617554\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.00754976272583\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0462104082107544\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9951725006103516\n",
      "14 th training episode\n",
      "mean of rewards,  -2.9456915476023362e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0108250379562378\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0325288772583008\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.023914098739624\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0208312273025513\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.012211561203003\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.021224856376648\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0106812715530396\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0060449838638306\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0136771202087402\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0061594247817993\n",
      "19 th training episode\n",
      "mean of rewards,  -9.710999025129185e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0230876207351685\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.019531488418579\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0274368524551392\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0128642320632935\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0151675939559937\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.026720404624939\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9996840953826904\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0165153741836548\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.022273063659668\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0207810401916504\n",
      "24 th training episode\n",
      "mean of rewards,  4.004636413747802e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0220838785171509\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0100761651992798\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0215327739715576\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0121241807937622\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0034213066101074\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0045595169067383\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0046460628509521\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0067802667617798\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.007662057876587\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9822975397109985\n",
      "29 th training episode\n",
      "mean of rewards,  -3.9772785056948166e-17\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.011358618736267\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0211206674575806\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.012986183166504\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0179458856582642\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9889479875564575\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0054720640182495\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0166265964508057\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0095984935760498\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9982636570930481\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0129055976867676\n",
      "34 th training episode\n",
      "mean of rewards,  -2.680084070034494e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0078027248382568\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0088051557540894\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0056912899017334\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0241851806640625\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9987160563468933\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9899395704269409\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0046629905700684\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.999681293964386\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.015425205230713\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0109206438064575\n",
      "39 th training episode\n",
      "mean of rewards,  -4.603747330484884e-17\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9984700679779053\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0110515356063843\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.002894401550293\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.005827784538269\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9975076913833618\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.004905343055725\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0013220310211182\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9932686686515808\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9925398826599121\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0211944580078125\n",
      "44 th training episode\n",
      "mean of rewards,  -8.252560713990977e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.986839234828949\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0010491609573364\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9852454662322998\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.000348448753357\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0068126916885376\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.004181146621704\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0026607513427734\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0046881437301636\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9967971444129944\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0054329633712769\n",
      "49 th training episode\n",
      "mean of rewards,  -4.565991129576635e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.003917932510376\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9896434545516968\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0144797563552856\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0126709938049316\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0016775131225586\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.997081995010376\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0107612609863281\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0000282526016235\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0170668363571167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.019686222076416\n",
      "54 th training episode\n",
      "mean of rewards,  2.056034040826524e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0004160404205322\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0028302669525146\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0056594610214233\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0061280727386475\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0041742324829102\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9975989460945129\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0265319347381592\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9889385104179382\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0115809440612793\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.02306067943573\n",
      "59 th training episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 01:08:29.523164 140228307916608 connectionpool.py:662] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f89323e6cf8>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/73c3bbb3-59df-4edc-b48c-9fd1d407c9c0/window\n",
      "W0817 01:08:29.524304 140228307916608 connectionpool.py:662] Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f893033d9b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/73c3bbb3-59df-4edc-b48c-9fd1d407c9c0/window\n",
      "W0817 01:08:29.525345 140228307916608 connectionpool.py:662] Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f89281019e8>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/73c3bbb3-59df-4edc-b48c-9fd1d407c9c0/window\n",
      "Exception ignored in: <function env2048.__del__ at 0x7f8932cbf950>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1a4beecc23dc>\", line 81, in __del__\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 688, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 319, in execute\n",
      "    response = self.command_executor.execute(driver_command, params)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 374, in execute\n",
      "    return self._request(command_info[0], url, body=data)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 397, in _request\n",
      "    resp = self._conn.request(method, url, body=body, headers=headers)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 68, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 89, in request_encode_url\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=36715): Max retries exceeded with url: /session/73c3bbb3-59df-4edc-b48c-9fd1d407c9c0/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928101f98>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "W0817 01:08:29.535582 140228307916608 connectionpool.py:662] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928528668>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c559b3f8-a6eb-4178-877f-45ab2a2c2c00/window\n",
      "W0817 01:08:29.536530 140228307916608 connectionpool.py:662] Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928526240>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c559b3f8-a6eb-4178-877f-45ab2a2c2c00/window\n",
      "W0817 01:08:29.537497 140228307916608 connectionpool.py:662] Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110588>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c559b3f8-a6eb-4178-877f-45ab2a2c2c00/window\n",
      "Exception ignored in: <function env2048.__del__ at 0x7f8932cbf950>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1a4beecc23dc>\", line 81, in __del__\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 688, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 319, in execute\n",
      "    response = self.command_executor.execute(driver_command, params)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 374, in execute\n",
      "    return self._request(command_info[0], url, body=data)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 397, in _request\n",
      "    resp = self._conn.request(method, url, body=body, headers=headers)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 68, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 89, in request_encode_url\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=41783): Max retries exceeded with url: /session/c559b3f8-a6eb-4178-877f-45ab2a2c2c00/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110630>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "W0817 01:08:29.544701 140228307916608 connectionpool.py:662] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928526080>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/956cc0c3-77df-43cb-8995-f78ba24a7e33/window\n",
      "W0817 01:08:29.545610 140228307916608 connectionpool.py:662] Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f89281106a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/956cc0c3-77df-43cb-8995-f78ba24a7e33/window\n",
      "W0817 01:08:29.546756 140228307916608 connectionpool.py:662] Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110550>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/956cc0c3-77df-43cb-8995-f78ba24a7e33/window\n",
      "Exception ignored in: <function env2048.__del__ at 0x7f8932cbf950>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-1a4beecc23dc>\", line 81, in __del__\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 688, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 319, in execute\n",
      "    response = self.command_executor.execute(driver_command, params)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 374, in execute\n",
      "    return self._request(command_info[0], url, body=data)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 397, in _request\n",
      "    resp = self._conn.request(method, url, body=body, headers=headers)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 68, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 89, in request_encode_url\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=55005): Max retries exceeded with url: /session/956cc0c3-77df-43cb-8995-f78ba24a7e33/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110898>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "W0817 01:08:29.553824 140228307916608 connectionpool.py:662] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f89281b75f8>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/20a3da34-5837-4d9c-a04a-006b6d0ef6d5/window\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 01:08:29.554677 140228307916608 connectionpool.py:662] Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110c50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/20a3da34-5837-4d9c-a04a-006b6d0ef6d5/window\n",
      "W0817 01:08:29.555813 140228307916608 connectionpool.py:662] Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110a58>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/20a3da34-5837-4d9c-a04a-006b6d0ef6d5/window\n",
      "Exception ignored in: <function env2048.__del__ at 0x7f89284aeea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-20-8a21c0e81e54>\", line 81, in __del__\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 688, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 319, in execute\n",
      "    response = self.command_executor.execute(driver_command, params)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 374, in execute\n",
      "    return self._request(command_info[0], url, body=data)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 397, in _request\n",
      "    resp = self._conn.request(method, url, body=body, headers=headers)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 68, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 89, in request_encode_url\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=60391): Max retries exceeded with url: /session/20a3da34-5837-4d9c-a04a-006b6d0ef6d5/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110be0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "W0817 01:08:29.563297 140228307916608 connectionpool.py:662] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928444e48>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e341554d-39ad-4866-a2d6-12e62964166e/window\n",
      "W0817 01:08:29.564178 140228307916608 connectionpool.py:662] Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110e48>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e341554d-39ad-4866-a2d6-12e62964166e/window\n",
      "W0817 01:08:29.565055 140228307916608 connectionpool.py:662] Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110f28>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e341554d-39ad-4866-a2d6-12e62964166e/window\n",
      "Exception ignored in: <function env2048.__del__ at 0x7f8928155598>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-24-7b9ae2eadb24>\", line 81, in __del__\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 688, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\", line 319, in execute\n",
      "    response = self.command_executor.execute(driver_command, params)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 374, in execute\n",
      "    return self._request(command_info[0], url, body=data)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/remote_connection.py\", line 397, in _request\n",
      "    resp = self._conn.request(method, url, body=body, headers=headers)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 68, in request\n",
      "    **urlopen_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/request.py\", line 89, in request_encode_url\n",
      "    return self.urlopen(method, url, **extra_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/poolmanager.py\", line 324, in urlopen\n",
      "    response = conn.urlopen(method, u.request_uri, **kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    **response_kw)\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 638, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/home/brett/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\", line 399, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=40157): Max retries exceeded with url: /session/e341554d-39ad-4866-a2d6-12e62964166e/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8928110f60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of rewards,  1.7115428196039562e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0146466493606567\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9840714931488037\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9886392951011658\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.003871202468872\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9963464140892029\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.009663701057434\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9877459406852722\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9986816644668579\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9916666746139526\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.00174081325531\n",
      "64 th training episode\n",
      "mean of rewards,  3.111017559380731e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9961923360824585\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9882507920265198\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0098743438720703\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0074427127838135\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9869065284729004\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9972942471504211\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0045791864395142\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0074681043624878\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9909301400184631\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0133823156356812\n",
      "69 th training episode\n",
      "mean of rewards,  -3.1289462530577777e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9937118887901306\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9851489663124084\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9898185729980469\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9995410442352295\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.006580114364624\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.009445071220398\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.006268858909607\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0062503814697266\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9879677891731262\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0060869455337524\n",
      "74 th training episode\n",
      "mean of rewards,  2.391118328853577e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9843475222587585\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9744328260421753\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9700916409492493\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9974009394645691\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0026220083236694\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0022752285003662\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9943455457687378\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9964884519577026\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9925802946090698\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9973695874214172\n",
      "79 th training episode\n",
      "mean of rewards,  1.3500472699354992e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0060354471206665\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9964668154716492\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9909411072731018\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.988076388835907\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0088154077529907\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.002400279045105\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9946730732917786\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9906187057495117\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9889704585075378\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9871886968612671\n",
      "84 th training episode\n",
      "mean of rewards,  -6.0541780097992645e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9977510571479797\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9991294145584106\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0155785083770752\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0011471509933472\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0089423656463623\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9927211403846741\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0150226354599\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0083972215652466\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0025043487548828\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9995461106300354\n",
      "89 th training episode\n",
      "mean of rewards,  -1.3676156027221157e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0044466257095337\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0124481916427612\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.002855658531189\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9984970092773438\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9870072603225708\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9974086880683899\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9964523911476135\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0137885808944702\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9901715517044067\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.006392478942871\n",
      "94 th training episode\n",
      "mean of rewards,  3.000715477661866e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0034390687942505\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9851559400558472\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0184166431427002\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9855110049247742\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9939683675765991\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9855251908302307\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9979265928268433\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0111095905303955\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9966586828231812\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9932098984718323\n",
      "99 th training episode\n",
      "mean of rewards,  7.120664372465812e-17\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.001930832862854\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0022227764129639\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.987740695476532\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9954774379730225\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9932639002799988\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9945808053016663\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9951623678207397\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9884323477745056\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0101929903030396\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9830099940299988\n",
      "104 th training episode\n",
      "mean of rewards,  -1.6507948523824925e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0036940574645996\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0132123231887817\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9794734120368958\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9917661547660828\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9877380728721619\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0005450248718262\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.006259799003601\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9943016767501831\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9831318259239197\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.005927562713623\n",
      "109 th training episode\n",
      "mean of rewards,  -8.703567251639431e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9939574599266052\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9963290691375732\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.0111943483352661\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.995684027671814\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 1.0120956897735596\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0017930269241333\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0107942819595337\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9818118214607239\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9930753111839294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9874922037124634\n",
      "114 th training episode\n",
      "mean of rewards,  1.6151352635817836e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9853821992874146\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0109453201293945\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9852930307388306\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9902034401893616\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.983303427696228\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0007520914077759\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.990890622138977\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9988718628883362\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.999021053314209\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9928194284439087\n",
      "119 th training episode\n",
      "mean of rewards,  -2.199882733779365e-15\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9998800754547119\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0012240409851074\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9955483675003052\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0000011920928955\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9982366561889648\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0012017488479614\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0020389556884766\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0053554773330688\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0066756010055542\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.004914402961731\n",
      "124 th training episode\n",
      "mean of rewards,  6.187436742686116e-18\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9936245083808899\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.0120460987091064\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9802658557891846\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0109375715255737\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9758831262588501\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.009343147277832\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9944679737091064\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0124932527542114\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9966720938682556\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9888896942138672\n",
      "129 th training episode\n",
      "mean of rewards,  -4.536351416502812e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0081745386123657\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 1.001372218132019\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 1.001391053199768\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9794227480888367\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9993531703948975\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0061938762664795\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0029882192611694\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.00296151638031\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 1.0180572271347046\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0103868246078491\n",
      "134 th training episode\n",
      "mean of rewards,  1.8817426447214403e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9867952466011047\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9841633439064026\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9829102754592896\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 1.0232681035995483\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.995642900466919\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9733717441558838\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 1.0062100887298584\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.0011757612228394\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9864510297775269\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0029661655426025\n",
      "139 th training episode\n",
      "mean of rewards,  5.3471983307234693e-17\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.9878069758415222\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9932250380516052\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9865569472312927\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9931380152702332\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9779115319252014\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 1.0066657066345215\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9986444711685181\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 1.018608808517456\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9912976026535034\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.0077829360961914\n",
      "144 th training episode\n",
      "mean of rewards,  -9.007316481107751e-17\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 1.0222201347351074\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9946156144142151\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9930699467658997\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9853691458702087\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.9969760179519653\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9995905756950378\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.9781001210212708\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9972850680351257\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9956283569335938\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 1.011887550354004\n",
      "149 th training episode\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: The element reference of <div class=\"tile tile-8 tile-position-3-2\"> is stale; either the element is no longer attached to the DOM, it is not in the current frame context, or the document has been refreshed\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7e8e8b79d844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mexperience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-8a21c0e81e54>\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-8a21c0e81e54>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self, normalized)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mtile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                         \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m\"\"\"The text of the element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_ELEMENT_TEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m: Message: The element reference of <div class=\"tile tile-8 tile-position-3-2\"> is stale; either the element is no longer attached to the DOM, it is not in the current frame context, or the document has been refreshed\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "env = env2048()\n",
    "num_episodes = 600\n",
    "for episode in range(num_episodes):\n",
    "    done = False\n",
    "    state = env.get_state()\n",
    "    experiences = []\n",
    "    while not done:\n",
    "        action = get_action(state)\n",
    "\n",
    "        next_state, reward, done = env.take_action(action)\n",
    "\n",
    "        experience = [state, action, next_state, reward, done]\n",
    "        experiences.append(experience)\n",
    "        state = next_state\n",
    "     \n",
    "    # cleanup experience\n",
    "    scores.append(env.score)\n",
    "    reward = env.get_reward()\n",
    "    for experience in experiences:\n",
    "        # punish uselss moves\n",
    "        if (experience[0] == experience[2]).all():\n",
    "            experience[3] = 0\n",
    "        if env.board_sum: # give all experiences in this episode a rewards that is equal to sum of board\n",
    "            experience[3] = reward\n",
    "    \n",
    "    # add experiences to memory\n",
    "    memo.extend(experiences)\n",
    "    data = np.array(memo)\n",
    "    \n",
    "    # occasionally train model on some of the collected data\n",
    "    if (1+episode) % 5 == 0:\n",
    "        train_step(model, data, repeat= 100)\n",
    "        print(episode, 'th training episode')\n",
    "    env.driver.find_element_by_class_name('restart-button').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of rewards,  -3.917610456673311e-16\n",
      "iteration:  0\n",
      "Iteration 0 , the loss is currently 0.97614586353302\n",
      "iteration:  10\n",
      "Iteration 10 , the loss is currently 0.9879736304283142\n",
      "iteration:  20\n",
      "Iteration 20 , the loss is currently 0.9930416345596313\n",
      "iteration:  30\n",
      "Iteration 30 , the loss is currently 0.9863708019256592\n",
      "iteration:  40\n",
      "Iteration 40 , the loss is currently 0.977749228477478\n",
      "iteration:  50\n",
      "Iteration 50 , the loss is currently 0.9935055375099182\n",
      "iteration:  60\n",
      "Iteration 60 , the loss is currently 0.982075572013855\n",
      "iteration:  70\n",
      "Iteration 70 , the loss is currently 0.9909610748291016\n",
      "iteration:  80\n",
      "Iteration 80 , the loss is currently 0.9928528070449829\n",
      "iteration:  90\n",
      "Iteration 90 , the loss is currently 0.9759883880615234\n",
      "iteration:  100\n",
      "Iteration 100 , the loss is currently 1.0141304731369019\n",
      "iteration:  110\n",
      "Iteration 110 , the loss is currently 0.9921926856040955\n",
      "iteration:  120\n",
      "Iteration 120 , the loss is currently 0.9871148467063904\n",
      "iteration:  130\n",
      "Iteration 130 , the loss is currently 1.0007176399230957\n",
      "iteration:  140\n",
      "Iteration 140 , the loss is currently 0.9892030954360962\n",
      "iteration:  150\n",
      "Iteration 150 , the loss is currently 0.9933419823646545\n",
      "iteration:  160\n",
      "Iteration 160 , the loss is currently 0.9954472780227661\n",
      "iteration:  170\n",
      "Iteration 170 , the loss is currently 0.9711583256721497\n",
      "iteration:  180\n",
      "Iteration 180 , the loss is currently 0.9622061252593994\n",
      "iteration:  190\n",
      "Iteration 190 , the loss is currently 0.9899682998657227\n",
      "iteration:  200\n",
      "Iteration 200 , the loss is currently 0.9956039190292358\n",
      "iteration:  210\n",
      "Iteration 210 , the loss is currently 1.0013673305511475\n",
      "iteration:  220\n",
      "Iteration 220 , the loss is currently 0.9966963529586792\n",
      "iteration:  230\n",
      "Iteration 230 , the loss is currently 0.9804617762565613\n",
      "iteration:  240\n",
      "Iteration 240 , the loss is currently 0.9887428283691406\n",
      "iteration:  250\n",
      "Iteration 250 , the loss is currently 0.9854592084884644\n",
      "iteration:  260\n",
      "Iteration 260 , the loss is currently 0.9748573899269104\n",
      "iteration:  270\n",
      "Iteration 270 , the loss is currently 0.9933447241783142\n",
      "iteration:  280\n",
      "Iteration 280 , the loss is currently 0.9913678765296936\n",
      "iteration:  290\n",
      "Iteration 290 , the loss is currently 0.9808756113052368\n",
      "iteration:  300\n",
      "Iteration 300 , the loss is currently 1.00395929813385\n",
      "iteration:  310\n",
      "Iteration 310 , the loss is currently 0.9949718117713928\n",
      "iteration:  320\n",
      "Iteration 320 , the loss is currently 0.9896737337112427\n",
      "iteration:  330\n",
      "Iteration 330 , the loss is currently 0.9984208941459656\n",
      "iteration:  340\n",
      "Iteration 340 , the loss is currently 0.9810375571250916\n",
      "iteration:  350\n",
      "Iteration 350 , the loss is currently 0.9921060800552368\n",
      "iteration:  360\n",
      "Iteration 360 , the loss is currently 1.0163072347640991\n",
      "iteration:  370\n",
      "Iteration 370 , the loss is currently 0.9762392044067383\n",
      "iteration:  380\n",
      "Iteration 380 , the loss is currently 0.9878779649734497\n",
      "iteration:  390\n",
      "Iteration 390 , the loss is currently 0.9950190186500549\n",
      "iteration:  400\n",
      "Iteration 400 , the loss is currently 0.9773625135421753\n",
      "iteration:  410\n",
      "Iteration 410 , the loss is currently 0.9718523025512695\n",
      "iteration:  420\n",
      "Iteration 420 , the loss is currently 0.9964239597320557\n",
      "iteration:  430\n",
      "Iteration 430 , the loss is currently 0.9923985600471497\n",
      "iteration:  440\n",
      "Iteration 440 , the loss is currently 0.9878582954406738\n",
      "iteration:  450\n",
      "Iteration 450 , the loss is currently 0.982814610004425\n",
      "iteration:  460\n",
      "Iteration 460 , the loss is currently 1.0176807641983032\n",
      "iteration:  470\n",
      "Iteration 470 , the loss is currently 0.9984596967697144\n",
      "iteration:  480\n",
      "Iteration 480 , the loss is currently 1.0073249340057373\n",
      "iteration:  490\n",
      "Iteration 490 , the loss is currently 0.9848600625991821\n",
      "iteration:  500\n",
      "Iteration 500 , the loss is currently 1.000876545906067\n",
      "iteration:  510\n",
      "Iteration 510 , the loss is currently 0.9824554324150085\n",
      "iteration:  520\n",
      "Iteration 520 , the loss is currently 0.988724410533905\n",
      "iteration:  530\n",
      "Iteration 530 , the loss is currently 0.9861695766448975\n",
      "iteration:  540\n",
      "Iteration 540 , the loss is currently 0.9976246953010559\n",
      "iteration:  550\n",
      "Iteration 550 , the loss is currently 0.9827386736869812\n",
      "iteration:  560\n",
      "Iteration 560 , the loss is currently 0.9970842003822327\n",
      "iteration:  570\n",
      "Iteration 570 , the loss is currently 0.9806913733482361\n",
      "iteration:  580\n",
      "Iteration 580 , the loss is currently 1.0057568550109863\n",
      "iteration:  590\n",
      "Iteration 590 , the loss is currently 0.9802957773208618\n",
      "iteration:  600\n",
      "Iteration 600 , the loss is currently 0.9991446733474731\n",
      "iteration:  610\n",
      "Iteration 610 , the loss is currently 1.0063726902008057\n",
      "iteration:  620\n",
      "Iteration 620 , the loss is currently 1.0070096254348755\n",
      "iteration:  630\n",
      "Iteration 630 , the loss is currently 0.9885541796684265\n",
      "iteration:  640\n",
      "Iteration 640 , the loss is currently 1.0015405416488647\n",
      "iteration:  650\n",
      "Iteration 650 , the loss is currently 1.001504898071289\n",
      "iteration:  660\n",
      "Iteration 660 , the loss is currently 0.9884846806526184\n",
      "iteration:  670\n",
      "Iteration 670 , the loss is currently 1.0025768280029297\n",
      "iteration:  680\n",
      "Iteration 680 , the loss is currently 0.9820131063461304\n",
      "iteration:  690\n",
      "Iteration 690 , the loss is currently 0.9858023524284363\n",
      "iteration:  700\n",
      "Iteration 700 , the loss is currently 1.0005160570144653\n",
      "iteration:  710\n",
      "Iteration 710 , the loss is currently 1.0036983489990234\n",
      "iteration:  720\n",
      "Iteration 720 , the loss is currently 1.0027130842208862\n",
      "iteration:  730\n",
      "Iteration 730 , the loss is currently 1.0140641927719116\n",
      "iteration:  740\n",
      "Iteration 740 , the loss is currently 0.9849429726600647\n",
      "iteration:  750\n",
      "Iteration 750 , the loss is currently 0.9743492007255554\n",
      "iteration:  760\n",
      "Iteration 760 , the loss is currently 1.0052955150604248\n",
      "iteration:  770\n",
      "Iteration 770 , the loss is currently 0.9708754420280457\n",
      "iteration:  780\n",
      "Iteration 780 , the loss is currently 0.9934198260307312\n",
      "iteration:  790\n",
      "Iteration 790 , the loss is currently 1.0180175304412842\n",
      "iteration:  800\n",
      "Iteration 800 , the loss is currently 1.0001680850982666\n",
      "iteration:  810\n",
      "Iteration 810 , the loss is currently 0.9901358485221863\n",
      "iteration:  820\n",
      "Iteration 820 , the loss is currently 1.0128822326660156\n",
      "iteration:  830\n",
      "Iteration 830 , the loss is currently 0.9858521819114685\n",
      "iteration:  840\n",
      "Iteration 840 , the loss is currently 1.019623875617981\n",
      "iteration:  850\n",
      "Iteration 850 , the loss is currently 0.9721323847770691\n",
      "iteration:  860\n",
      "Iteration 860 , the loss is currently 1.0108389854431152\n",
      "iteration:  870\n",
      "Iteration 870 , the loss is currently 1.0075438022613525\n",
      "iteration:  880\n",
      "Iteration 880 , the loss is currently 0.9826252460479736\n",
      "iteration:  890\n",
      "Iteration 890 , the loss is currently 0.9851117730140686\n",
      "iteration:  900\n",
      "Iteration 900 , the loss is currently 0.9830368161201477\n",
      "iteration:  910\n",
      "Iteration 910 , the loss is currently 1.0050216913223267\n",
      "iteration:  920\n",
      "Iteration 920 , the loss is currently 1.0018831491470337\n",
      "iteration:  930\n",
      "Iteration 930 , the loss is currently 1.0000554323196411\n",
      "iteration:  940\n",
      "Iteration 940 , the loss is currently 1.0165276527404785\n",
      "iteration:  950\n",
      "Iteration 950 , the loss is currently 0.9851614832878113\n",
      "iteration:  960\n",
      "Iteration 960 , the loss is currently 1.0021283626556396\n",
      "iteration:  970\n",
      "Iteration 970 , the loss is currently 0.9955588579177856\n",
      "iteration:  980\n",
      "Iteration 980 , the loss is currently 0.9819040298461914\n",
      "iteration:  990\n",
      "Iteration 990 , the loss is currently 0.9877730011940002\n",
      "iteration:  1000\n",
      "Iteration 1000 , the loss is currently 0.9940928220748901\n",
      "iteration:  1010\n",
      "Iteration 1010 , the loss is currently 1.0056350231170654\n",
      "iteration:  1020\n",
      "Iteration 1020 , the loss is currently 0.9859985709190369\n",
      "iteration:  1030\n",
      "Iteration 1030 , the loss is currently 0.9879440665245056\n",
      "iteration:  1040\n",
      "Iteration 1040 , the loss is currently 1.0012916326522827\n",
      "iteration:  1050\n",
      "Iteration 1050 , the loss is currently 0.9985429644584656\n",
      "iteration:  1060\n",
      "Iteration 1060 , the loss is currently 0.9930421113967896\n",
      "iteration:  1070\n",
      "Iteration 1070 , the loss is currently 0.9897980093955994\n",
      "iteration:  1080\n",
      "Iteration 1080 , the loss is currently 1.00224769115448\n",
      "iteration:  1090\n",
      "Iteration 1090 , the loss is currently 0.9809178113937378\n",
      "iteration:  1100\n",
      "Iteration 1100 , the loss is currently 0.9887685775756836\n",
      "iteration:  1110\n",
      "Iteration 1110 , the loss is currently 0.9815531969070435\n",
      "iteration:  1120\n",
      "Iteration 1120 , the loss is currently 0.9960198998451233\n",
      "iteration:  1130\n",
      "Iteration 1130 , the loss is currently 0.9943723678588867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  1140\n",
      "Iteration 1140 , the loss is currently 0.9995238780975342\n",
      "iteration:  1150\n",
      "Iteration 1150 , the loss is currently 1.0040082931518555\n",
      "iteration:  1160\n",
      "Iteration 1160 , the loss is currently 0.9946731328964233\n",
      "iteration:  1170\n",
      "Iteration 1170 , the loss is currently 0.9765210747718811\n",
      "iteration:  1180\n",
      "Iteration 1180 , the loss is currently 0.9822522401809692\n",
      "iteration:  1190\n",
      "Iteration 1190 , the loss is currently 0.9834169745445251\n",
      "iteration:  1200\n",
      "Iteration 1200 , the loss is currently 1.0011552572250366\n",
      "iteration:  1210\n",
      "Iteration 1210 , the loss is currently 0.9737172722816467\n",
      "iteration:  1220\n",
      "Iteration 1220 , the loss is currently 0.9755123853683472\n",
      "iteration:  1230\n",
      "Iteration 1230 , the loss is currently 0.98545902967453\n",
      "iteration:  1240\n",
      "Iteration 1240 , the loss is currently 0.9850686192512512\n",
      "iteration:  1250\n",
      "Iteration 1250 , the loss is currently 0.9848220944404602\n",
      "iteration:  1260\n",
      "Iteration 1260 , the loss is currently 1.0045324563980103\n",
      "iteration:  1270\n",
      "Iteration 1270 , the loss is currently 0.9849545359611511\n",
      "iteration:  1280\n",
      "Iteration 1280 , the loss is currently 0.9922154545783997\n",
      "iteration:  1290\n",
      "Iteration 1290 , the loss is currently 0.9970713257789612\n",
      "iteration:  1300\n",
      "Iteration 1300 , the loss is currently 0.9831501841545105\n",
      "iteration:  1310\n",
      "Iteration 1310 , the loss is currently 0.9833009243011475\n",
      "iteration:  1320\n",
      "Iteration 1320 , the loss is currently 0.9892104268074036\n",
      "iteration:  1330\n",
      "Iteration 1330 , the loss is currently 0.9712284207344055\n",
      "iteration:  1340\n",
      "Iteration 1340 , the loss is currently 1.0050708055496216\n",
      "iteration:  1350\n",
      "Iteration 1350 , the loss is currently 0.9828988909721375\n",
      "iteration:  1360\n",
      "Iteration 1360 , the loss is currently 0.9671754837036133\n",
      "iteration:  1370\n",
      "Iteration 1370 , the loss is currently 0.9828223586082458\n",
      "iteration:  1380\n",
      "Iteration 1380 , the loss is currently 0.9841288328170776\n",
      "iteration:  1390\n",
      "Iteration 1390 , the loss is currently 0.9922592639923096\n",
      "iteration:  1400\n",
      "Iteration 1400 , the loss is currently 0.983083188533783\n",
      "iteration:  1410\n",
      "Iteration 1410 , the loss is currently 0.9716134667396545\n",
      "iteration:  1420\n",
      "Iteration 1420 , the loss is currently 0.9956282377243042\n",
      "iteration:  1430\n",
      "Iteration 1430 , the loss is currently 0.9800804257392883\n",
      "iteration:  1440\n",
      "Iteration 1440 , the loss is currently 0.9977006912231445\n",
      "iteration:  1450\n",
      "Iteration 1450 , the loss is currently 1.010718822479248\n",
      "iteration:  1460\n",
      "Iteration 1460 , the loss is currently 0.9676151275634766\n",
      "iteration:  1470\n",
      "Iteration 1470 , the loss is currently 0.9978904128074646\n",
      "iteration:  1480\n",
      "Iteration 1480 , the loss is currently 0.9850202798843384\n",
      "iteration:  1490\n",
      "Iteration 1490 , the loss is currently 0.9770304560661316\n",
      "iteration:  1500\n",
      "Iteration 1500 , the loss is currently 0.9659934043884277\n",
      "iteration:  1510\n",
      "Iteration 1510 , the loss is currently 0.984081506729126\n",
      "iteration:  1520\n",
      "Iteration 1520 , the loss is currently 0.9822149276733398\n",
      "iteration:  1530\n",
      "Iteration 1530 , the loss is currently 0.9916049838066101\n",
      "iteration:  1540\n",
      "Iteration 1540 , the loss is currently 0.9751018285751343\n",
      "iteration:  1550\n",
      "Iteration 1550 , the loss is currently 1.005334496498108\n",
      "iteration:  1560\n",
      "Iteration 1560 , the loss is currently 0.9731327295303345\n",
      "iteration:  1570\n",
      "Iteration 1570 , the loss is currently 0.9644732475280762\n",
      "iteration:  1580\n",
      "Iteration 1580 , the loss is currently 0.9801774621009827\n",
      "iteration:  1590\n",
      "Iteration 1590 , the loss is currently 1.0028239488601685\n",
      "iteration:  1600\n",
      "Iteration 1600 , the loss is currently 0.9849787354469299\n",
      "iteration:  1610\n",
      "Iteration 1610 , the loss is currently 0.9987397789955139\n",
      "iteration:  1620\n",
      "Iteration 1620 , the loss is currently 0.9928447008132935\n",
      "iteration:  1630\n",
      "Iteration 1630 , the loss is currently 0.9846946597099304\n",
      "iteration:  1640\n",
      "Iteration 1640 , the loss is currently 0.989628255367279\n",
      "iteration:  1650\n",
      "Iteration 1650 , the loss is currently 1.0030800104141235\n",
      "iteration:  1660\n",
      "Iteration 1660 , the loss is currently 0.9878500699996948\n",
      "iteration:  1670\n",
      "Iteration 1670 , the loss is currently 0.9923626780509949\n",
      "iteration:  1680\n",
      "Iteration 1680 , the loss is currently 0.982195258140564\n",
      "iteration:  1690\n",
      "Iteration 1690 , the loss is currently 0.9803218245506287\n",
      "iteration:  1700\n",
      "Iteration 1700 , the loss is currently 1.0007578134536743\n",
      "iteration:  1710\n",
      "Iteration 1710 , the loss is currently 0.985783576965332\n",
      "iteration:  1720\n",
      "Iteration 1720 , the loss is currently 0.9929882287979126\n",
      "iteration:  1730\n",
      "Iteration 1730 , the loss is currently 0.9931721687316895\n",
      "iteration:  1740\n",
      "Iteration 1740 , the loss is currently 0.9805650115013123\n",
      "iteration:  1750\n",
      "Iteration 1750 , the loss is currently 0.9974371194839478\n",
      "iteration:  1760\n",
      "Iteration 1760 , the loss is currently 0.9759514927864075\n",
      "iteration:  1770\n",
      "Iteration 1770 , the loss is currently 1.0010952949523926\n",
      "iteration:  1780\n",
      "Iteration 1780 , the loss is currently 0.976382315158844\n",
      "iteration:  1790\n",
      "Iteration 1790 , the loss is currently 0.9788038730621338\n",
      "iteration:  1800\n",
      "Iteration 1800 , the loss is currently 0.9797977805137634\n",
      "iteration:  1810\n",
      "Iteration 1810 , the loss is currently 0.9879294037818909\n",
      "iteration:  1820\n",
      "Iteration 1820 , the loss is currently 0.9872148633003235\n",
      "iteration:  1830\n",
      "Iteration 1830 , the loss is currently 0.9953315258026123\n",
      "iteration:  1840\n",
      "Iteration 1840 , the loss is currently 0.9964697957038879\n",
      "iteration:  1850\n",
      "Iteration 1850 , the loss is currently 1.000830888748169\n",
      "iteration:  1860\n",
      "Iteration 1860 , the loss is currently 0.9943220019340515\n",
      "iteration:  1870\n",
      "Iteration 1870 , the loss is currently 0.997327446937561\n",
      "iteration:  1880\n",
      "Iteration 1880 , the loss is currently 0.9826833009719849\n",
      "iteration:  1890\n",
      "Iteration 1890 , the loss is currently 0.9884958267211914\n",
      "iteration:  1900\n",
      "Iteration 1900 , the loss is currently 0.9756426811218262\n",
      "iteration:  1910\n",
      "Iteration 1910 , the loss is currently 0.9916350841522217\n",
      "iteration:  1920\n",
      "Iteration 1920 , the loss is currently 0.9760574102401733\n",
      "iteration:  1930\n",
      "Iteration 1930 , the loss is currently 1.0081257820129395\n",
      "iteration:  1940\n",
      "Iteration 1940 , the loss is currently 0.9962186217308044\n",
      "iteration:  1950\n",
      "Iteration 1950 , the loss is currently 1.000918984413147\n",
      "iteration:  1960\n",
      "Iteration 1960 , the loss is currently 0.9935929179191589\n",
      "iteration:  1970\n",
      "Iteration 1970 , the loss is currently 0.988815188407898\n",
      "iteration:  1980\n",
      "Iteration 1980 , the loss is currently 0.9943917393684387\n",
      "iteration:  1990\n",
      "Iteration 1990 , the loss is currently 0.9965454936027527\n",
      "iteration:  2000\n",
      "Iteration 2000 , the loss is currently 0.984165370464325\n",
      "iteration:  2010\n",
      "Iteration 2010 , the loss is currently 0.9992818832397461\n",
      "iteration:  2020\n",
      "Iteration 2020 , the loss is currently 0.9986482858657837\n",
      "iteration:  2030\n",
      "Iteration 2030 , the loss is currently 0.9950055480003357\n",
      "iteration:  2040\n",
      "Iteration 2040 , the loss is currently 0.9877981543540955\n",
      "iteration:  2050\n",
      "Iteration 2050 , the loss is currently 0.9707626104354858\n",
      "iteration:  2060\n",
      "Iteration 2060 , the loss is currently 0.9939826130867004\n",
      "iteration:  2070\n",
      "Iteration 2070 , the loss is currently 1.0157889127731323\n",
      "iteration:  2080\n",
      "Iteration 2080 , the loss is currently 0.9758477807044983\n",
      "iteration:  2090\n",
      "Iteration 2090 , the loss is currently 0.9982160329818726\n",
      "iteration:  2100\n",
      "Iteration 2100 , the loss is currently 0.9801098108291626\n",
      "iteration:  2110\n",
      "Iteration 2110 , the loss is currently 1.0068851709365845\n",
      "iteration:  2120\n",
      "Iteration 2120 , the loss is currently 0.9833240509033203\n",
      "iteration:  2130\n",
      "Iteration 2130 , the loss is currently 0.9773933291435242\n",
      "iteration:  2140\n",
      "Iteration 2140 , the loss is currently 1.0096327066421509\n",
      "iteration:  2150\n",
      "Iteration 2150 , the loss is currently 0.9657213091850281\n",
      "iteration:  2160\n",
      "Iteration 2160 , the loss is currently 1.005086898803711\n",
      "iteration:  2170\n",
      "Iteration 2170 , the loss is currently 0.9838696122169495\n",
      "iteration:  2180\n",
      "Iteration 2180 , the loss is currently 0.9865177869796753\n",
      "iteration:  2190\n",
      "Iteration 2190 , the loss is currently 0.9782273769378662\n",
      "iteration:  2200\n",
      "Iteration 2200 , the loss is currently 0.9896129965782166\n",
      "iteration:  2210\n",
      "Iteration 2210 , the loss is currently 0.986714243888855\n",
      "iteration:  2220\n",
      "Iteration 2220 , the loss is currently 0.9873799681663513\n",
      "iteration:  2230\n",
      "Iteration 2230 , the loss is currently 0.9835121631622314\n",
      "iteration:  2240\n",
      "Iteration 2240 , the loss is currently 0.9972720146179199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  2250\n",
      "Iteration 2250 , the loss is currently 1.0005340576171875\n",
      "iteration:  2260\n",
      "Iteration 2260 , the loss is currently 0.9980844855308533\n",
      "iteration:  2270\n",
      "Iteration 2270 , the loss is currently 1.0030970573425293\n",
      "iteration:  2280\n",
      "Iteration 2280 , the loss is currently 0.9952865839004517\n",
      "iteration:  2290\n",
      "Iteration 2290 , the loss is currently 0.9883131384849548\n",
      "iteration:  2300\n",
      "Iteration 2300 , the loss is currently 0.9885216355323792\n",
      "iteration:  2310\n",
      "Iteration 2310 , the loss is currently 0.9821539521217346\n",
      "iteration:  2320\n",
      "Iteration 2320 , the loss is currently 0.9952273368835449\n",
      "iteration:  2330\n",
      "Iteration 2330 , the loss is currently 1.0068597793579102\n",
      "iteration:  2340\n",
      "Iteration 2340 , the loss is currently 1.0064204931259155\n",
      "iteration:  2350\n",
      "Iteration 2350 , the loss is currently 0.986929714679718\n",
      "iteration:  2360\n",
      "Iteration 2360 , the loss is currently 0.9917290210723877\n",
      "iteration:  2370\n",
      "Iteration 2370 , the loss is currently 0.9792357087135315\n",
      "iteration:  2380\n",
      "Iteration 2380 , the loss is currently 0.9581016302108765\n",
      "iteration:  2390\n",
      "Iteration 2390 , the loss is currently 1.0109578371047974\n",
      "iteration:  2400\n",
      "Iteration 2400 , the loss is currently 0.9664304852485657\n",
      "iteration:  2410\n",
      "Iteration 2410 , the loss is currently 0.9784754514694214\n",
      "iteration:  2420\n",
      "Iteration 2420 , the loss is currently 0.9981420040130615\n",
      "iteration:  2430\n",
      "Iteration 2430 , the loss is currently 0.9864177703857422\n",
      "iteration:  2440\n",
      "Iteration 2440 , the loss is currently 1.0003788471221924\n",
      "iteration:  2450\n",
      "Iteration 2450 , the loss is currently 0.9780905842781067\n",
      "iteration:  2460\n",
      "Iteration 2460 , the loss is currently 0.9959278106689453\n",
      "iteration:  2470\n",
      "Iteration 2470 , the loss is currently 1.000819206237793\n",
      "iteration:  2480\n",
      "Iteration 2480 , the loss is currently 0.9965331554412842\n",
      "iteration:  2490\n",
      "Iteration 2490 , the loss is currently 0.9676378965377808\n",
      "iteration:  2500\n",
      "Iteration 2500 , the loss is currently 1.0167750120162964\n",
      "iteration:  2510\n",
      "Iteration 2510 , the loss is currently 0.990351140499115\n",
      "iteration:  2520\n",
      "Iteration 2520 , the loss is currently 0.9979214072227478\n",
      "iteration:  2530\n",
      "Iteration 2530 , the loss is currently 1.011978268623352\n",
      "iteration:  2540\n",
      "Iteration 2540 , the loss is currently 0.9905232787132263\n",
      "iteration:  2550\n",
      "Iteration 2550 , the loss is currently 0.9792196154594421\n",
      "iteration:  2560\n",
      "Iteration 2560 , the loss is currently 1.0015225410461426\n",
      "iteration:  2570\n",
      "Iteration 2570 , the loss is currently 0.987739622592926\n",
      "iteration:  2580\n",
      "Iteration 2580 , the loss is currently 0.989191472530365\n",
      "iteration:  2590\n",
      "Iteration 2590 , the loss is currently 0.9750248789787292\n",
      "iteration:  2600\n",
      "Iteration 2600 , the loss is currently 1.0027416944503784\n",
      "iteration:  2610\n",
      "Iteration 2610 , the loss is currently 1.0064749717712402\n",
      "iteration:  2620\n",
      "Iteration 2620 , the loss is currently 1.0019453763961792\n",
      "iteration:  2630\n",
      "Iteration 2630 , the loss is currently 0.9668850898742676\n",
      "iteration:  2640\n",
      "Iteration 2640 , the loss is currently 0.9712159037590027\n",
      "iteration:  2650\n",
      "Iteration 2650 , the loss is currently 1.0033743381500244\n",
      "iteration:  2660\n",
      "Iteration 2660 , the loss is currently 1.0054514408111572\n",
      "iteration:  2670\n",
      "Iteration 2670 , the loss is currently 0.9964829683303833\n",
      "iteration:  2680\n",
      "Iteration 2680 , the loss is currently 0.9873350858688354\n",
      "iteration:  2690\n",
      "Iteration 2690 , the loss is currently 0.987312912940979\n",
      "iteration:  2700\n",
      "Iteration 2700 , the loss is currently 0.9830552935600281\n",
      "iteration:  2710\n",
      "Iteration 2710 , the loss is currently 0.9949367046356201\n",
      "iteration:  2720\n",
      "Iteration 2720 , the loss is currently 0.987615704536438\n",
      "iteration:  2730\n",
      "Iteration 2730 , the loss is currently 0.9774746298789978\n",
      "iteration:  2740\n",
      "Iteration 2740 , the loss is currently 0.976101815700531\n",
      "iteration:  2750\n",
      "Iteration 2750 , the loss is currently 0.9724641442298889\n",
      "iteration:  2760\n",
      "Iteration 2760 , the loss is currently 0.990439236164093\n",
      "iteration:  2770\n",
      "Iteration 2770 , the loss is currently 0.9736742377281189\n",
      "iteration:  2780\n",
      "Iteration 2780 , the loss is currently 1.0011576414108276\n",
      "iteration:  2790\n",
      "Iteration 2790 , the loss is currently 0.9939700961112976\n",
      "iteration:  2800\n",
      "Iteration 2800 , the loss is currently 0.9848040342330933\n",
      "iteration:  2810\n",
      "Iteration 2810 , the loss is currently 0.9753824472427368\n",
      "iteration:  2820\n",
      "Iteration 2820 , the loss is currently 0.9913232326507568\n",
      "iteration:  2830\n",
      "Iteration 2830 , the loss is currently 1.0015747547149658\n",
      "iteration:  2840\n",
      "Iteration 2840 , the loss is currently 1.0088297128677368\n",
      "iteration:  2850\n",
      "Iteration 2850 , the loss is currently 0.997067391872406\n",
      "iteration:  2860\n",
      "Iteration 2860 , the loss is currently 0.9884065985679626\n",
      "iteration:  2870\n",
      "Iteration 2870 , the loss is currently 0.9920260310173035\n",
      "iteration:  2880\n",
      "Iteration 2880 , the loss is currently 0.9746947884559631\n",
      "iteration:  2890\n",
      "Iteration 2890 , the loss is currently 0.9816981554031372\n",
      "iteration:  2900\n",
      "Iteration 2900 , the loss is currently 0.9949197769165039\n",
      "iteration:  2910\n",
      "Iteration 2910 , the loss is currently 0.991191029548645\n",
      "iteration:  2920\n",
      "Iteration 2920 , the loss is currently 0.980033814907074\n",
      "iteration:  2930\n",
      "Iteration 2930 , the loss is currently 1.0028607845306396\n",
      "iteration:  2940\n",
      "Iteration 2940 , the loss is currently 1.0035831928253174\n",
      "iteration:  2950\n",
      "Iteration 2950 , the loss is currently 0.9899558424949646\n",
      "iteration:  2960\n",
      "Iteration 2960 , the loss is currently 0.9993306994438171\n",
      "iteration:  2970\n",
      "Iteration 2970 , the loss is currently 0.9804583787918091\n",
      "iteration:  2980\n",
      "Iteration 2980 , the loss is currently 0.9906107783317566\n",
      "iteration:  2990\n",
      "Iteration 2990 , the loss is currently 0.9916315674781799\n",
      "iteration:  3000\n",
      "Iteration 3000 , the loss is currently 0.9894523024559021\n",
      "iteration:  3010\n",
      "Iteration 3010 , the loss is currently 0.9957661032676697\n",
      "iteration:  3020\n",
      "Iteration 3020 , the loss is currently 0.9799596667289734\n",
      "iteration:  3030\n",
      "Iteration 3030 , the loss is currently 0.9955734610557556\n",
      "iteration:  3040\n",
      "Iteration 3040 , the loss is currently 0.9819517731666565\n",
      "iteration:  3050\n",
      "Iteration 3050 , the loss is currently 0.9778054356575012\n",
      "iteration:  3060\n",
      "Iteration 3060 , the loss is currently 0.9889271855354309\n",
      "iteration:  3070\n",
      "Iteration 3070 , the loss is currently 0.9983344078063965\n",
      "iteration:  3080\n",
      "Iteration 3080 , the loss is currently 0.9932042360305786\n",
      "iteration:  3090\n",
      "Iteration 3090 , the loss is currently 0.9829874634742737\n",
      "iteration:  3100\n",
      "Iteration 3100 , the loss is currently 1.0087978839874268\n",
      "iteration:  3110\n",
      "Iteration 3110 , the loss is currently 0.9817750453948975\n",
      "iteration:  3120\n",
      "Iteration 3120 , the loss is currently 1.000394344329834\n",
      "iteration:  3130\n",
      "Iteration 3130 , the loss is currently 0.9929202198982239\n",
      "iteration:  3140\n",
      "Iteration 3140 , the loss is currently 0.9957624077796936\n",
      "iteration:  3150\n",
      "Iteration 3150 , the loss is currently 0.9769268035888672\n",
      "iteration:  3160\n",
      "Iteration 3160 , the loss is currently 0.9796514511108398\n",
      "iteration:  3170\n",
      "Iteration 3170 , the loss is currently 0.9886471033096313\n",
      "iteration:  3180\n",
      "Iteration 3180 , the loss is currently 0.9912070035934448\n",
      "iteration:  3190\n",
      "Iteration 3190 , the loss is currently 0.9912794828414917\n",
      "iteration:  3200\n",
      "Iteration 3200 , the loss is currently 0.9998177886009216\n",
      "iteration:  3210\n",
      "Iteration 3210 , the loss is currently 1.0053824186325073\n",
      "iteration:  3220\n",
      "Iteration 3220 , the loss is currently 1.0039393901824951\n",
      "iteration:  3230\n",
      "Iteration 3230 , the loss is currently 0.996496319770813\n",
      "iteration:  3240\n",
      "Iteration 3240 , the loss is currently 0.9919189214706421\n",
      "iteration:  3250\n",
      "Iteration 3250 , the loss is currently 0.9820340275764465\n",
      "iteration:  3260\n",
      "Iteration 3260 , the loss is currently 0.9971492886543274\n",
      "iteration:  3270\n",
      "Iteration 3270 , the loss is currently 0.9849737882614136\n",
      "iteration:  3280\n",
      "Iteration 3280 , the loss is currently 1.0154614448547363\n",
      "iteration:  3290\n",
      "Iteration 3290 , the loss is currently 1.00310480594635\n",
      "iteration:  3300\n",
      "Iteration 3300 , the loss is currently 0.9941887259483337\n",
      "iteration:  3310\n",
      "Iteration 3310 , the loss is currently 0.9877294898033142\n",
      "iteration:  3320\n",
      "Iteration 3320 , the loss is currently 0.9856383800506592\n",
      "iteration:  3330\n",
      "Iteration 3330 , the loss is currently 1.0047990083694458\n",
      "iteration:  3340\n",
      "Iteration 3340 , the loss is currently 0.9970923662185669\n",
      "iteration:  3350\n",
      "Iteration 3350 , the loss is currently 0.9991340041160583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  3360\n",
      "Iteration 3360 , the loss is currently 0.9928527474403381\n",
      "iteration:  3370\n",
      "Iteration 3370 , the loss is currently 0.9926616549491882\n",
      "iteration:  3380\n",
      "Iteration 3380 , the loss is currently 0.9823941588401794\n",
      "iteration:  3390\n",
      "Iteration 3390 , the loss is currently 0.9912351965904236\n",
      "iteration:  3400\n",
      "Iteration 3400 , the loss is currently 1.0117264986038208\n",
      "iteration:  3410\n",
      "Iteration 3410 , the loss is currently 0.9964115023612976\n",
      "iteration:  3420\n",
      "Iteration 3420 , the loss is currently 0.9906454682350159\n",
      "iteration:  3430\n",
      "Iteration 3430 , the loss is currently 1.0032330751419067\n",
      "iteration:  3440\n",
      "Iteration 3440 , the loss is currently 0.9999409317970276\n",
      "iteration:  3450\n",
      "Iteration 3450 , the loss is currently 0.993975043296814\n",
      "iteration:  3460\n",
      "Iteration 3460 , the loss is currently 0.9714041352272034\n",
      "iteration:  3470\n",
      "Iteration 3470 , the loss is currently 0.9740735292434692\n",
      "iteration:  3480\n",
      "Iteration 3480 , the loss is currently 1.0092452764511108\n",
      "iteration:  3490\n",
      "Iteration 3490 , the loss is currently 1.0105088949203491\n",
      "iteration:  3500\n",
      "Iteration 3500 , the loss is currently 0.9917898774147034\n",
      "iteration:  3510\n",
      "Iteration 3510 , the loss is currently 0.9775020480155945\n",
      "iteration:  3520\n",
      "Iteration 3520 , the loss is currently 1.0166157484054565\n",
      "iteration:  3530\n",
      "Iteration 3530 , the loss is currently 0.9919420480728149\n",
      "iteration:  3540\n",
      "Iteration 3540 , the loss is currently 0.9720103144645691\n",
      "iteration:  3550\n",
      "Iteration 3550 , the loss is currently 0.9990748167037964\n",
      "iteration:  3560\n",
      "Iteration 3560 , the loss is currently 1.007339358329773\n",
      "iteration:  3570\n",
      "Iteration 3570 , the loss is currently 0.9676613807678223\n",
      "iteration:  3580\n",
      "Iteration 3580 , the loss is currently 1.0039478540420532\n",
      "iteration:  3590\n",
      "Iteration 3590 , the loss is currently 0.9900568127632141\n",
      "iteration:  3600\n",
      "Iteration 3600 , the loss is currently 0.9896419048309326\n",
      "iteration:  3610\n",
      "Iteration 3610 , the loss is currently 0.9957807660102844\n",
      "iteration:  3620\n",
      "Iteration 3620 , the loss is currently 0.9987935423851013\n",
      "iteration:  3630\n",
      "Iteration 3630 , the loss is currently 0.9753552079200745\n",
      "iteration:  3640\n",
      "Iteration 3640 , the loss is currently 0.9845581650733948\n",
      "iteration:  3650\n",
      "Iteration 3650 , the loss is currently 0.982427716255188\n",
      "iteration:  3660\n",
      "Iteration 3660 , the loss is currently 0.995775580406189\n",
      "iteration:  3670\n",
      "Iteration 3670 , the loss is currently 0.9755187630653381\n",
      "iteration:  3680\n",
      "Iteration 3680 , the loss is currently 1.0083656311035156\n",
      "iteration:  3690\n",
      "Iteration 3690 , the loss is currently 1.0284664630889893\n",
      "iteration:  3700\n",
      "Iteration 3700 , the loss is currently 0.9845395684242249\n",
      "iteration:  3710\n",
      "Iteration 3710 , the loss is currently 0.9979709982872009\n",
      "iteration:  3720\n",
      "Iteration 3720 , the loss is currently 0.9887374639511108\n",
      "iteration:  3730\n",
      "Iteration 3730 , the loss is currently 0.9733375310897827\n",
      "iteration:  3740\n",
      "Iteration 3740 , the loss is currently 0.9957495927810669\n",
      "iteration:  3750\n",
      "Iteration 3750 , the loss is currently 0.9939244389533997\n",
      "iteration:  3760\n",
      "Iteration 3760 , the loss is currently 0.9921183586120605\n",
      "iteration:  3770\n",
      "Iteration 3770 , the loss is currently 0.9802221059799194\n",
      "iteration:  3780\n",
      "Iteration 3780 , the loss is currently 0.9908594489097595\n",
      "iteration:  3790\n",
      "Iteration 3790 , the loss is currently 0.9618607759475708\n",
      "iteration:  3800\n",
      "Iteration 3800 , the loss is currently 0.9943653345108032\n",
      "iteration:  3810\n",
      "Iteration 3810 , the loss is currently 0.9977315664291382\n",
      "iteration:  3820\n",
      "Iteration 3820 , the loss is currently 0.9896985292434692\n",
      "iteration:  3830\n",
      "Iteration 3830 , the loss is currently 0.9849740862846375\n",
      "iteration:  3840\n",
      "Iteration 3840 , the loss is currently 0.9695055484771729\n",
      "iteration:  3850\n",
      "Iteration 3850 , the loss is currently 0.993101179599762\n",
      "iteration:  3860\n",
      "Iteration 3860 , the loss is currently 0.9777982234954834\n",
      "iteration:  3870\n",
      "Iteration 3870 , the loss is currently 1.0100324153900146\n",
      "iteration:  3880\n",
      "Iteration 3880 , the loss is currently 0.9963776469230652\n",
      "iteration:  3890\n",
      "Iteration 3890 , the loss is currently 0.978685736656189\n",
      "iteration:  3900\n",
      "Iteration 3900 , the loss is currently 1.0085344314575195\n",
      "iteration:  3910\n",
      "Iteration 3910 , the loss is currently 0.9802924394607544\n",
      "iteration:  3920\n",
      "Iteration 3920 , the loss is currently 0.9980001449584961\n",
      "iteration:  3930\n",
      "Iteration 3930 , the loss is currently 0.9905930161476135\n",
      "iteration:  3940\n",
      "Iteration 3940 , the loss is currently 0.999855101108551\n",
      "iteration:  3950\n",
      "Iteration 3950 , the loss is currently 0.9792459011077881\n",
      "iteration:  3960\n",
      "Iteration 3960 , the loss is currently 0.9926785826683044\n",
      "iteration:  3970\n",
      "Iteration 3970 , the loss is currently 0.9807354211807251\n",
      "iteration:  3980\n",
      "Iteration 3980 , the loss is currently 0.9795020818710327\n",
      "iteration:  3990\n",
      "Iteration 3990 , the loss is currently 0.9941846132278442\n",
      "iteration:  4000\n",
      "Iteration 4000 , the loss is currently 0.9781274795532227\n",
      "iteration:  4010\n",
      "Iteration 4010 , the loss is currently 1.0070139169692993\n",
      "iteration:  4020\n",
      "Iteration 4020 , the loss is currently 0.9818645119667053\n",
      "iteration:  4030\n",
      "Iteration 4030 , the loss is currently 0.9912365674972534\n",
      "iteration:  4040\n",
      "Iteration 4040 , the loss is currently 0.9787935614585876\n",
      "iteration:  4050\n",
      "Iteration 4050 , the loss is currently 0.9923432469367981\n",
      "iteration:  4060\n",
      "Iteration 4060 , the loss is currently 1.0054608583450317\n",
      "iteration:  4070\n",
      "Iteration 4070 , the loss is currently 1.0062772035598755\n",
      "iteration:  4080\n",
      "Iteration 4080 , the loss is currently 0.9974811673164368\n",
      "iteration:  4090\n",
      "Iteration 4090 , the loss is currently 1.0066546201705933\n",
      "iteration:  4100\n",
      "Iteration 4100 , the loss is currently 0.9999908804893494\n",
      "iteration:  4110\n",
      "Iteration 4110 , the loss is currently 0.9922271966934204\n",
      "iteration:  4120\n",
      "Iteration 4120 , the loss is currently 0.9790127277374268\n",
      "iteration:  4130\n",
      "Iteration 4130 , the loss is currently 0.9831496477127075\n",
      "iteration:  4140\n",
      "Iteration 4140 , the loss is currently 0.9934293627738953\n",
      "iteration:  4150\n",
      "Iteration 4150 , the loss is currently 0.9944274425506592\n",
      "iteration:  4160\n",
      "Iteration 4160 , the loss is currently 0.9866670966148376\n",
      "iteration:  4170\n",
      "Iteration 4170 , the loss is currently 0.9987103939056396\n",
      "iteration:  4180\n",
      "Iteration 4180 , the loss is currently 0.9842925071716309\n",
      "iteration:  4190\n",
      "Iteration 4190 , the loss is currently 0.9875925779342651\n",
      "iteration:  4200\n",
      "Iteration 4200 , the loss is currently 1.0070579051971436\n",
      "iteration:  4210\n",
      "Iteration 4210 , the loss is currently 1.0030694007873535\n",
      "iteration:  4220\n",
      "Iteration 4220 , the loss is currently 0.98280268907547\n",
      "iteration:  4230\n",
      "Iteration 4230 , the loss is currently 0.9966099262237549\n",
      "iteration:  4240\n",
      "Iteration 4240 , the loss is currently 0.9983294606208801\n",
      "iteration:  4250\n",
      "Iteration 4250 , the loss is currently 0.998257040977478\n",
      "iteration:  4260\n",
      "Iteration 4260 , the loss is currently 0.9968904256820679\n",
      "iteration:  4270\n",
      "Iteration 4270 , the loss is currently 0.9773842692375183\n",
      "iteration:  4280\n",
      "Iteration 4280 , the loss is currently 0.9873020648956299\n",
      "iteration:  4290\n",
      "Iteration 4290 , the loss is currently 0.9889995455741882\n",
      "iteration:  4300\n",
      "Iteration 4300 , the loss is currently 0.9792213439941406\n",
      "iteration:  4310\n",
      "Iteration 4310 , the loss is currently 0.9850531220436096\n",
      "iteration:  4320\n",
      "Iteration 4320 , the loss is currently 1.005294680595398\n",
      "iteration:  4330\n",
      "Iteration 4330 , the loss is currently 0.9851071238517761\n",
      "iteration:  4340\n",
      "Iteration 4340 , the loss is currently 0.9941624402999878\n",
      "iteration:  4350\n",
      "Iteration 4350 , the loss is currently 0.990166187286377\n",
      "iteration:  4360\n",
      "Iteration 4360 , the loss is currently 1.0100622177124023\n",
      "iteration:  4370\n",
      "Iteration 4370 , the loss is currently 0.9775122404098511\n",
      "iteration:  4380\n",
      "Iteration 4380 , the loss is currently 1.0015677213668823\n",
      "iteration:  4390\n",
      "Iteration 4390 , the loss is currently 1.0124894380569458\n",
      "iteration:  4400\n",
      "Iteration 4400 , the loss is currently 0.9967347979545593\n",
      "iteration:  4410\n",
      "Iteration 4410 , the loss is currently 0.9985880255699158\n",
      "iteration:  4420\n",
      "Iteration 4420 , the loss is currently 0.9828333854675293\n",
      "iteration:  4430\n",
      "Iteration 4430 , the loss is currently 1.0097521543502808\n",
      "iteration:  4440\n",
      "Iteration 4440 , the loss is currently 0.9856915473937988\n",
      "iteration:  4450\n",
      "Iteration 4450 , the loss is currently 0.988692581653595\n",
      "iteration:  4460\n",
      "Iteration 4460 , the loss is currently 0.9838628172874451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  4470\n",
      "Iteration 4470 , the loss is currently 1.0195415019989014\n",
      "iteration:  4480\n",
      "Iteration 4480 , the loss is currently 0.9794303178787231\n",
      "iteration:  4490\n",
      "Iteration 4490 , the loss is currently 0.9861177206039429\n",
      "iteration:  4500\n",
      "Iteration 4500 , the loss is currently 0.9904816746711731\n",
      "iteration:  4510\n",
      "Iteration 4510 , the loss is currently 0.990121066570282\n",
      "iteration:  4520\n",
      "Iteration 4520 , the loss is currently 0.9908421635627747\n",
      "iteration:  4530\n",
      "Iteration 4530 , the loss is currently 0.9721967577934265\n",
      "iteration:  4540\n",
      "Iteration 4540 , the loss is currently 1.0068144798278809\n",
      "iteration:  4550\n",
      "Iteration 4550 , the loss is currently 1.0056573152542114\n",
      "iteration:  4560\n",
      "Iteration 4560 , the loss is currently 0.992931067943573\n",
      "iteration:  4570\n",
      "Iteration 4570 , the loss is currently 0.9811499118804932\n",
      "iteration:  4580\n",
      "Iteration 4580 , the loss is currently 0.9841657280921936\n",
      "iteration:  4590\n",
      "Iteration 4590 , the loss is currently 1.0058313608169556\n",
      "iteration:  4600\n",
      "Iteration 4600 , the loss is currently 0.973848819732666\n",
      "iteration:  4610\n",
      "Iteration 4610 , the loss is currently 0.9949080348014832\n",
      "iteration:  4620\n",
      "Iteration 4620 , the loss is currently 0.9852796792984009\n",
      "iteration:  4630\n",
      "Iteration 4630 , the loss is currently 1.0103962421417236\n",
      "iteration:  4640\n",
      "Iteration 4640 , the loss is currently 0.9988622665405273\n",
      "iteration:  4650\n",
      "Iteration 4650 , the loss is currently 0.98105388879776\n",
      "iteration:  4660\n",
      "Iteration 4660 , the loss is currently 0.9779936075210571\n",
      "iteration:  4670\n",
      "Iteration 4670 , the loss is currently 0.975801944732666\n",
      "iteration:  4680\n",
      "Iteration 4680 , the loss is currently 0.9911144375801086\n",
      "iteration:  4690\n",
      "Iteration 4690 , the loss is currently 0.9979439377784729\n",
      "iteration:  4700\n",
      "Iteration 4700 , the loss is currently 0.9966354370117188\n",
      "iteration:  4710\n",
      "Iteration 4710 , the loss is currently 0.9931089878082275\n",
      "iteration:  4720\n",
      "Iteration 4720 , the loss is currently 1.007007360458374\n",
      "iteration:  4730\n",
      "Iteration 4730 , the loss is currently 0.9773375988006592\n",
      "iteration:  4740\n",
      "Iteration 4740 , the loss is currently 0.9855280518531799\n",
      "iteration:  4750\n",
      "Iteration 4750 , the loss is currently 1.0023176670074463\n",
      "iteration:  4760\n",
      "Iteration 4760 , the loss is currently 0.9972879886627197\n",
      "iteration:  4770\n",
      "Iteration 4770 , the loss is currently 0.9966106414794922\n",
      "iteration:  4780\n",
      "Iteration 4780 , the loss is currently 0.9700577855110168\n",
      "iteration:  4790\n",
      "Iteration 4790 , the loss is currently 0.9866415858268738\n",
      "iteration:  4800\n",
      "Iteration 4800 , the loss is currently 0.9762635231018066\n",
      "iteration:  4810\n",
      "Iteration 4810 , the loss is currently 0.986927330493927\n",
      "iteration:  4820\n",
      "Iteration 4820 , the loss is currently 0.9980066418647766\n",
      "iteration:  4830\n",
      "Iteration 4830 , the loss is currently 0.9718449115753174\n",
      "iteration:  4840\n",
      "Iteration 4840 , the loss is currently 0.9969446659088135\n",
      "iteration:  4850\n",
      "Iteration 4850 , the loss is currently 0.9983483552932739\n",
      "iteration:  4860\n",
      "Iteration 4860 , the loss is currently 1.0114374160766602\n",
      "iteration:  4870\n",
      "Iteration 4870 , the loss is currently 0.9781865477561951\n",
      "iteration:  4880\n",
      "Iteration 4880 , the loss is currently 0.997748613357544\n",
      "iteration:  4890\n",
      "Iteration 4890 , the loss is currently 0.9956938624382019\n",
      "iteration:  4900\n",
      "Iteration 4900 , the loss is currently 0.9829656481742859\n",
      "iteration:  4910\n",
      "Iteration 4910 , the loss is currently 0.9991120100021362\n",
      "iteration:  4920\n",
      "Iteration 4920 , the loss is currently 0.9831031560897827\n",
      "iteration:  4930\n",
      "Iteration 4930 , the loss is currently 1.0145992040634155\n",
      "iteration:  4940\n",
      "Iteration 4940 , the loss is currently 0.9950661659240723\n",
      "iteration:  4950\n",
      "Iteration 4950 , the loss is currently 1.000348687171936\n",
      "iteration:  4960\n",
      "Iteration 4960 , the loss is currently 1.0008320808410645\n",
      "iteration:  4970\n",
      "Iteration 4970 , the loss is currently 1.0116735696792603\n",
      "iteration:  4980\n",
      "Iteration 4980 , the loss is currently 1.0079503059387207\n",
      "iteration:  4990\n",
      "Iteration 4990 , the loss is currently 0.9806938767433167\n",
      "iteration:  5000\n",
      "Iteration 5000 , the loss is currently 0.973307728767395\n",
      "iteration:  5010\n",
      "Iteration 5010 , the loss is currently 0.9823835492134094\n",
      "iteration:  5020\n",
      "Iteration 5020 , the loss is currently 0.9803375005722046\n",
      "iteration:  5030\n",
      "Iteration 5030 , the loss is currently 0.9901346564292908\n",
      "iteration:  5040\n",
      "Iteration 5040 , the loss is currently 0.9993878602981567\n",
      "iteration:  5050\n",
      "Iteration 5050 , the loss is currently 0.9897312521934509\n",
      "iteration:  5060\n",
      "Iteration 5060 , the loss is currently 0.9845764636993408\n",
      "iteration:  5070\n",
      "Iteration 5070 , the loss is currently 0.9901460409164429\n",
      "iteration:  5080\n",
      "Iteration 5080 , the loss is currently 0.9902264475822449\n",
      "iteration:  5090\n",
      "Iteration 5090 , the loss is currently 0.9843723773956299\n",
      "iteration:  5100\n",
      "Iteration 5100 , the loss is currently 0.9848324656486511\n",
      "iteration:  5110\n",
      "Iteration 5110 , the loss is currently 0.9923474788665771\n",
      "iteration:  5120\n",
      "Iteration 5120 , the loss is currently 0.988111138343811\n",
      "iteration:  5130\n",
      "Iteration 5130 , the loss is currently 0.990792453289032\n",
      "iteration:  5140\n",
      "Iteration 5140 , the loss is currently 0.9769999980926514\n",
      "iteration:  5150\n",
      "Iteration 5150 , the loss is currently 0.995391845703125\n",
      "iteration:  5160\n",
      "Iteration 5160 , the loss is currently 1.0111231803894043\n",
      "iteration:  5170\n",
      "Iteration 5170 , the loss is currently 0.9805671572685242\n",
      "iteration:  5180\n",
      "Iteration 5180 , the loss is currently 0.9985279440879822\n",
      "iteration:  5190\n",
      "Iteration 5190 , the loss is currently 0.9970513582229614\n",
      "iteration:  5200\n",
      "Iteration 5200 , the loss is currently 1.0089404582977295\n",
      "iteration:  5210\n",
      "Iteration 5210 , the loss is currently 0.974821150302887\n",
      "iteration:  5220\n",
      "Iteration 5220 , the loss is currently 1.0154781341552734\n",
      "iteration:  5230\n",
      "Iteration 5230 , the loss is currently 1.0069670677185059\n",
      "iteration:  5240\n",
      "Iteration 5240 , the loss is currently 0.9810865521430969\n",
      "iteration:  5250\n",
      "Iteration 5250 , the loss is currently 0.9894117712974548\n",
      "iteration:  5260\n",
      "Iteration 5260 , the loss is currently 0.9818555116653442\n",
      "iteration:  5270\n",
      "Iteration 5270 , the loss is currently 0.9784042835235596\n",
      "iteration:  5280\n",
      "Iteration 5280 , the loss is currently 1.0129717588424683\n",
      "iteration:  5290\n",
      "Iteration 5290 , the loss is currently 1.0018709897994995\n",
      "iteration:  5300\n",
      "Iteration 5300 , the loss is currently 0.9732615351676941\n",
      "iteration:  5310\n",
      "Iteration 5310 , the loss is currently 0.9856350421905518\n",
      "iteration:  5320\n",
      "Iteration 5320 , the loss is currently 1.0048197507858276\n",
      "iteration:  5330\n",
      "Iteration 5330 , the loss is currently 0.9844605922698975\n",
      "iteration:  5340\n",
      "Iteration 5340 , the loss is currently 1.0241427421569824\n",
      "iteration:  5350\n",
      "Iteration 5350 , the loss is currently 0.9863736033439636\n",
      "iteration:  5360\n",
      "Iteration 5360 , the loss is currently 0.9913292527198792\n",
      "iteration:  5370\n",
      "Iteration 5370 , the loss is currently 1.0014817714691162\n",
      "iteration:  5380\n",
      "Iteration 5380 , the loss is currently 0.9859532117843628\n",
      "iteration:  5390\n",
      "Iteration 5390 , the loss is currently 0.9884301424026489\n",
      "iteration:  5400\n",
      "Iteration 5400 , the loss is currently 0.9666076898574829\n",
      "iteration:  5410\n",
      "Iteration 5410 , the loss is currently 0.992867112159729\n",
      "iteration:  5420\n",
      "Iteration 5420 , the loss is currently 0.9918637871742249\n",
      "iteration:  5430\n",
      "Iteration 5430 , the loss is currently 1.006738305091858\n",
      "iteration:  5440\n",
      "Iteration 5440 , the loss is currently 0.9890291690826416\n",
      "iteration:  5450\n",
      "Iteration 5450 , the loss is currently 0.984184741973877\n",
      "iteration:  5460\n",
      "Iteration 5460 , the loss is currently 1.0005743503570557\n",
      "iteration:  5470\n",
      "Iteration 5470 , the loss is currently 0.9895893335342407\n",
      "iteration:  5480\n",
      "Iteration 5480 , the loss is currently 0.9977630376815796\n",
      "iteration:  5490\n",
      "Iteration 5490 , the loss is currently 0.9918140769004822\n",
      "iteration:  5500\n",
      "Iteration 5500 , the loss is currently 0.9874581694602966\n",
      "iteration:  5510\n",
      "Iteration 5510 , the loss is currently 0.9880988001823425\n",
      "iteration:  5520\n",
      "Iteration 5520 , the loss is currently 1.0030065774917603\n",
      "iteration:  5530\n",
      "Iteration 5530 , the loss is currently 0.9769399762153625\n",
      "iteration:  5540\n",
      "Iteration 5540 , the loss is currently 0.9629469513893127\n",
      "iteration:  5550\n",
      "Iteration 5550 , the loss is currently 0.9883466958999634\n",
      "iteration:  5560\n",
      "Iteration 5560 , the loss is currently 0.9979947805404663\n",
      "iteration:  5570\n",
      "Iteration 5570 , the loss is currently 0.9841395616531372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  5580\n",
      "Iteration 5580 , the loss is currently 0.9906922578811646\n",
      "iteration:  5590\n",
      "Iteration 5590 , the loss is currently 1.0025815963745117\n",
      "iteration:  5600\n",
      "Iteration 5600 , the loss is currently 0.9916584491729736\n",
      "iteration:  5610\n",
      "Iteration 5610 , the loss is currently 0.9889116287231445\n",
      "iteration:  5620\n",
      "Iteration 5620 , the loss is currently 0.982477605342865\n",
      "iteration:  5630\n",
      "Iteration 5630 , the loss is currently 0.9914987087249756\n",
      "iteration:  5640\n",
      "Iteration 5640 , the loss is currently 0.9838247895240784\n",
      "iteration:  5650\n",
      "Iteration 5650 , the loss is currently 0.9825847744941711\n",
      "iteration:  5660\n",
      "Iteration 5660 , the loss is currently 0.9846867322921753\n",
      "iteration:  5670\n",
      "Iteration 5670 , the loss is currently 0.9918268918991089\n",
      "iteration:  5680\n",
      "Iteration 5680 , the loss is currently 0.985223650932312\n",
      "iteration:  5690\n",
      "Iteration 5690 , the loss is currently 0.9902732968330383\n",
      "iteration:  5700\n",
      "Iteration 5700 , the loss is currently 0.9735593199729919\n",
      "iteration:  5710\n",
      "Iteration 5710 , the loss is currently 0.9852128624916077\n",
      "iteration:  5720\n",
      "Iteration 5720 , the loss is currently 0.9679203629493713\n",
      "iteration:  5730\n",
      "Iteration 5730 , the loss is currently 0.9682294130325317\n",
      "iteration:  5740\n",
      "Iteration 5740 , the loss is currently 0.975001871585846\n",
      "iteration:  5750\n",
      "Iteration 5750 , the loss is currently 0.9953706860542297\n",
      "iteration:  5760\n",
      "Iteration 5760 , the loss is currently 0.9981843829154968\n",
      "iteration:  5770\n",
      "Iteration 5770 , the loss is currently 0.9724199771881104\n",
      "iteration:  5780\n",
      "Iteration 5780 , the loss is currently 0.9720005393028259\n",
      "iteration:  5790\n",
      "Iteration 5790 , the loss is currently 0.9922040700912476\n",
      "iteration:  5800\n",
      "Iteration 5800 , the loss is currently 0.9927866458892822\n",
      "iteration:  5810\n",
      "Iteration 5810 , the loss is currently 1.0051831007003784\n",
      "iteration:  5820\n",
      "Iteration 5820 , the loss is currently 1.0136245489120483\n",
      "iteration:  5830\n",
      "Iteration 5830 , the loss is currently 0.9932792782783508\n",
      "iteration:  5840\n",
      "Iteration 5840 , the loss is currently 1.001865267753601\n",
      "iteration:  5850\n",
      "Iteration 5850 , the loss is currently 0.999187171459198\n",
      "iteration:  5860\n",
      "Iteration 5860 , the loss is currently 0.9781008958816528\n",
      "iteration:  5870\n",
      "Iteration 5870 , the loss is currently 0.9919020533561707\n",
      "iteration:  5880\n",
      "Iteration 5880 , the loss is currently 0.9833745360374451\n",
      "iteration:  5890\n",
      "Iteration 5890 , the loss is currently 1.005501627922058\n",
      "iteration:  5900\n",
      "Iteration 5900 , the loss is currently 1.008716344833374\n",
      "iteration:  5910\n",
      "Iteration 5910 , the loss is currently 0.9890860319137573\n",
      "iteration:  5920\n",
      "Iteration 5920 , the loss is currently 0.9978074431419373\n",
      "iteration:  5930\n",
      "Iteration 5930 , the loss is currently 0.991303563117981\n",
      "iteration:  5940\n",
      "Iteration 5940 , the loss is currently 1.0062534809112549\n",
      "iteration:  5950\n",
      "Iteration 5950 , the loss is currently 0.9845057725906372\n",
      "iteration:  5960\n",
      "Iteration 5960 , the loss is currently 1.0116747617721558\n",
      "iteration:  5970\n",
      "Iteration 5970 , the loss is currently 0.9974374771118164\n",
      "iteration:  5980\n",
      "Iteration 5980 , the loss is currently 0.9832364320755005\n",
      "iteration:  5990\n",
      "Iteration 5990 , the loss is currently 0.9882925748825073\n",
      "iteration:  6000\n",
      "Iteration 6000 , the loss is currently 0.9852957725524902\n",
      "iteration:  6010\n",
      "Iteration 6010 , the loss is currently 0.9816410541534424\n",
      "iteration:  6020\n",
      "Iteration 6020 , the loss is currently 0.997948944568634\n",
      "iteration:  6030\n",
      "Iteration 6030 , the loss is currently 1.0089292526245117\n",
      "iteration:  6040\n",
      "Iteration 6040 , the loss is currently 1.0005384683609009\n",
      "iteration:  6050\n",
      "Iteration 6050 , the loss is currently 0.9902657866477966\n",
      "iteration:  6060\n",
      "Iteration 6060 , the loss is currently 0.9904083609580994\n",
      "iteration:  6070\n",
      "Iteration 6070 , the loss is currently 0.9804798364639282\n",
      "iteration:  6080\n",
      "Iteration 6080 , the loss is currently 0.997921884059906\n",
      "iteration:  6090\n",
      "Iteration 6090 , the loss is currently 1.0057302713394165\n",
      "iteration:  6100\n",
      "Iteration 6100 , the loss is currently 0.9932838082313538\n",
      "iteration:  6110\n",
      "Iteration 6110 , the loss is currently 0.9845271110534668\n",
      "iteration:  6120\n",
      "Iteration 6120 , the loss is currently 0.9988937973976135\n",
      "iteration:  6130\n",
      "Iteration 6130 , the loss is currently 1.0054259300231934\n",
      "iteration:  6140\n",
      "Iteration 6140 , the loss is currently 1.0020662546157837\n",
      "iteration:  6150\n",
      "Iteration 6150 , the loss is currently 0.9785568714141846\n",
      "iteration:  6160\n",
      "Iteration 6160 , the loss is currently 0.9909981489181519\n",
      "iteration:  6170\n",
      "Iteration 6170 , the loss is currently 1.0068676471710205\n",
      "iteration:  6180\n",
      "Iteration 6180 , the loss is currently 0.9867122173309326\n",
      "iteration:  6190\n",
      "Iteration 6190 , the loss is currently 0.9999365210533142\n",
      "iteration:  6200\n",
      "Iteration 6200 , the loss is currently 0.9995776414871216\n",
      "iteration:  6210\n",
      "Iteration 6210 , the loss is currently 1.0006927251815796\n",
      "iteration:  6220\n",
      "Iteration 6220 , the loss is currently 1.005104422569275\n",
      "iteration:  6230\n",
      "Iteration 6230 , the loss is currently 0.9797130823135376\n",
      "iteration:  6240\n",
      "Iteration 6240 , the loss is currently 0.9993263483047485\n",
      "iteration:  6250\n",
      "Iteration 6250 , the loss is currently 1.001655101776123\n",
      "iteration:  6260\n",
      "Iteration 6260 , the loss is currently 0.9934905767440796\n",
      "iteration:  6270\n",
      "Iteration 6270 , the loss is currently 0.9998817443847656\n",
      "iteration:  6280\n",
      "Iteration 6280 , the loss is currently 0.9938933253288269\n",
      "iteration:  6290\n",
      "Iteration 6290 , the loss is currently 0.9866952896118164\n",
      "iteration:  6300\n",
      "Iteration 6300 , the loss is currently 0.9880821108818054\n",
      "iteration:  6310\n",
      "Iteration 6310 , the loss is currently 0.986737072467804\n",
      "iteration:  6320\n",
      "Iteration 6320 , the loss is currently 0.9932509064674377\n",
      "iteration:  6330\n",
      "Iteration 6330 , the loss is currently 0.9867569208145142\n",
      "iteration:  6340\n",
      "Iteration 6340 , the loss is currently 1.0156761407852173\n",
      "iteration:  6350\n",
      "Iteration 6350 , the loss is currently 0.9880920648574829\n",
      "iteration:  6360\n",
      "Iteration 6360 , the loss is currently 0.9840590953826904\n",
      "iteration:  6370\n",
      "Iteration 6370 , the loss is currently 0.999134361743927\n",
      "iteration:  6380\n",
      "Iteration 6380 , the loss is currently 0.9983893036842346\n",
      "iteration:  6390\n",
      "Iteration 6390 , the loss is currently 0.9935994744300842\n",
      "iteration:  6400\n",
      "Iteration 6400 , the loss is currently 0.9900222420692444\n",
      "iteration:  6410\n",
      "Iteration 6410 , the loss is currently 1.0032103061676025\n",
      "iteration:  6420\n",
      "Iteration 6420 , the loss is currently 0.9714654088020325\n",
      "iteration:  6430\n",
      "Iteration 6430 , the loss is currently 0.985548734664917\n",
      "iteration:  6440\n",
      "Iteration 6440 , the loss is currently 0.9941548705101013\n",
      "iteration:  6450\n",
      "Iteration 6450 , the loss is currently 0.9937626719474792\n",
      "iteration:  6460\n",
      "Iteration 6460 , the loss is currently 0.9845325350761414\n",
      "iteration:  6470\n",
      "Iteration 6470 , the loss is currently 0.975122332572937\n",
      "iteration:  6480\n",
      "Iteration 6480 , the loss is currently 0.9670816659927368\n",
      "iteration:  6490\n",
      "Iteration 6490 , the loss is currently 0.9715288281440735\n",
      "iteration:  6500\n",
      "Iteration 6500 , the loss is currently 0.9926204681396484\n",
      "iteration:  6510\n",
      "Iteration 6510 , the loss is currently 1.0015817880630493\n",
      "iteration:  6520\n",
      "Iteration 6520 , the loss is currently 0.9914409518241882\n",
      "iteration:  6530\n",
      "Iteration 6530 , the loss is currently 0.9891442656517029\n",
      "iteration:  6540\n",
      "Iteration 6540 , the loss is currently 0.9924782514572144\n",
      "iteration:  6550\n",
      "Iteration 6550 , the loss is currently 1.0004985332489014\n",
      "iteration:  6560\n",
      "Iteration 6560 , the loss is currently 0.9916388988494873\n",
      "iteration:  6570\n",
      "Iteration 6570 , the loss is currently 1.0051485300064087\n",
      "iteration:  6580\n",
      "Iteration 6580 , the loss is currently 0.9765486717224121\n",
      "iteration:  6590\n",
      "Iteration 6590 , the loss is currently 1.00530207157135\n",
      "iteration:  6600\n",
      "Iteration 6600 , the loss is currently 0.9758045673370361\n",
      "iteration:  6610\n",
      "Iteration 6610 , the loss is currently 0.984673023223877\n",
      "iteration:  6620\n",
      "Iteration 6620 , the loss is currently 0.9949264526367188\n",
      "iteration:  6630\n",
      "Iteration 6630 , the loss is currently 0.9791119694709778\n",
      "iteration:  6640\n",
      "Iteration 6640 , the loss is currently 0.9718192219734192\n",
      "iteration:  6650\n",
      "Iteration 6650 , the loss is currently 0.9752008318901062\n",
      "iteration:  6660\n",
      "Iteration 6660 , the loss is currently 0.9833177924156189\n",
      "iteration:  6670\n",
      "Iteration 6670 , the loss is currently 0.9981704950332642\n",
      "iteration:  6680\n",
      "Iteration 6680 , the loss is currently 0.9940522313117981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  6690\n",
      "Iteration 6690 , the loss is currently 1.0074599981307983\n",
      "iteration:  6700\n",
      "Iteration 6700 , the loss is currently 1.0046104192733765\n",
      "iteration:  6710\n",
      "Iteration 6710 , the loss is currently 0.9921020269393921\n",
      "iteration:  6720\n",
      "Iteration 6720 , the loss is currently 0.9868764281272888\n",
      "iteration:  6730\n",
      "Iteration 6730 , the loss is currently 0.9858216047286987\n",
      "iteration:  6740\n",
      "Iteration 6740 , the loss is currently 0.9833104014396667\n",
      "iteration:  6750\n",
      "Iteration 6750 , the loss is currently 0.9979032874107361\n",
      "iteration:  6760\n",
      "Iteration 6760 , the loss is currently 1.0127909183502197\n",
      "iteration:  6770\n",
      "Iteration 6770 , the loss is currently 1.0001381635665894\n",
      "iteration:  6780\n",
      "Iteration 6780 , the loss is currently 0.9947106838226318\n",
      "iteration:  6790\n",
      "Iteration 6790 , the loss is currently 0.9831426739692688\n",
      "iteration:  6800\n",
      "Iteration 6800 , the loss is currently 0.9838276505470276\n",
      "iteration:  6810\n",
      "Iteration 6810 , the loss is currently 1.0028942823410034\n",
      "iteration:  6820\n",
      "Iteration 6820 , the loss is currently 1.0046602487564087\n",
      "iteration:  6830\n",
      "Iteration 6830 , the loss is currently 1.0141098499298096\n",
      "iteration:  6840\n",
      "Iteration 6840 , the loss is currently 0.9968249201774597\n",
      "iteration:  6850\n",
      "Iteration 6850 , the loss is currently 0.9983429312705994\n",
      "iteration:  6860\n",
      "Iteration 6860 , the loss is currently 1.01228666305542\n",
      "iteration:  6870\n",
      "Iteration 6870 , the loss is currently 1.0007261037826538\n",
      "iteration:  6880\n",
      "Iteration 6880 , the loss is currently 0.9909578561782837\n",
      "iteration:  6890\n",
      "Iteration 6890 , the loss is currently 0.9910005927085876\n",
      "iteration:  6900\n",
      "Iteration 6900 , the loss is currently 0.9770216941833496\n",
      "iteration:  6910\n",
      "Iteration 6910 , the loss is currently 0.9857641458511353\n",
      "iteration:  6920\n",
      "Iteration 6920 , the loss is currently 0.9922806024551392\n",
      "iteration:  6930\n",
      "Iteration 6930 , the loss is currently 0.9898972511291504\n",
      "iteration:  6940\n",
      "Iteration 6940 , the loss is currently 0.9803135395050049\n",
      "iteration:  6950\n",
      "Iteration 6950 , the loss is currently 0.9997389912605286\n",
      "iteration:  6960\n",
      "Iteration 6960 , the loss is currently 0.9939860701560974\n",
      "iteration:  6970\n",
      "Iteration 6970 , the loss is currently 0.9920251369476318\n",
      "iteration:  6980\n",
      "Iteration 6980 , the loss is currently 0.9906066060066223\n",
      "iteration:  6990\n",
      "Iteration 6990 , the loss is currently 0.999372661113739\n",
      "iteration:  7000\n",
      "Iteration 7000 , the loss is currently 1.0138128995895386\n",
      "iteration:  7010\n",
      "Iteration 7010 , the loss is currently 0.9995884299278259\n",
      "iteration:  7020\n",
      "Iteration 7020 , the loss is currently 0.9925632476806641\n",
      "iteration:  7030\n",
      "Iteration 7030 , the loss is currently 0.9916877746582031\n",
      "iteration:  7040\n",
      "Iteration 7040 , the loss is currently 1.012771725654602\n",
      "iteration:  7050\n",
      "Iteration 7050 , the loss is currently 0.9795881509780884\n",
      "iteration:  7060\n",
      "Iteration 7060 , the loss is currently 0.9963527917861938\n",
      "iteration:  7070\n",
      "Iteration 7070 , the loss is currently 1.007643222808838\n",
      "iteration:  7080\n",
      "Iteration 7080 , the loss is currently 0.9895771741867065\n",
      "iteration:  7090\n",
      "Iteration 7090 , the loss is currently 1.00645911693573\n",
      "iteration:  7100\n",
      "Iteration 7100 , the loss is currently 0.994732677936554\n",
      "iteration:  7110\n",
      "Iteration 7110 , the loss is currently 1.0047199726104736\n",
      "iteration:  7120\n",
      "Iteration 7120 , the loss is currently 0.9971989989280701\n",
      "iteration:  7130\n",
      "Iteration 7130 , the loss is currently 0.9986266493797302\n",
      "iteration:  7140\n",
      "Iteration 7140 , the loss is currently 0.9921781420707703\n",
      "iteration:  7150\n",
      "Iteration 7150 , the loss is currently 1.0064373016357422\n",
      "iteration:  7160\n",
      "Iteration 7160 , the loss is currently 0.9832007884979248\n",
      "iteration:  7170\n",
      "Iteration 7170 , the loss is currently 1.0118508338928223\n",
      "iteration:  7180\n",
      "Iteration 7180 , the loss is currently 0.9603745937347412\n",
      "iteration:  7190\n",
      "Iteration 7190 , the loss is currently 0.9918755292892456\n",
      "iteration:  7200\n",
      "Iteration 7200 , the loss is currently 0.9747757315635681\n",
      "iteration:  7210\n",
      "Iteration 7210 , the loss is currently 0.9817997217178345\n",
      "iteration:  7220\n",
      "Iteration 7220 , the loss is currently 0.9908877611160278\n",
      "iteration:  7230\n",
      "Iteration 7230 , the loss is currently 1.00761878490448\n",
      "iteration:  7240\n",
      "Iteration 7240 , the loss is currently 1.0120607614517212\n",
      "iteration:  7250\n",
      "Iteration 7250 , the loss is currently 0.9967441558837891\n",
      "iteration:  7260\n",
      "Iteration 7260 , the loss is currently 1.0037373304367065\n",
      "iteration:  7270\n",
      "Iteration 7270 , the loss is currently 0.9946733117103577\n",
      "iteration:  7280\n",
      "Iteration 7280 , the loss is currently 0.9760028123855591\n",
      "iteration:  7290\n",
      "Iteration 7290 , the loss is currently 0.9943286776542664\n",
      "iteration:  7300\n",
      "Iteration 7300 , the loss is currently 0.97658771276474\n",
      "iteration:  7310\n",
      "Iteration 7310 , the loss is currently 0.9894564747810364\n",
      "iteration:  7320\n",
      "Iteration 7320 , the loss is currently 0.9915919899940491\n",
      "iteration:  7330\n",
      "Iteration 7330 , the loss is currently 0.9796138405799866\n",
      "iteration:  7340\n",
      "Iteration 7340 , the loss is currently 0.9845462441444397\n",
      "iteration:  7350\n",
      "Iteration 7350 , the loss is currently 0.9843040704727173\n",
      "iteration:  7360\n",
      "Iteration 7360 , the loss is currently 0.995166003704071\n",
      "iteration:  7370\n",
      "Iteration 7370 , the loss is currently 1.0036503076553345\n",
      "iteration:  7380\n",
      "Iteration 7380 , the loss is currently 0.9909947514533997\n",
      "iteration:  7390\n",
      "Iteration 7390 , the loss is currently 0.9905474185943604\n",
      "iteration:  7400\n",
      "Iteration 7400 , the loss is currently 1.0070972442626953\n",
      "iteration:  7410\n",
      "Iteration 7410 , the loss is currently 1.0123414993286133\n",
      "iteration:  7420\n",
      "Iteration 7420 , the loss is currently 1.006775140762329\n",
      "iteration:  7430\n",
      "Iteration 7430 , the loss is currently 1.020859718322754\n",
      "iteration:  7440\n",
      "Iteration 7440 , the loss is currently 0.9968395829200745\n",
      "iteration:  7450\n",
      "Iteration 7450 , the loss is currently 0.9994121789932251\n",
      "iteration:  7460\n",
      "Iteration 7460 , the loss is currently 1.0121363401412964\n",
      "iteration:  7470\n",
      "Iteration 7470 , the loss is currently 0.989852786064148\n",
      "iteration:  7480\n",
      "Iteration 7480 , the loss is currently 0.9877763986587524\n",
      "iteration:  7490\n",
      "Iteration 7490 , the loss is currently 1.00345778465271\n",
      "iteration:  7500\n",
      "Iteration 7500 , the loss is currently 1.003899097442627\n",
      "iteration:  7510\n",
      "Iteration 7510 , the loss is currently 1.003564715385437\n",
      "iteration:  7520\n",
      "Iteration 7520 , the loss is currently 0.992099940776825\n",
      "iteration:  7530\n",
      "Iteration 7530 , the loss is currently 0.9927587509155273\n",
      "iteration:  7540\n",
      "Iteration 7540 , the loss is currently 0.9909045100212097\n",
      "iteration:  7550\n",
      "Iteration 7550 , the loss is currently 0.9950206279754639\n",
      "iteration:  7560\n",
      "Iteration 7560 , the loss is currently 0.9810061454772949\n",
      "iteration:  7570\n",
      "Iteration 7570 , the loss is currently 0.9990783929824829\n",
      "iteration:  7580\n",
      "Iteration 7580 , the loss is currently 1.0159995555877686\n",
      "iteration:  7590\n",
      "Iteration 7590 , the loss is currently 0.9932767152786255\n",
      "iteration:  7600\n",
      "Iteration 7600 , the loss is currently 0.9691511392593384\n",
      "iteration:  7610\n",
      "Iteration 7610 , the loss is currently 0.970194935798645\n",
      "iteration:  7620\n",
      "Iteration 7620 , the loss is currently 0.995124101638794\n",
      "iteration:  7630\n",
      "Iteration 7630 , the loss is currently 0.9879912734031677\n",
      "iteration:  7640\n",
      "Iteration 7640 , the loss is currently 0.9846332669258118\n",
      "iteration:  7650\n",
      "Iteration 7650 , the loss is currently 0.9935977458953857\n",
      "iteration:  7660\n",
      "Iteration 7660 , the loss is currently 0.9899122714996338\n",
      "iteration:  7670\n",
      "Iteration 7670 , the loss is currently 0.9886142611503601\n",
      "iteration:  7680\n",
      "Iteration 7680 , the loss is currently 0.9708113074302673\n",
      "iteration:  7690\n",
      "Iteration 7690 , the loss is currently 0.9887458086013794\n",
      "iteration:  7700\n",
      "Iteration 7700 , the loss is currently 1.0054614543914795\n",
      "iteration:  7710\n",
      "Iteration 7710 , the loss is currently 0.9974601864814758\n",
      "iteration:  7720\n",
      "Iteration 7720 , the loss is currently 0.9862239956855774\n",
      "iteration:  7730\n",
      "Iteration 7730 , the loss is currently 0.9968923330307007\n",
      "iteration:  7740\n",
      "Iteration 7740 , the loss is currently 1.0127067565917969\n",
      "iteration:  7750\n",
      "Iteration 7750 , the loss is currently 0.9847493171691895\n",
      "iteration:  7760\n",
      "Iteration 7760 , the loss is currently 1.00824773311615\n",
      "iteration:  7770\n",
      "Iteration 7770 , the loss is currently 0.9909769892692566\n",
      "iteration:  7780\n",
      "Iteration 7780 , the loss is currently 0.9937520027160645\n",
      "iteration:  7790\n",
      "Iteration 7790 , the loss is currently 0.9949777722358704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  7800\n",
      "Iteration 7800 , the loss is currently 0.9724952578544617\n",
      "iteration:  7810\n",
      "Iteration 7810 , the loss is currently 0.9944964051246643\n",
      "iteration:  7820\n",
      "Iteration 7820 , the loss is currently 0.9892306327819824\n",
      "iteration:  7830\n",
      "Iteration 7830 , the loss is currently 0.9857139587402344\n",
      "iteration:  7840\n",
      "Iteration 7840 , the loss is currently 1.0018149614334106\n",
      "iteration:  7850\n",
      "Iteration 7850 , the loss is currently 0.997289776802063\n",
      "iteration:  7860\n",
      "Iteration 7860 , the loss is currently 0.994086503982544\n",
      "iteration:  7870\n",
      "Iteration 7870 , the loss is currently 1.0099509954452515\n",
      "iteration:  7880\n",
      "Iteration 7880 , the loss is currently 0.9739608764648438\n",
      "iteration:  7890\n",
      "Iteration 7890 , the loss is currently 0.9926446676254272\n",
      "iteration:  7900\n",
      "Iteration 7900 , the loss is currently 0.9687709212303162\n",
      "iteration:  7910\n",
      "Iteration 7910 , the loss is currently 0.9847828149795532\n",
      "iteration:  7920\n",
      "Iteration 7920 , the loss is currently 0.9779659509658813\n",
      "iteration:  7930\n",
      "Iteration 7930 , the loss is currently 0.9808741211891174\n",
      "iteration:  7940\n",
      "Iteration 7940 , the loss is currently 0.9761467576026917\n",
      "iteration:  7950\n",
      "Iteration 7950 , the loss is currently 0.9750808477401733\n",
      "iteration:  7960\n",
      "Iteration 7960 , the loss is currently 0.9842352867126465\n",
      "iteration:  7970\n",
      "Iteration 7970 , the loss is currently 0.9914857149124146\n",
      "iteration:  7980\n",
      "Iteration 7980 , the loss is currently 0.9945608377456665\n",
      "iteration:  7990\n",
      "Iteration 7990 , the loss is currently 0.9850606918334961\n",
      "iteration:  8000\n",
      "Iteration 8000 , the loss is currently 0.9844374060630798\n",
      "iteration:  8010\n",
      "Iteration 8010 , the loss is currently 0.9897021055221558\n",
      "iteration:  8020\n",
      "Iteration 8020 , the loss is currently 0.9985516667366028\n",
      "iteration:  8030\n",
      "Iteration 8030 , the loss is currently 0.9882548451423645\n",
      "iteration:  8040\n",
      "Iteration 8040 , the loss is currently 0.988933265209198\n",
      "iteration:  8050\n",
      "Iteration 8050 , the loss is currently 1.0176589488983154\n",
      "iteration:  8060\n",
      "Iteration 8060 , the loss is currently 0.9955230355262756\n",
      "iteration:  8070\n",
      "Iteration 8070 , the loss is currently 1.0110819339752197\n",
      "iteration:  8080\n",
      "Iteration 8080 , the loss is currently 0.994795024394989\n",
      "iteration:  8090\n",
      "Iteration 8090 , the loss is currently 0.9938923716545105\n",
      "iteration:  8100\n",
      "Iteration 8100 , the loss is currently 0.9817277193069458\n",
      "iteration:  8110\n",
      "Iteration 8110 , the loss is currently 0.9949098229408264\n",
      "iteration:  8120\n",
      "Iteration 8120 , the loss is currently 0.992296040058136\n",
      "iteration:  8130\n",
      "Iteration 8130 , the loss is currently 0.9851346611976624\n",
      "iteration:  8140\n",
      "Iteration 8140 , the loss is currently 1.0177303552627563\n",
      "iteration:  8150\n",
      "Iteration 8150 , the loss is currently 0.9874407052993774\n",
      "iteration:  8160\n",
      "Iteration 8160 , the loss is currently 1.0055557489395142\n",
      "iteration:  8170\n",
      "Iteration 8170 , the loss is currently 1.0034672021865845\n",
      "iteration:  8180\n",
      "Iteration 8180 , the loss is currently 0.9915817379951477\n",
      "iteration:  8190\n",
      "Iteration 8190 , the loss is currently 0.9979835152626038\n",
      "iteration:  8200\n",
      "Iteration 8200 , the loss is currently 0.9978303909301758\n",
      "iteration:  8210\n",
      "Iteration 8210 , the loss is currently 0.9961652755737305\n",
      "iteration:  8220\n",
      "Iteration 8220 , the loss is currently 0.968827486038208\n",
      "iteration:  8230\n",
      "Iteration 8230 , the loss is currently 0.9779787063598633\n",
      "iteration:  8240\n",
      "Iteration 8240 , the loss is currently 0.985673189163208\n",
      "iteration:  8250\n",
      "Iteration 8250 , the loss is currently 0.9975817203521729\n",
      "iteration:  8260\n",
      "Iteration 8260 , the loss is currently 0.9802259206771851\n",
      "iteration:  8270\n",
      "Iteration 8270 , the loss is currently 1.0126779079437256\n",
      "iteration:  8280\n",
      "Iteration 8280 , the loss is currently 0.9957688450813293\n",
      "iteration:  8290\n",
      "Iteration 8290 , the loss is currently 1.0034805536270142\n",
      "iteration:  8300\n",
      "Iteration 8300 , the loss is currently 0.9667583107948303\n",
      "iteration:  8310\n",
      "Iteration 8310 , the loss is currently 1.0100789070129395\n",
      "iteration:  8320\n",
      "Iteration 8320 , the loss is currently 0.9915708303451538\n",
      "iteration:  8330\n",
      "Iteration 8330 , the loss is currently 0.9950283765792847\n",
      "iteration:  8340\n",
      "Iteration 8340 , the loss is currently 0.9939230680465698\n",
      "iteration:  8350\n",
      "Iteration 8350 , the loss is currently 1.0104162693023682\n",
      "iteration:  8360\n",
      "Iteration 8360 , the loss is currently 0.9787214994430542\n",
      "iteration:  8370\n",
      "Iteration 8370 , the loss is currently 0.9759302735328674\n",
      "iteration:  8380\n",
      "Iteration 8380 , the loss is currently 1.0149681568145752\n",
      "iteration:  8390\n",
      "Iteration 8390 , the loss is currently 0.9840790629386902\n",
      "iteration:  8400\n",
      "Iteration 8400 , the loss is currently 0.9814904928207397\n",
      "iteration:  8410\n",
      "Iteration 8410 , the loss is currently 0.9867310523986816\n",
      "iteration:  8420\n",
      "Iteration 8420 , the loss is currently 0.9921596050262451\n",
      "iteration:  8430\n",
      "Iteration 8430 , the loss is currently 0.9742766618728638\n",
      "iteration:  8440\n",
      "Iteration 8440 , the loss is currently 1.0088311433792114\n",
      "iteration:  8450\n",
      "Iteration 8450 , the loss is currently 0.9954017400741577\n",
      "iteration:  8460\n",
      "Iteration 8460 , the loss is currently 0.9986315965652466\n",
      "iteration:  8470\n",
      "Iteration 8470 , the loss is currently 1.0003461837768555\n",
      "iteration:  8480\n",
      "Iteration 8480 , the loss is currently 0.9859055280685425\n",
      "iteration:  8490\n",
      "Iteration 8490 , the loss is currently 0.9811902642250061\n",
      "iteration:  8500\n",
      "Iteration 8500 , the loss is currently 0.9828078746795654\n",
      "iteration:  8510\n",
      "Iteration 8510 , the loss is currently 0.9856430888175964\n",
      "iteration:  8520\n",
      "Iteration 8520 , the loss is currently 0.9864220023155212\n",
      "iteration:  8530\n",
      "Iteration 8530 , the loss is currently 0.9799503684043884\n",
      "iteration:  8540\n",
      "Iteration 8540 , the loss is currently 0.997403621673584\n",
      "iteration:  8550\n",
      "Iteration 8550 , the loss is currently 0.9830920100212097\n",
      "iteration:  8560\n",
      "Iteration 8560 , the loss is currently 0.9956637620925903\n",
      "iteration:  8570\n",
      "Iteration 8570 , the loss is currently 0.9716187119483948\n",
      "iteration:  8580\n",
      "Iteration 8580 , the loss is currently 0.9691329002380371\n",
      "iteration:  8590\n",
      "Iteration 8590 , the loss is currently 0.976604163646698\n",
      "iteration:  8600\n",
      "Iteration 8600 , the loss is currently 0.9761554002761841\n",
      "iteration:  8610\n",
      "Iteration 8610 , the loss is currently 0.9986138343811035\n",
      "iteration:  8620\n",
      "Iteration 8620 , the loss is currently 0.972357988357544\n",
      "iteration:  8630\n",
      "Iteration 8630 , the loss is currently 0.9728356599807739\n",
      "iteration:  8640\n",
      "Iteration 8640 , the loss is currently 1.0002455711364746\n",
      "iteration:  8650\n",
      "Iteration 8650 , the loss is currently 0.9838463068008423\n",
      "iteration:  8660\n",
      "Iteration 8660 , the loss is currently 1.0018638372421265\n",
      "iteration:  8670\n",
      "Iteration 8670 , the loss is currently 1.0032695531845093\n",
      "iteration:  8680\n",
      "Iteration 8680 , the loss is currently 0.9847660660743713\n",
      "iteration:  8690\n",
      "Iteration 8690 , the loss is currently 0.958382785320282\n",
      "iteration:  8700\n",
      "Iteration 8700 , the loss is currently 0.9874942302703857\n",
      "iteration:  8710\n",
      "Iteration 8710 , the loss is currently 0.9913316965103149\n",
      "iteration:  8720\n",
      "Iteration 8720 , the loss is currently 0.9908726811408997\n",
      "iteration:  8730\n",
      "Iteration 8730 , the loss is currently 1.003532886505127\n",
      "iteration:  8740\n",
      "Iteration 8740 , the loss is currently 0.9795088171958923\n",
      "iteration:  8750\n",
      "Iteration 8750 , the loss is currently 0.9960885643959045\n",
      "iteration:  8760\n",
      "Iteration 8760 , the loss is currently 0.9943971037864685\n",
      "iteration:  8770\n",
      "Iteration 8770 , the loss is currently 0.984254777431488\n",
      "iteration:  8780\n",
      "Iteration 8780 , the loss is currently 1.0045530796051025\n",
      "iteration:  8790\n",
      "Iteration 8790 , the loss is currently 1.007357120513916\n",
      "iteration:  8800\n",
      "Iteration 8800 , the loss is currently 0.995732843875885\n",
      "iteration:  8810\n",
      "Iteration 8810 , the loss is currently 0.9842783212661743\n",
      "iteration:  8820\n",
      "Iteration 8820 , the loss is currently 0.9938869476318359\n",
      "iteration:  8830\n",
      "Iteration 8830 , the loss is currently 0.9921597838401794\n",
      "iteration:  8840\n",
      "Iteration 8840 , the loss is currently 0.9940937757492065\n",
      "iteration:  8850\n",
      "Iteration 8850 , the loss is currently 0.9907901287078857\n",
      "iteration:  8860\n",
      "Iteration 8860 , the loss is currently 1.003568410873413\n",
      "iteration:  8870\n",
      "Iteration 8870 , the loss is currently 0.9909371733665466\n",
      "iteration:  8880\n",
      "Iteration 8880 , the loss is currently 0.9992457628250122\n",
      "iteration:  8890\n",
      "Iteration 8890 , the loss is currently 0.9858046174049377\n",
      "iteration:  8900\n",
      "Iteration 8900 , the loss is currently 0.992562472820282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  8910\n",
      "Iteration 8910 , the loss is currently 0.997951090335846\n",
      "iteration:  8920\n",
      "Iteration 8920 , the loss is currently 1.0111066102981567\n",
      "iteration:  8930\n",
      "Iteration 8930 , the loss is currently 0.9950307011604309\n",
      "iteration:  8940\n",
      "Iteration 8940 , the loss is currently 0.9838929176330566\n",
      "iteration:  8950\n",
      "Iteration 8950 , the loss is currently 0.9857507944107056\n",
      "iteration:  8960\n",
      "Iteration 8960 , the loss is currently 1.0230776071548462\n",
      "iteration:  8970\n",
      "Iteration 8970 , the loss is currently 0.986275851726532\n",
      "iteration:  8980\n",
      "Iteration 8980 , the loss is currently 0.9959891438484192\n",
      "iteration:  8990\n",
      "Iteration 8990 , the loss is currently 0.98146653175354\n",
      "iteration:  9000\n",
      "Iteration 9000 , the loss is currently 1.0016330480575562\n",
      "iteration:  9010\n",
      "Iteration 9010 , the loss is currently 0.985580563545227\n",
      "iteration:  9020\n",
      "Iteration 9020 , the loss is currently 0.9920894503593445\n",
      "iteration:  9030\n",
      "Iteration 9030 , the loss is currently 0.9892982840538025\n",
      "iteration:  9040\n",
      "Iteration 9040 , the loss is currently 0.9857757091522217\n",
      "iteration:  9050\n",
      "Iteration 9050 , the loss is currently 0.995689332485199\n",
      "iteration:  9060\n",
      "Iteration 9060 , the loss is currently 1.0088272094726562\n",
      "iteration:  9070\n",
      "Iteration 9070 , the loss is currently 1.0050019025802612\n",
      "iteration:  9080\n",
      "Iteration 9080 , the loss is currently 0.9871498942375183\n",
      "iteration:  9090\n",
      "Iteration 9090 , the loss is currently 0.9870307445526123\n",
      "iteration:  9100\n",
      "Iteration 9100 , the loss is currently 0.9920358657836914\n",
      "iteration:  9110\n",
      "Iteration 9110 , the loss is currently 0.993542492389679\n",
      "iteration:  9120\n",
      "Iteration 9120 , the loss is currently 0.9826294779777527\n",
      "iteration:  9130\n",
      "Iteration 9130 , the loss is currently 0.9688189625740051\n",
      "iteration:  9140\n",
      "Iteration 9140 , the loss is currently 1.0120587348937988\n",
      "iteration:  9150\n",
      "Iteration 9150 , the loss is currently 0.9558806419372559\n",
      "iteration:  9160\n",
      "Iteration 9160 , the loss is currently 0.991456151008606\n",
      "iteration:  9170\n",
      "Iteration 9170 , the loss is currently 0.9731526970863342\n",
      "iteration:  9180\n",
      "Iteration 9180 , the loss is currently 1.0056889057159424\n",
      "iteration:  9190\n",
      "Iteration 9190 , the loss is currently 0.9760593771934509\n",
      "iteration:  9200\n",
      "Iteration 9200 , the loss is currently 1.0026193857192993\n",
      "iteration:  9210\n",
      "Iteration 9210 , the loss is currently 0.9873299598693848\n",
      "iteration:  9220\n",
      "Iteration 9220 , the loss is currently 0.997166633605957\n",
      "iteration:  9230\n",
      "Iteration 9230 , the loss is currently 1.007220983505249\n",
      "iteration:  9240\n",
      "Iteration 9240 , the loss is currently 0.9860380291938782\n",
      "iteration:  9250\n",
      "Iteration 9250 , the loss is currently 0.9934431910514832\n",
      "iteration:  9260\n",
      "Iteration 9260 , the loss is currently 1.000030517578125\n",
      "iteration:  9270\n",
      "Iteration 9270 , the loss is currently 0.9955023527145386\n",
      "iteration:  9280\n",
      "Iteration 9280 , the loss is currently 0.9989528656005859\n",
      "iteration:  9290\n",
      "Iteration 9290 , the loss is currently 0.9921299815177917\n",
      "iteration:  9300\n",
      "Iteration 9300 , the loss is currently 1.0012768507003784\n",
      "iteration:  9310\n",
      "Iteration 9310 , the loss is currently 0.9780880808830261\n",
      "iteration:  9320\n",
      "Iteration 9320 , the loss is currently 0.9920493364334106\n",
      "iteration:  9330\n",
      "Iteration 9330 , the loss is currently 0.9987770318984985\n",
      "iteration:  9340\n",
      "Iteration 9340 , the loss is currently 0.9650298953056335\n",
      "iteration:  9350\n",
      "Iteration 9350 , the loss is currently 0.9694188237190247\n",
      "iteration:  9360\n",
      "Iteration 9360 , the loss is currently 1.007510781288147\n",
      "iteration:  9370\n",
      "Iteration 9370 , the loss is currently 0.9878780841827393\n",
      "iteration:  9380\n",
      "Iteration 9380 , the loss is currently 0.9981656074523926\n",
      "iteration:  9390\n",
      "Iteration 9390 , the loss is currently 0.9960523247718811\n",
      "iteration:  9400\n",
      "Iteration 9400 , the loss is currently 0.9767831563949585\n",
      "iteration:  9410\n",
      "Iteration 9410 , the loss is currently 0.9979372024536133\n",
      "iteration:  9420\n",
      "Iteration 9420 , the loss is currently 0.9898831844329834\n",
      "iteration:  9430\n",
      "Iteration 9430 , the loss is currently 0.998771071434021\n",
      "iteration:  9440\n",
      "Iteration 9440 , the loss is currently 0.9921552538871765\n",
      "iteration:  9450\n",
      "Iteration 9450 , the loss is currently 0.9779046773910522\n",
      "iteration:  9460\n",
      "Iteration 9460 , the loss is currently 0.9909127950668335\n",
      "iteration:  9470\n",
      "Iteration 9470 , the loss is currently 0.9882311224937439\n",
      "iteration:  9480\n",
      "Iteration 9480 , the loss is currently 0.9925006031990051\n",
      "iteration:  9490\n",
      "Iteration 9490 , the loss is currently 0.9899753928184509\n",
      "iteration:  9500\n",
      "Iteration 9500 , the loss is currently 0.980886697769165\n",
      "iteration:  9510\n",
      "Iteration 9510 , the loss is currently 0.9670904278755188\n",
      "iteration:  9520\n",
      "Iteration 9520 , the loss is currently 0.9858520030975342\n",
      "iteration:  9530\n",
      "Iteration 9530 , the loss is currently 0.9833124876022339\n",
      "iteration:  9540\n",
      "Iteration 9540 , the loss is currently 0.9960757493972778\n",
      "iteration:  9550\n",
      "Iteration 9550 , the loss is currently 0.9885572195053101\n",
      "iteration:  9560\n",
      "Iteration 9560 , the loss is currently 0.9890260696411133\n",
      "iteration:  9570\n",
      "Iteration 9570 , the loss is currently 1.004883885383606\n",
      "iteration:  9580\n",
      "Iteration 9580 , the loss is currently 0.9827927350997925\n",
      "iteration:  9590\n",
      "Iteration 9590 , the loss is currently 1.0077553987503052\n",
      "iteration:  9600\n",
      "Iteration 9600 , the loss is currently 0.9749143123626709\n",
      "iteration:  9610\n",
      "Iteration 9610 , the loss is currently 0.9829913973808289\n",
      "iteration:  9620\n",
      "Iteration 9620 , the loss is currently 0.9895262122154236\n",
      "iteration:  9630\n",
      "Iteration 9630 , the loss is currently 0.9861116409301758\n",
      "iteration:  9640\n",
      "Iteration 9640 , the loss is currently 0.9896095991134644\n",
      "iteration:  9650\n",
      "Iteration 9650 , the loss is currently 0.9844330549240112\n",
      "iteration:  9660\n",
      "Iteration 9660 , the loss is currently 0.9783555865287781\n",
      "iteration:  9670\n",
      "Iteration 9670 , the loss is currently 0.9996216297149658\n",
      "iteration:  9680\n",
      "Iteration 9680 , the loss is currently 0.9823974370956421\n",
      "iteration:  9690\n",
      "Iteration 9690 , the loss is currently 0.977290689945221\n",
      "iteration:  9700\n",
      "Iteration 9700 , the loss is currently 0.9918070435523987\n",
      "iteration:  9710\n",
      "Iteration 9710 , the loss is currently 1.0080095529556274\n",
      "iteration:  9720\n",
      "Iteration 9720 , the loss is currently 0.9899430871009827\n",
      "iteration:  9730\n",
      "Iteration 9730 , the loss is currently 0.9704594612121582\n",
      "iteration:  9740\n",
      "Iteration 9740 , the loss is currently 0.9958227872848511\n",
      "iteration:  9750\n",
      "Iteration 9750 , the loss is currently 0.9987884163856506\n",
      "iteration:  9760\n",
      "Iteration 9760 , the loss is currently 1.0111098289489746\n",
      "iteration:  9770\n",
      "Iteration 9770 , the loss is currently 1.0009915828704834\n",
      "iteration:  9780\n",
      "Iteration 9780 , the loss is currently 0.9918925762176514\n",
      "iteration:  9790\n",
      "Iteration 9790 , the loss is currently 0.981119692325592\n",
      "iteration:  9800\n",
      "Iteration 9800 , the loss is currently 0.9857116341590881\n",
      "iteration:  9810\n",
      "Iteration 9810 , the loss is currently 1.002724289894104\n",
      "iteration:  9820\n",
      "Iteration 9820 , the loss is currently 0.9814502596855164\n",
      "iteration:  9830\n",
      "Iteration 9830 , the loss is currently 0.9760772585868835\n",
      "iteration:  9840\n",
      "Iteration 9840 , the loss is currently 1.0072141885757446\n",
      "iteration:  9850\n",
      "Iteration 9850 , the loss is currently 0.9763235449790955\n",
      "iteration:  9860\n",
      "Iteration 9860 , the loss is currently 0.9840955138206482\n",
      "iteration:  9870\n",
      "Iteration 9870 , the loss is currently 0.9732435941696167\n",
      "iteration:  9880\n",
      "Iteration 9880 , the loss is currently 0.981081485748291\n",
      "iteration:  9890\n",
      "Iteration 9890 , the loss is currently 0.9937009811401367\n",
      "iteration:  9900\n",
      "Iteration 9900 , the loss is currently 1.0005667209625244\n",
      "iteration:  9910\n",
      "Iteration 9910 , the loss is currently 0.9830042719841003\n",
      "iteration:  9920\n",
      "Iteration 9920 , the loss is currently 0.9809771180152893\n",
      "iteration:  9930\n",
      "Iteration 9930 , the loss is currently 0.9888744950294495\n",
      "iteration:  9940\n",
      "Iteration 9940 , the loss is currently 0.9965848326683044\n",
      "iteration:  9950\n",
      "Iteration 9950 , the loss is currently 1.0002413988113403\n",
      "iteration:  9960\n",
      "Iteration 9960 , the loss is currently 0.9788641929626465\n",
      "iteration:  9970\n",
      "Iteration 9970 , the loss is currently 1.0108225345611572\n",
      "iteration:  9980\n",
      "Iteration 9980 , the loss is currently 0.9934590458869934\n",
      "iteration:  9990\n",
      "Iteration 9990 , the loss is currently 1.0015944242477417\n",
      "iteration:  10000\n",
      "Iteration 10000 , the loss is currently 0.9870343208312988\n",
      "iteration:  10010\n",
      "Iteration 10010 , the loss is currently 0.9881237149238586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  10020\n",
      "Iteration 10020 , the loss is currently 0.9777771234512329\n",
      "iteration:  10030\n",
      "Iteration 10030 , the loss is currently 0.9861372709274292\n",
      "iteration:  10040\n",
      "Iteration 10040 , the loss is currently 0.9717156887054443\n",
      "iteration:  10050\n",
      "Iteration 10050 , the loss is currently 1.0182220935821533\n",
      "iteration:  10060\n",
      "Iteration 10060 , the loss is currently 0.998673677444458\n",
      "iteration:  10070\n",
      "Iteration 10070 , the loss is currently 1.0169265270233154\n",
      "iteration:  10080\n",
      "Iteration 10080 , the loss is currently 0.9837365746498108\n",
      "iteration:  10090\n",
      "Iteration 10090 , the loss is currently 0.9962963461875916\n",
      "iteration:  10100\n",
      "Iteration 10100 , the loss is currently 0.9900927543640137\n",
      "iteration:  10110\n",
      "Iteration 10110 , the loss is currently 0.9956473708152771\n",
      "iteration:  10120\n",
      "Iteration 10120 , the loss is currently 0.9796669483184814\n",
      "iteration:  10130\n",
      "Iteration 10130 , the loss is currently 0.9772153496742249\n",
      "iteration:  10140\n",
      "Iteration 10140 , the loss is currently 0.9668422341346741\n",
      "iteration:  10150\n",
      "Iteration 10150 , the loss is currently 0.9904056191444397\n",
      "iteration:  10160\n",
      "Iteration 10160 , the loss is currently 1.0014421939849854\n",
      "iteration:  10170\n",
      "Iteration 10170 , the loss is currently 1.0143054723739624\n",
      "iteration:  10180\n",
      "Iteration 10180 , the loss is currently 0.9707491993904114\n",
      "iteration:  10190\n",
      "Iteration 10190 , the loss is currently 0.972830593585968\n",
      "iteration:  10200\n",
      "Iteration 10200 , the loss is currently 1.0035439729690552\n",
      "iteration:  10210\n",
      "Iteration 10210 , the loss is currently 1.0200029611587524\n",
      "iteration:  10220\n",
      "Iteration 10220 , the loss is currently 0.9953978657722473\n",
      "iteration:  10230\n",
      "Iteration 10230 , the loss is currently 0.9977436661720276\n",
      "iteration:  10240\n",
      "Iteration 10240 , the loss is currently 0.9942601919174194\n",
      "iteration:  10250\n",
      "Iteration 10250 , the loss is currently 0.9817652106285095\n",
      "iteration:  10260\n",
      "Iteration 10260 , the loss is currently 0.989361047744751\n",
      "iteration:  10270\n",
      "Iteration 10270 , the loss is currently 0.9664532542228699\n",
      "iteration:  10280\n",
      "Iteration 10280 , the loss is currently 1.0110965967178345\n",
      "iteration:  10290\n",
      "Iteration 10290 , the loss is currently 1.0004167556762695\n",
      "iteration:  10300\n",
      "Iteration 10300 , the loss is currently 1.005478858947754\n",
      "iteration:  10310\n",
      "Iteration 10310 , the loss is currently 1.0000088214874268\n",
      "iteration:  10320\n",
      "Iteration 10320 , the loss is currently 0.9832728505134583\n",
      "iteration:  10330\n",
      "Iteration 10330 , the loss is currently 0.9833113551139832\n",
      "iteration:  10340\n",
      "Iteration 10340 , the loss is currently 1.0028184652328491\n",
      "iteration:  10350\n",
      "Iteration 10350 , the loss is currently 0.9785681366920471\n",
      "iteration:  10360\n",
      "Iteration 10360 , the loss is currently 0.9861419796943665\n",
      "iteration:  10370\n",
      "Iteration 10370 , the loss is currently 0.9986164569854736\n",
      "iteration:  10380\n",
      "Iteration 10380 , the loss is currently 0.9858001470565796\n",
      "iteration:  10390\n",
      "Iteration 10390 , the loss is currently 0.9807875156402588\n",
      "iteration:  10400\n",
      "Iteration 10400 , the loss is currently 1.0069276094436646\n",
      "iteration:  10410\n",
      "Iteration 10410 , the loss is currently 0.9875941872596741\n",
      "iteration:  10420\n",
      "Iteration 10420 , the loss is currently 0.990570068359375\n",
      "iteration:  10430\n",
      "Iteration 10430 , the loss is currently 0.973080575466156\n",
      "iteration:  10440\n",
      "Iteration 10440 , the loss is currently 0.9755733609199524\n",
      "iteration:  10450\n",
      "Iteration 10450 , the loss is currently 0.99345463514328\n",
      "iteration:  10460\n",
      "Iteration 10460 , the loss is currently 0.9885608553886414\n",
      "iteration:  10470\n",
      "Iteration 10470 , the loss is currently 0.966935932636261\n",
      "iteration:  10480\n",
      "Iteration 10480 , the loss is currently 0.9865551590919495\n",
      "iteration:  10490\n",
      "Iteration 10490 , the loss is currently 0.9922608733177185\n",
      "iteration:  10500\n",
      "Iteration 10500 , the loss is currently 0.9827354550361633\n",
      "iteration:  10510\n",
      "Iteration 10510 , the loss is currently 0.998806893825531\n",
      "iteration:  10520\n",
      "Iteration 10520 , the loss is currently 0.99703049659729\n",
      "iteration:  10530\n",
      "Iteration 10530 , the loss is currently 0.9967917799949646\n",
      "iteration:  10540\n",
      "Iteration 10540 , the loss is currently 1.013066291809082\n",
      "iteration:  10550\n",
      "Iteration 10550 , the loss is currently 1.0039969682693481\n",
      "iteration:  10560\n",
      "Iteration 10560 , the loss is currently 0.976131021976471\n",
      "iteration:  10570\n",
      "Iteration 10570 , the loss is currently 0.9850247502326965\n",
      "iteration:  10580\n",
      "Iteration 10580 , the loss is currently 0.9861682057380676\n",
      "iteration:  10590\n",
      "Iteration 10590 , the loss is currently 0.99977707862854\n",
      "iteration:  10600\n",
      "Iteration 10600 , the loss is currently 0.985368549823761\n",
      "iteration:  10610\n",
      "Iteration 10610 , the loss is currently 1.0049917697906494\n",
      "iteration:  10620\n",
      "Iteration 10620 , the loss is currently 1.006631851196289\n",
      "iteration:  10630\n",
      "Iteration 10630 , the loss is currently 1.011017084121704\n",
      "iteration:  10640\n",
      "Iteration 10640 , the loss is currently 1.005708932876587\n",
      "iteration:  10650\n",
      "Iteration 10650 , the loss is currently 0.9902560114860535\n",
      "iteration:  10660\n",
      "Iteration 10660 , the loss is currently 0.9613660573959351\n",
      "iteration:  10670\n",
      "Iteration 10670 , the loss is currently 0.9783339500427246\n",
      "iteration:  10680\n",
      "Iteration 10680 , the loss is currently 0.9918697476387024\n",
      "iteration:  10690\n",
      "Iteration 10690 , the loss is currently 0.9826616644859314\n",
      "iteration:  10700\n",
      "Iteration 10700 , the loss is currently 1.0067973136901855\n",
      "iteration:  10710\n",
      "Iteration 10710 , the loss is currently 0.9962096810340881\n",
      "iteration:  10720\n",
      "Iteration 10720 , the loss is currently 0.9890202879905701\n",
      "iteration:  10730\n",
      "Iteration 10730 , the loss is currently 0.9812537431716919\n",
      "iteration:  10740\n",
      "Iteration 10740 , the loss is currently 1.0020557641983032\n",
      "iteration:  10750\n",
      "Iteration 10750 , the loss is currently 1.0070571899414062\n",
      "iteration:  10760\n",
      "Iteration 10760 , the loss is currently 1.01055908203125\n",
      "iteration:  10770\n",
      "Iteration 10770 , the loss is currently 0.9884904026985168\n",
      "iteration:  10780\n",
      "Iteration 10780 , the loss is currently 0.9871249198913574\n",
      "iteration:  10790\n",
      "Iteration 10790 , the loss is currently 0.9950994253158569\n",
      "iteration:  10800\n",
      "Iteration 10800 , the loss is currently 0.9842962622642517\n",
      "iteration:  10810\n",
      "Iteration 10810 , the loss is currently 1.0070269107818604\n",
      "iteration:  10820\n",
      "Iteration 10820 , the loss is currently 0.9969096779823303\n",
      "iteration:  10830\n",
      "Iteration 10830 , the loss is currently 0.9694221615791321\n",
      "iteration:  10840\n",
      "Iteration 10840 , the loss is currently 0.9745087027549744\n",
      "iteration:  10850\n",
      "Iteration 10850 , the loss is currently 1.0076407194137573\n",
      "iteration:  10860\n",
      "Iteration 10860 , the loss is currently 0.995487630367279\n",
      "iteration:  10870\n",
      "Iteration 10870 , the loss is currently 0.9831681847572327\n",
      "iteration:  10880\n",
      "Iteration 10880 , the loss is currently 0.9911432266235352\n",
      "iteration:  10890\n",
      "Iteration 10890 , the loss is currently 0.9929786920547485\n",
      "iteration:  10900\n",
      "Iteration 10900 , the loss is currently 0.991002082824707\n",
      "iteration:  10910\n",
      "Iteration 10910 , the loss is currently 0.9897586703300476\n",
      "iteration:  10920\n",
      "Iteration 10920 , the loss is currently 0.9919806122779846\n",
      "iteration:  10930\n",
      "Iteration 10930 , the loss is currently 0.996178925037384\n",
      "iteration:  10940\n",
      "Iteration 10940 , the loss is currently 1.0086948871612549\n",
      "iteration:  10950\n",
      "Iteration 10950 , the loss is currently 0.9808603525161743\n",
      "iteration:  10960\n",
      "Iteration 10960 , the loss is currently 0.9872017502784729\n",
      "iteration:  10970\n",
      "Iteration 10970 , the loss is currently 0.9808034896850586\n",
      "iteration:  10980\n",
      "Iteration 10980 , the loss is currently 0.9990142583847046\n",
      "iteration:  10990\n",
      "Iteration 10990 , the loss is currently 0.9694243669509888\n",
      "iteration:  11000\n",
      "Iteration 11000 , the loss is currently 0.9831631183624268\n",
      "iteration:  11010\n",
      "Iteration 11010 , the loss is currently 1.0027836561203003\n",
      "iteration:  11020\n",
      "Iteration 11020 , the loss is currently 0.9603719115257263\n",
      "iteration:  11030\n",
      "Iteration 11030 , the loss is currently 0.994052529335022\n",
      "iteration:  11040\n",
      "Iteration 11040 , the loss is currently 0.9850592613220215\n",
      "iteration:  11050\n",
      "Iteration 11050 , the loss is currently 0.9947072267532349\n",
      "iteration:  11060\n",
      "Iteration 11060 , the loss is currently 1.0044631958007812\n",
      "iteration:  11070\n",
      "Iteration 11070 , the loss is currently 0.9951208829879761\n",
      "iteration:  11080\n",
      "Iteration 11080 , the loss is currently 0.983173131942749\n",
      "iteration:  11090\n",
      "Iteration 11090 , the loss is currently 0.9839398860931396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  11100\n",
      "Iteration 11100 , the loss is currently 0.9866983890533447\n",
      "iteration:  11110\n",
      "Iteration 11110 , the loss is currently 0.9720643758773804\n",
      "iteration:  11120\n",
      "Iteration 11120 , the loss is currently 0.9827709197998047\n",
      "iteration:  11130\n",
      "Iteration 11130 , the loss is currently 0.9870912432670593\n",
      "iteration:  11140\n",
      "Iteration 11140 , the loss is currently 0.9874051213264465\n",
      "iteration:  11150\n",
      "Iteration 11150 , the loss is currently 0.9844852685928345\n",
      "iteration:  11160\n",
      "Iteration 11160 , the loss is currently 0.9901267886161804\n",
      "iteration:  11170\n",
      "Iteration 11170 , the loss is currently 0.9731242656707764\n",
      "iteration:  11180\n",
      "Iteration 11180 , the loss is currently 0.9964451193809509\n",
      "iteration:  11190\n",
      "Iteration 11190 , the loss is currently 0.9895825982093811\n",
      "iteration:  11200\n",
      "Iteration 11200 , the loss is currently 1.0048234462738037\n",
      "iteration:  11210\n",
      "Iteration 11210 , the loss is currently 0.9921010136604309\n",
      "iteration:  11220\n",
      "Iteration 11220 , the loss is currently 0.9872345924377441\n",
      "iteration:  11230\n",
      "Iteration 11230 , the loss is currently 0.9914226531982422\n",
      "iteration:  11240\n",
      "Iteration 11240 , the loss is currently 0.9722630977630615\n",
      "iteration:  11250\n",
      "Iteration 11250 , the loss is currently 0.9905558228492737\n",
      "iteration:  11260\n",
      "Iteration 11260 , the loss is currently 1.0122301578521729\n",
      "iteration:  11270\n",
      "Iteration 11270 , the loss is currently 0.9704533815383911\n",
      "iteration:  11280\n",
      "Iteration 11280 , the loss is currently 1.002116084098816\n",
      "iteration:  11290\n",
      "Iteration 11290 , the loss is currently 0.9999696016311646\n",
      "iteration:  11300\n",
      "Iteration 11300 , the loss is currently 1.0033295154571533\n",
      "iteration:  11310\n",
      "Iteration 11310 , the loss is currently 0.9961864948272705\n",
      "iteration:  11320\n",
      "Iteration 11320 , the loss is currently 1.0054988861083984\n",
      "iteration:  11330\n",
      "Iteration 11330 , the loss is currently 0.9762892723083496\n",
      "iteration:  11340\n",
      "Iteration 11340 , the loss is currently 0.987396240234375\n",
      "iteration:  11350\n",
      "Iteration 11350 , the loss is currently 0.9878326654434204\n",
      "iteration:  11360\n",
      "Iteration 11360 , the loss is currently 0.9907374382019043\n",
      "iteration:  11370\n",
      "Iteration 11370 , the loss is currently 0.9990091919898987\n",
      "iteration:  11380\n",
      "Iteration 11380 , the loss is currently 0.979690432548523\n",
      "iteration:  11390\n",
      "Iteration 11390 , the loss is currently 1.0029621124267578\n",
      "iteration:  11400\n",
      "Iteration 11400 , the loss is currently 1.007245659828186\n",
      "iteration:  11410\n",
      "Iteration 11410 , the loss is currently 0.993584156036377\n",
      "iteration:  11420\n",
      "Iteration 11420 , the loss is currently 0.9744567275047302\n",
      "iteration:  11430\n",
      "Iteration 11430 , the loss is currently 0.9810189604759216\n",
      "iteration:  11440\n",
      "Iteration 11440 , the loss is currently 0.9954707622528076\n",
      "iteration:  11450\n",
      "Iteration 11450 , the loss is currently 0.9911304712295532\n",
      "iteration:  11460\n",
      "Iteration 11460 , the loss is currently 1.003523826599121\n",
      "iteration:  11470\n",
      "Iteration 11470 , the loss is currently 0.9939821362495422\n",
      "iteration:  11480\n",
      "Iteration 11480 , the loss is currently 0.9922791123390198\n",
      "iteration:  11490\n",
      "Iteration 11490 , the loss is currently 0.9912495017051697\n",
      "iteration:  11500\n",
      "Iteration 11500 , the loss is currently 1.0074889659881592\n",
      "iteration:  11510\n",
      "Iteration 11510 , the loss is currently 1.005610466003418\n",
      "iteration:  11520\n",
      "Iteration 11520 , the loss is currently 0.9946519136428833\n",
      "iteration:  11530\n",
      "Iteration 11530 , the loss is currently 0.9970895648002625\n",
      "iteration:  11540\n",
      "Iteration 11540 , the loss is currently 0.9750037789344788\n",
      "iteration:  11550\n",
      "Iteration 11550 , the loss is currently 0.9943816065788269\n",
      "iteration:  11560\n",
      "Iteration 11560 , the loss is currently 1.0056661367416382\n",
      "iteration:  11570\n",
      "Iteration 11570 , the loss is currently 0.9990555644035339\n",
      "iteration:  11580\n",
      "Iteration 11580 , the loss is currently 0.9887673258781433\n",
      "iteration:  11590\n",
      "Iteration 11590 , the loss is currently 1.0002503395080566\n",
      "iteration:  11600\n",
      "Iteration 11600 , the loss is currently 0.984222948551178\n",
      "iteration:  11610\n",
      "Iteration 11610 , the loss is currently 1.0093770027160645\n",
      "iteration:  11620\n",
      "Iteration 11620 , the loss is currently 0.9862186908721924\n",
      "iteration:  11630\n",
      "Iteration 11630 , the loss is currently 1.000603199005127\n",
      "iteration:  11640\n",
      "Iteration 11640 , the loss is currently 0.9750451445579529\n",
      "iteration:  11650\n",
      "Iteration 11650 , the loss is currently 1.019986867904663\n",
      "iteration:  11660\n",
      "Iteration 11660 , the loss is currently 0.9734816551208496\n",
      "iteration:  11670\n",
      "Iteration 11670 , the loss is currently 1.0052860975265503\n",
      "iteration:  11680\n",
      "Iteration 11680 , the loss is currently 0.9916437268257141\n",
      "iteration:  11690\n",
      "Iteration 11690 , the loss is currently 0.979930579662323\n",
      "iteration:  11700\n",
      "Iteration 11700 , the loss is currently 0.975775957107544\n",
      "iteration:  11710\n",
      "Iteration 11710 , the loss is currently 0.995797336101532\n",
      "iteration:  11720\n",
      "Iteration 11720 , the loss is currently 1.001927137374878\n",
      "iteration:  11730\n",
      "Iteration 11730 , the loss is currently 1.002468228340149\n",
      "iteration:  11740\n",
      "Iteration 11740 , the loss is currently 0.9820842146873474\n",
      "iteration:  11750\n",
      "Iteration 11750 , the loss is currently 0.997251033782959\n",
      "iteration:  11760\n",
      "Iteration 11760 , the loss is currently 0.9949973821640015\n",
      "iteration:  11770\n",
      "Iteration 11770 , the loss is currently 0.9943330883979797\n",
      "iteration:  11780\n",
      "Iteration 11780 , the loss is currently 1.002310037612915\n",
      "iteration:  11790\n",
      "Iteration 11790 , the loss is currently 0.9817798137664795\n",
      "iteration:  11800\n",
      "Iteration 11800 , the loss is currently 0.9880440831184387\n",
      "iteration:  11810\n",
      "Iteration 11810 , the loss is currently 0.9966752529144287\n",
      "iteration:  11820\n",
      "Iteration 11820 , the loss is currently 1.0037908554077148\n",
      "iteration:  11830\n",
      "Iteration 11830 , the loss is currently 0.9911077618598938\n",
      "iteration:  11840\n",
      "Iteration 11840 , the loss is currently 0.9978246688842773\n",
      "iteration:  11850\n",
      "Iteration 11850 , the loss is currently 0.9884321093559265\n",
      "iteration:  11860\n",
      "Iteration 11860 , the loss is currently 0.985316812992096\n",
      "iteration:  11870\n",
      "Iteration 11870 , the loss is currently 0.9780969023704529\n",
      "iteration:  11880\n",
      "Iteration 11880 , the loss is currently 0.9924278855323792\n",
      "iteration:  11890\n",
      "Iteration 11890 , the loss is currently 0.9881051778793335\n",
      "iteration:  11900\n",
      "Iteration 11900 , the loss is currently 0.9821507334709167\n",
      "iteration:  11910\n",
      "Iteration 11910 , the loss is currently 0.9695528149604797\n",
      "iteration:  11920\n",
      "Iteration 11920 , the loss is currently 1.0082330703735352\n",
      "iteration:  11930\n",
      "Iteration 11930 , the loss is currently 0.9974223375320435\n",
      "iteration:  11940\n",
      "Iteration 11940 , the loss is currently 0.9966195225715637\n",
      "iteration:  11950\n",
      "Iteration 11950 , the loss is currently 0.9852741360664368\n",
      "iteration:  11960\n",
      "Iteration 11960 , the loss is currently 0.9827430248260498\n",
      "iteration:  11970\n",
      "Iteration 11970 , the loss is currently 0.9980401396751404\n",
      "iteration:  11980\n",
      "Iteration 11980 , the loss is currently 1.0165963172912598\n",
      "iteration:  11990\n",
      "Iteration 11990 , the loss is currently 0.985488772392273\n",
      "iteration:  12000\n",
      "Iteration 12000 , the loss is currently 0.9650382399559021\n",
      "iteration:  12010\n",
      "Iteration 12010 , the loss is currently 0.9883173704147339\n",
      "iteration:  12020\n",
      "Iteration 12020 , the loss is currently 1.0048797130584717\n",
      "iteration:  12030\n",
      "Iteration 12030 , the loss is currently 0.9793882369995117\n",
      "iteration:  12040\n",
      "Iteration 12040 , the loss is currently 1.0093348026275635\n",
      "iteration:  12050\n",
      "Iteration 12050 , the loss is currently 0.9978567957878113\n",
      "iteration:  12060\n",
      "Iteration 12060 , the loss is currently 1.0003304481506348\n",
      "iteration:  12070\n",
      "Iteration 12070 , the loss is currently 0.9994620084762573\n",
      "iteration:  12080\n",
      "Iteration 12080 , the loss is currently 0.9974293112754822\n",
      "iteration:  12090\n",
      "Iteration 12090 , the loss is currently 0.9829666018486023\n",
      "iteration:  12100\n",
      "Iteration 12100 , the loss is currently 0.9965264201164246\n",
      "iteration:  12110\n",
      "Iteration 12110 , the loss is currently 0.9849562644958496\n",
      "iteration:  12120\n",
      "Iteration 12120 , the loss is currently 0.9720023274421692\n",
      "iteration:  12130\n",
      "Iteration 12130 , the loss is currently 0.97441565990448\n",
      "iteration:  12140\n",
      "Iteration 12140 , the loss is currently 0.9859600067138672\n",
      "iteration:  12150\n",
      "Iteration 12150 , the loss is currently 0.9904231429100037\n",
      "iteration:  12160\n",
      "Iteration 12160 , the loss is currently 0.9943867921829224\n",
      "iteration:  12170\n",
      "Iteration 12170 , the loss is currently 0.9913520812988281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  12180\n",
      "Iteration 12180 , the loss is currently 0.9854208827018738\n",
      "iteration:  12190\n",
      "Iteration 12190 , the loss is currently 0.9881625771522522\n",
      "iteration:  12200\n",
      "Iteration 12200 , the loss is currently 1.0098131895065308\n",
      "iteration:  12210\n",
      "Iteration 12210 , the loss is currently 0.9974445700645447\n",
      "iteration:  12220\n",
      "Iteration 12220 , the loss is currently 1.0076849460601807\n",
      "iteration:  12230\n",
      "Iteration 12230 , the loss is currently 0.9905790686607361\n",
      "iteration:  12240\n",
      "Iteration 12240 , the loss is currently 0.9801363945007324\n",
      "iteration:  12250\n",
      "Iteration 12250 , the loss is currently 0.9818775057792664\n",
      "iteration:  12260\n",
      "Iteration 12260 , the loss is currently 0.9927117824554443\n",
      "iteration:  12270\n",
      "Iteration 12270 , the loss is currently 0.9812253713607788\n",
      "iteration:  12280\n",
      "Iteration 12280 , the loss is currently 0.988093376159668\n",
      "iteration:  12290\n",
      "Iteration 12290 , the loss is currently 0.9739258885383606\n",
      "iteration:  12300\n",
      "Iteration 12300 , the loss is currently 0.9817134141921997\n",
      "iteration:  12310\n",
      "Iteration 12310 , the loss is currently 0.9716732501983643\n",
      "iteration:  12320\n",
      "Iteration 12320 , the loss is currently 0.9913827180862427\n",
      "iteration:  12330\n",
      "Iteration 12330 , the loss is currently 1.0075442790985107\n",
      "iteration:  12340\n",
      "Iteration 12340 , the loss is currently 0.984499454498291\n",
      "iteration:  12350\n",
      "Iteration 12350 , the loss is currently 0.9995267391204834\n",
      "iteration:  12360\n",
      "Iteration 12360 , the loss is currently 0.9868195652961731\n",
      "iteration:  12370\n",
      "Iteration 12370 , the loss is currently 0.9916669726371765\n",
      "iteration:  12380\n",
      "Iteration 12380 , the loss is currently 0.996062159538269\n",
      "iteration:  12390\n",
      "Iteration 12390 , the loss is currently 0.9936301708221436\n",
      "iteration:  12400\n",
      "Iteration 12400 , the loss is currently 0.9850758910179138\n",
      "iteration:  12410\n",
      "Iteration 12410 , the loss is currently 0.9967972040176392\n",
      "iteration:  12420\n",
      "Iteration 12420 , the loss is currently 1.0004891157150269\n",
      "iteration:  12430\n",
      "Iteration 12430 , the loss is currently 0.9952537417411804\n",
      "iteration:  12440\n",
      "Iteration 12440 , the loss is currently 0.9865128397941589\n",
      "iteration:  12450\n",
      "Iteration 12450 , the loss is currently 0.9784072041511536\n",
      "iteration:  12460\n",
      "Iteration 12460 , the loss is currently 0.9846040606498718\n",
      "iteration:  12470\n",
      "Iteration 12470 , the loss is currently 0.9979537725448608\n",
      "iteration:  12480\n",
      "Iteration 12480 , the loss is currently 1.0097229480743408\n",
      "iteration:  12490\n",
      "Iteration 12490 , the loss is currently 0.9735369682312012\n",
      "iteration:  12500\n",
      "Iteration 12500 , the loss is currently 0.970716118812561\n",
      "iteration:  12510\n",
      "Iteration 12510 , the loss is currently 0.9835799336433411\n",
      "iteration:  12520\n",
      "Iteration 12520 , the loss is currently 1.0072062015533447\n",
      "iteration:  12530\n",
      "Iteration 12530 , the loss is currently 0.9930132627487183\n",
      "iteration:  12540\n",
      "Iteration 12540 , the loss is currently 0.9861412048339844\n",
      "iteration:  12550\n",
      "Iteration 12550 , the loss is currently 0.9729793667793274\n",
      "iteration:  12560\n",
      "Iteration 12560 , the loss is currently 0.998938798904419\n",
      "iteration:  12570\n",
      "Iteration 12570 , the loss is currently 1.0012547969818115\n",
      "iteration:  12580\n",
      "Iteration 12580 , the loss is currently 0.9831238985061646\n",
      "iteration:  12590\n",
      "Iteration 12590 , the loss is currently 0.995998203754425\n",
      "iteration:  12600\n",
      "Iteration 12600 , the loss is currently 0.973905622959137\n",
      "iteration:  12610\n",
      "Iteration 12610 , the loss is currently 1.0154510736465454\n",
      "iteration:  12620\n",
      "Iteration 12620 , the loss is currently 1.0099838972091675\n",
      "iteration:  12630\n",
      "Iteration 12630 , the loss is currently 0.9971836805343628\n",
      "iteration:  12640\n",
      "Iteration 12640 , the loss is currently 1.009501338005066\n",
      "iteration:  12650\n",
      "Iteration 12650 , the loss is currently 1.0017451047897339\n",
      "iteration:  12660\n",
      "Iteration 12660 , the loss is currently 0.9727222323417664\n",
      "iteration:  12670\n",
      "Iteration 12670 , the loss is currently 0.9901953935623169\n",
      "iteration:  12680\n",
      "Iteration 12680 , the loss is currently 0.9795588850975037\n",
      "iteration:  12690\n",
      "Iteration 12690 , the loss is currently 0.9849393963813782\n",
      "iteration:  12700\n",
      "Iteration 12700 , the loss is currently 0.9729783535003662\n",
      "iteration:  12710\n",
      "Iteration 12710 , the loss is currently 0.9798502922058105\n",
      "iteration:  12720\n",
      "Iteration 12720 , the loss is currently 0.9830350875854492\n",
      "iteration:  12730\n",
      "Iteration 12730 , the loss is currently 0.9845461249351501\n",
      "iteration:  12740\n",
      "Iteration 12740 , the loss is currently 1.0024082660675049\n",
      "iteration:  12750\n",
      "Iteration 12750 , the loss is currently 0.9866860508918762\n",
      "iteration:  12760\n",
      "Iteration 12760 , the loss is currently 1.0060954093933105\n",
      "iteration:  12770\n",
      "Iteration 12770 , the loss is currently 0.9972182512283325\n",
      "iteration:  12780\n",
      "Iteration 12780 , the loss is currently 0.962470531463623\n",
      "iteration:  12790\n",
      "Iteration 12790 , the loss is currently 1.0189144611358643\n",
      "iteration:  12800\n",
      "Iteration 12800 , the loss is currently 0.9856117367744446\n",
      "iteration:  12810\n",
      "Iteration 12810 , the loss is currently 0.9815344214439392\n",
      "iteration:  12820\n",
      "Iteration 12820 , the loss is currently 1.003710389137268\n",
      "iteration:  12830\n",
      "Iteration 12830 , the loss is currently 1.004299521446228\n",
      "iteration:  12840\n",
      "Iteration 12840 , the loss is currently 0.9959923028945923\n",
      "iteration:  12850\n",
      "Iteration 12850 , the loss is currently 0.9893937706947327\n",
      "iteration:  12860\n",
      "Iteration 12860 , the loss is currently 0.9969039559364319\n",
      "iteration:  12870\n",
      "Iteration 12870 , the loss is currently 0.9900771379470825\n",
      "iteration:  12880\n",
      "Iteration 12880 , the loss is currently 0.9998853802680969\n",
      "iteration:  12890\n",
      "Iteration 12890 , the loss is currently 0.9897186756134033\n",
      "iteration:  12900\n",
      "Iteration 12900 , the loss is currently 0.9994761347770691\n",
      "iteration:  12910\n",
      "Iteration 12910 , the loss is currently 1.0069435834884644\n",
      "iteration:  12920\n",
      "Iteration 12920 , the loss is currently 0.9875013828277588\n",
      "iteration:  12930\n",
      "Iteration 12930 , the loss is currently 0.9841163158416748\n",
      "iteration:  12940\n",
      "Iteration 12940 , the loss is currently 1.0109071731567383\n",
      "iteration:  12950\n",
      "Iteration 12950 , the loss is currently 0.9651634693145752\n",
      "iteration:  12960\n",
      "Iteration 12960 , the loss is currently 0.9882553815841675\n",
      "iteration:  12970\n",
      "Iteration 12970 , the loss is currently 0.9955477118492126\n",
      "iteration:  12980\n",
      "Iteration 12980 , the loss is currently 0.9741255640983582\n",
      "iteration:  12990\n",
      "Iteration 12990 , the loss is currently 0.9737759828567505\n",
      "iteration:  13000\n",
      "Iteration 13000 , the loss is currently 0.9898603558540344\n",
      "iteration:  13010\n",
      "Iteration 13010 , the loss is currently 0.970777153968811\n",
      "iteration:  13020\n",
      "Iteration 13020 , the loss is currently 0.9788590669631958\n",
      "iteration:  13030\n",
      "Iteration 13030 , the loss is currently 0.9942821264266968\n",
      "iteration:  13040\n",
      "Iteration 13040 , the loss is currently 0.9897820949554443\n",
      "iteration:  13050\n",
      "Iteration 13050 , the loss is currently 0.998383104801178\n",
      "iteration:  13060\n",
      "Iteration 13060 , the loss is currently 0.9622207283973694\n",
      "iteration:  13070\n",
      "Iteration 13070 , the loss is currently 0.9881676435470581\n",
      "iteration:  13080\n",
      "Iteration 13080 , the loss is currently 0.984826922416687\n",
      "iteration:  13090\n",
      "Iteration 13090 , the loss is currently 0.9750800728797913\n",
      "iteration:  13100\n",
      "Iteration 13100 , the loss is currently 1.0035408735275269\n",
      "iteration:  13110\n",
      "Iteration 13110 , the loss is currently 0.9483777284622192\n",
      "iteration:  13120\n",
      "Iteration 13120 , the loss is currently 0.9985809922218323\n",
      "iteration:  13130\n",
      "Iteration 13130 , the loss is currently 0.9954808950424194\n",
      "iteration:  13140\n",
      "Iteration 13140 , the loss is currently 0.9932291507720947\n",
      "iteration:  13150\n",
      "Iteration 13150 , the loss is currently 0.9886605143547058\n",
      "iteration:  13160\n",
      "Iteration 13160 , the loss is currently 0.9951685070991516\n",
      "iteration:  13170\n",
      "Iteration 13170 , the loss is currently 0.9811016321182251\n",
      "iteration:  13180\n",
      "Iteration 13180 , the loss is currently 0.9821948409080505\n",
      "iteration:  13190\n",
      "Iteration 13190 , the loss is currently 0.9924255013465881\n",
      "iteration:  13200\n",
      "Iteration 13200 , the loss is currently 1.0208185911178589\n",
      "iteration:  13210\n",
      "Iteration 13210 , the loss is currently 1.0015259981155396\n",
      "iteration:  13220\n",
      "Iteration 13220 , the loss is currently 0.9818309545516968\n",
      "iteration:  13230\n",
      "Iteration 13230 , the loss is currently 0.9968942999839783\n",
      "iteration:  13240\n",
      "Iteration 13240 , the loss is currently 0.9968399405479431\n",
      "iteration:  13250\n",
      "Iteration 13250 , the loss is currently 0.9873499274253845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  13260\n",
      "Iteration 13260 , the loss is currently 0.9883837699890137\n",
      "iteration:  13270\n",
      "Iteration 13270 , the loss is currently 1.004470944404602\n",
      "iteration:  13280\n",
      "Iteration 13280 , the loss is currently 0.9823307991027832\n",
      "iteration:  13290\n",
      "Iteration 13290 , the loss is currently 0.9938676953315735\n",
      "iteration:  13300\n",
      "Iteration 13300 , the loss is currently 0.9961507320404053\n",
      "iteration:  13310\n",
      "Iteration 13310 , the loss is currently 1.0095739364624023\n",
      "iteration:  13320\n",
      "Iteration 13320 , the loss is currently 0.980341374874115\n",
      "iteration:  13330\n",
      "Iteration 13330 , the loss is currently 0.9846641421318054\n",
      "iteration:  13340\n",
      "Iteration 13340 , the loss is currently 0.9807814955711365\n",
      "iteration:  13350\n",
      "Iteration 13350 , the loss is currently 0.9867941737174988\n",
      "iteration:  13360\n",
      "Iteration 13360 , the loss is currently 0.9836260676383972\n",
      "iteration:  13370\n",
      "Iteration 13370 , the loss is currently 0.9863780736923218\n",
      "iteration:  13380\n",
      "Iteration 13380 , the loss is currently 1.0041435956954956\n",
      "iteration:  13390\n",
      "Iteration 13390 , the loss is currently 0.9884027242660522\n",
      "iteration:  13400\n",
      "Iteration 13400 , the loss is currently 1.0002069473266602\n",
      "iteration:  13410\n",
      "Iteration 13410 , the loss is currently 0.9841806888580322\n",
      "iteration:  13420\n",
      "Iteration 13420 , the loss is currently 0.9952929019927979\n",
      "iteration:  13430\n",
      "Iteration 13430 , the loss is currently 0.9994446039199829\n",
      "iteration:  13440\n",
      "Iteration 13440 , the loss is currently 0.98436039686203\n",
      "iteration:  13450\n",
      "Iteration 13450 , the loss is currently 0.999900758266449\n",
      "iteration:  13460\n",
      "Iteration 13460 , the loss is currently 0.9904207587242126\n",
      "iteration:  13470\n",
      "Iteration 13470 , the loss is currently 0.9808667302131653\n",
      "iteration:  13480\n",
      "Iteration 13480 , the loss is currently 0.9910159707069397\n",
      "iteration:  13490\n",
      "Iteration 13490 , the loss is currently 0.9910249710083008\n",
      "iteration:  13500\n",
      "Iteration 13500 , the loss is currently 0.9756293296813965\n",
      "iteration:  13510\n",
      "Iteration 13510 , the loss is currently 0.9839001297950745\n",
      "iteration:  13520\n",
      "Iteration 13520 , the loss is currently 0.9984591007232666\n",
      "iteration:  13530\n",
      "Iteration 13530 , the loss is currently 0.9804654717445374\n",
      "iteration:  13540\n",
      "Iteration 13540 , the loss is currently 0.9983141422271729\n",
      "iteration:  13550\n",
      "Iteration 13550 , the loss is currently 0.9900428652763367\n",
      "iteration:  13560\n",
      "Iteration 13560 , the loss is currently 1.0103495121002197\n",
      "iteration:  13570\n",
      "Iteration 13570 , the loss is currently 0.9752435684204102\n",
      "iteration:  13580\n",
      "Iteration 13580 , the loss is currently 0.9869339466094971\n",
      "iteration:  13590\n",
      "Iteration 13590 , the loss is currently 0.9742026329040527\n",
      "iteration:  13600\n",
      "Iteration 13600 , the loss is currently 0.9752723574638367\n",
      "iteration:  13610\n",
      "Iteration 13610 , the loss is currently 0.9946659803390503\n",
      "iteration:  13620\n",
      "Iteration 13620 , the loss is currently 0.993052065372467\n",
      "iteration:  13630\n",
      "Iteration 13630 , the loss is currently 0.9908319711685181\n",
      "iteration:  13640\n",
      "Iteration 13640 , the loss is currently 1.0092490911483765\n",
      "iteration:  13650\n",
      "Iteration 13650 , the loss is currently 0.9946311116218567\n",
      "iteration:  13660\n",
      "Iteration 13660 , the loss is currently 1.0004494190216064\n",
      "iteration:  13670\n",
      "Iteration 13670 , the loss is currently 0.994076669216156\n",
      "iteration:  13680\n",
      "Iteration 13680 , the loss is currently 1.0082277059555054\n",
      "iteration:  13690\n",
      "Iteration 13690 , the loss is currently 0.983461320400238\n",
      "iteration:  13700\n",
      "Iteration 13700 , the loss is currently 0.9739359617233276\n",
      "iteration:  13710\n",
      "Iteration 13710 , the loss is currently 0.9825243353843689\n",
      "iteration:  13720\n",
      "Iteration 13720 , the loss is currently 1.0098146200180054\n",
      "iteration:  13730\n",
      "Iteration 13730 , the loss is currently 0.9847809076309204\n",
      "iteration:  13740\n",
      "Iteration 13740 , the loss is currently 0.9678654670715332\n",
      "iteration:  13750\n",
      "Iteration 13750 , the loss is currently 0.998826265335083\n",
      "iteration:  13760\n",
      "Iteration 13760 , the loss is currently 0.9869018197059631\n",
      "iteration:  13770\n",
      "Iteration 13770 , the loss is currently 0.9699963927268982\n",
      "iteration:  13780\n",
      "Iteration 13780 , the loss is currently 0.994519829750061\n",
      "iteration:  13790\n",
      "Iteration 13790 , the loss is currently 1.0037842988967896\n",
      "iteration:  13800\n",
      "Iteration 13800 , the loss is currently 1.0033591985702515\n",
      "iteration:  13810\n",
      "Iteration 13810 , the loss is currently 0.995959460735321\n",
      "iteration:  13820\n",
      "Iteration 13820 , the loss is currently 0.984088122844696\n",
      "iteration:  13830\n",
      "Iteration 13830 , the loss is currently 0.9808622598648071\n",
      "iteration:  13840\n",
      "Iteration 13840 , the loss is currently 1.0070239305496216\n",
      "iteration:  13850\n",
      "Iteration 13850 , the loss is currently 0.9780863523483276\n",
      "iteration:  13860\n",
      "Iteration 13860 , the loss is currently 1.0009791851043701\n",
      "iteration:  13870\n",
      "Iteration 13870 , the loss is currently 0.9937241673469543\n",
      "iteration:  13880\n",
      "Iteration 13880 , the loss is currently 0.9811529517173767\n",
      "iteration:  13890\n",
      "Iteration 13890 , the loss is currently 0.9838248491287231\n",
      "iteration:  13900\n",
      "Iteration 13900 , the loss is currently 0.9996479153633118\n",
      "iteration:  13910\n",
      "Iteration 13910 , the loss is currently 1.0117148160934448\n",
      "iteration:  13920\n",
      "Iteration 13920 , the loss is currently 0.9719050526618958\n",
      "iteration:  13930\n",
      "Iteration 13930 , the loss is currently 1.009624719619751\n",
      "iteration:  13940\n",
      "Iteration 13940 , the loss is currently 0.9988471269607544\n",
      "iteration:  13950\n",
      "Iteration 13950 , the loss is currently 0.9890345931053162\n",
      "iteration:  13960\n",
      "Iteration 13960 , the loss is currently 1.0014824867248535\n",
      "iteration:  13970\n",
      "Iteration 13970 , the loss is currently 1.0055681467056274\n",
      "iteration:  13980\n",
      "Iteration 13980 , the loss is currently 0.9878043532371521\n",
      "iteration:  13990\n",
      "Iteration 13990 , the loss is currently 0.9896696209907532\n",
      "iteration:  14000\n",
      "Iteration 14000 , the loss is currently 0.9924032688140869\n",
      "iteration:  14010\n",
      "Iteration 14010 , the loss is currently 1.0034452676773071\n",
      "iteration:  14020\n",
      "Iteration 14020 , the loss is currently 0.9877855181694031\n",
      "iteration:  14030\n",
      "Iteration 14030 , the loss is currently 0.9891844987869263\n",
      "iteration:  14040\n",
      "Iteration 14040 , the loss is currently 1.0001899003982544\n",
      "iteration:  14050\n",
      "Iteration 14050 , the loss is currently 0.9964097738265991\n",
      "iteration:  14060\n",
      "Iteration 14060 , the loss is currently 0.9829223155975342\n",
      "iteration:  14070\n",
      "Iteration 14070 , the loss is currently 0.986569881439209\n",
      "iteration:  14080\n",
      "Iteration 14080 , the loss is currently 0.9654414653778076\n",
      "iteration:  14090\n",
      "Iteration 14090 , the loss is currently 0.9945634007453918\n",
      "iteration:  14100\n",
      "Iteration 14100 , the loss is currently 0.9909400939941406\n",
      "iteration:  14110\n",
      "Iteration 14110 , the loss is currently 0.9845036864280701\n",
      "iteration:  14120\n",
      "Iteration 14120 , the loss is currently 0.9873530268669128\n",
      "iteration:  14130\n",
      "Iteration 14130 , the loss is currently 0.9999400973320007\n",
      "iteration:  14140\n",
      "Iteration 14140 , the loss is currently 0.994465708732605\n",
      "iteration:  14150\n",
      "Iteration 14150 , the loss is currently 0.9950692653656006\n",
      "iteration:  14160\n",
      "Iteration 14160 , the loss is currently 0.9713826775550842\n",
      "iteration:  14170\n",
      "Iteration 14170 , the loss is currently 0.9965422749519348\n",
      "iteration:  14180\n",
      "Iteration 14180 , the loss is currently 0.9844517111778259\n",
      "iteration:  14190\n",
      "Iteration 14190 , the loss is currently 0.978589653968811\n",
      "iteration:  14200\n",
      "Iteration 14200 , the loss is currently 0.9940152764320374\n",
      "iteration:  14210\n",
      "Iteration 14210 , the loss is currently 0.9649866819381714\n",
      "iteration:  14220\n",
      "Iteration 14220 , the loss is currently 0.9676775932312012\n",
      "iteration:  14230\n",
      "Iteration 14230 , the loss is currently 0.9925318956375122\n",
      "iteration:  14240\n",
      "Iteration 14240 , the loss is currently 0.9901432394981384\n",
      "iteration:  14250\n",
      "Iteration 14250 , the loss is currently 0.9928526878356934\n",
      "iteration:  14260\n",
      "Iteration 14260 , the loss is currently 1.0088286399841309\n",
      "iteration:  14270\n",
      "Iteration 14270 , the loss is currently 0.9924987554550171\n",
      "iteration:  14280\n",
      "Iteration 14280 , the loss is currently 1.005029320716858\n",
      "iteration:  14290\n",
      "Iteration 14290 , the loss is currently 0.9720993041992188\n",
      "iteration:  14300\n",
      "Iteration 14300 , the loss is currently 0.9741519689559937\n",
      "iteration:  14310\n",
      "Iteration 14310 , the loss is currently 0.9780816435813904\n",
      "iteration:  14320\n",
      "Iteration 14320 , the loss is currently 0.983898401260376\n",
      "iteration:  14330\n",
      "Iteration 14330 , the loss is currently 0.9853794574737549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  14340\n",
      "Iteration 14340 , the loss is currently 1.0035208463668823\n",
      "iteration:  14350\n",
      "Iteration 14350 , the loss is currently 0.9758137464523315\n",
      "iteration:  14360\n",
      "Iteration 14360 , the loss is currently 0.9985358715057373\n",
      "iteration:  14370\n",
      "Iteration 14370 , the loss is currently 0.9768197536468506\n",
      "iteration:  14380\n",
      "Iteration 14380 , the loss is currently 0.9839121103286743\n",
      "iteration:  14390\n",
      "Iteration 14390 , the loss is currently 0.9799050092697144\n",
      "iteration:  14400\n",
      "Iteration 14400 , the loss is currently 0.9947698712348938\n",
      "iteration:  14410\n",
      "Iteration 14410 , the loss is currently 0.9813967347145081\n",
      "iteration:  14420\n",
      "Iteration 14420 , the loss is currently 0.9595072269439697\n",
      "iteration:  14430\n",
      "Iteration 14430 , the loss is currently 0.990152895450592\n",
      "iteration:  14440\n",
      "Iteration 14440 , the loss is currently 0.9832794070243835\n",
      "iteration:  14450\n",
      "Iteration 14450 , the loss is currently 0.9885333776473999\n",
      "iteration:  14460\n",
      "Iteration 14460 , the loss is currently 0.993457555770874\n",
      "iteration:  14470\n",
      "Iteration 14470 , the loss is currently 0.996959388256073\n",
      "iteration:  14480\n",
      "Iteration 14480 , the loss is currently 0.9925768375396729\n",
      "iteration:  14490\n",
      "Iteration 14490 , the loss is currently 1.0127569437026978\n",
      "iteration:  14500\n",
      "Iteration 14500 , the loss is currently 0.9965795278549194\n",
      "iteration:  14510\n",
      "Iteration 14510 , the loss is currently 0.9941065907478333\n",
      "iteration:  14520\n",
      "Iteration 14520 , the loss is currently 0.9752695560455322\n",
      "iteration:  14530\n",
      "Iteration 14530 , the loss is currently 0.9975945949554443\n",
      "iteration:  14540\n",
      "Iteration 14540 , the loss is currently 0.9926637411117554\n",
      "iteration:  14550\n",
      "Iteration 14550 , the loss is currently 0.9958004355430603\n",
      "iteration:  14560\n",
      "Iteration 14560 , the loss is currently 0.9962702989578247\n",
      "iteration:  14570\n",
      "Iteration 14570 , the loss is currently 0.9934031963348389\n",
      "iteration:  14580\n",
      "Iteration 14580 , the loss is currently 1.0176059007644653\n",
      "iteration:  14590\n",
      "Iteration 14590 , the loss is currently 0.9874925017356873\n",
      "iteration:  14600\n",
      "Iteration 14600 , the loss is currently 1.015377163887024\n",
      "iteration:  14610\n",
      "Iteration 14610 , the loss is currently 0.9885231256484985\n",
      "iteration:  14620\n",
      "Iteration 14620 , the loss is currently 0.9969538450241089\n",
      "iteration:  14630\n",
      "Iteration 14630 , the loss is currently 0.9969971776008606\n",
      "iteration:  14640\n",
      "Iteration 14640 , the loss is currently 0.9988338351249695\n",
      "iteration:  14650\n",
      "Iteration 14650 , the loss is currently 1.0042861700057983\n",
      "iteration:  14660\n",
      "Iteration 14660 , the loss is currently 0.9918685555458069\n",
      "iteration:  14670\n",
      "Iteration 14670 , the loss is currently 0.9897034764289856\n",
      "iteration:  14680\n",
      "Iteration 14680 , the loss is currently 0.9914076924324036\n",
      "iteration:  14690\n",
      "Iteration 14690 , the loss is currently 0.985203742980957\n",
      "iteration:  14700\n",
      "Iteration 14700 , the loss is currently 0.9564185738563538\n",
      "iteration:  14710\n",
      "Iteration 14710 , the loss is currently 0.9833102822303772\n",
      "iteration:  14720\n",
      "Iteration 14720 , the loss is currently 0.9824122190475464\n",
      "iteration:  14730\n",
      "Iteration 14730 , the loss is currently 0.9836663603782654\n",
      "iteration:  14740\n",
      "Iteration 14740 , the loss is currently 0.9716372489929199\n",
      "iteration:  14750\n",
      "Iteration 14750 , the loss is currently 0.9787419438362122\n",
      "iteration:  14760\n",
      "Iteration 14760 , the loss is currently 1.016029715538025\n",
      "iteration:  14770\n",
      "Iteration 14770 , the loss is currently 0.9687058925628662\n",
      "iteration:  14780\n",
      "Iteration 14780 , the loss is currently 1.0033681392669678\n",
      "iteration:  14790\n",
      "Iteration 14790 , the loss is currently 0.9862148761749268\n",
      "iteration:  14800\n",
      "Iteration 14800 , the loss is currently 0.9887101054191589\n",
      "iteration:  14810\n",
      "Iteration 14810 , the loss is currently 0.9848869442939758\n",
      "iteration:  14820\n",
      "Iteration 14820 , the loss is currently 1.0077557563781738\n",
      "iteration:  14830\n",
      "Iteration 14830 , the loss is currently 0.9896738529205322\n",
      "iteration:  14840\n",
      "Iteration 14840 , the loss is currently 0.9886690974235535\n",
      "iteration:  14850\n",
      "Iteration 14850 , the loss is currently 1.015704870223999\n",
      "iteration:  14860\n",
      "Iteration 14860 , the loss is currently 0.9788723587989807\n",
      "iteration:  14870\n",
      "Iteration 14870 , the loss is currently 1.0026123523712158\n",
      "iteration:  14880\n",
      "Iteration 14880 , the loss is currently 0.9918543100357056\n",
      "iteration:  14890\n",
      "Iteration 14890 , the loss is currently 0.9965900182723999\n",
      "iteration:  14900\n",
      "Iteration 14900 , the loss is currently 0.988452136516571\n",
      "iteration:  14910\n",
      "Iteration 14910 , the loss is currently 1.003008484840393\n",
      "iteration:  14920\n",
      "Iteration 14920 , the loss is currently 0.9909883141517639\n",
      "iteration:  14930\n",
      "Iteration 14930 , the loss is currently 0.9876393675804138\n",
      "iteration:  14940\n",
      "Iteration 14940 , the loss is currently 0.9969291687011719\n",
      "iteration:  14950\n",
      "Iteration 14950 , the loss is currently 0.9860725998878479\n",
      "iteration:  14960\n",
      "Iteration 14960 , the loss is currently 0.9848131537437439\n",
      "iteration:  14970\n",
      "Iteration 14970 , the loss is currently 1.0072648525238037\n",
      "iteration:  14980\n",
      "Iteration 14980 , the loss is currently 0.980499804019928\n",
      "iteration:  14990\n",
      "Iteration 14990 , the loss is currently 1.003620982170105\n",
      "iteration:  15000\n",
      "Iteration 15000 , the loss is currently 0.9981006383895874\n",
      "iteration:  15010\n",
      "Iteration 15010 , the loss is currently 0.9947608113288879\n",
      "iteration:  15020\n",
      "Iteration 15020 , the loss is currently 1.0072681903839111\n",
      "iteration:  15030\n",
      "Iteration 15030 , the loss is currently 0.9812409281730652\n",
      "iteration:  15040\n",
      "Iteration 15040 , the loss is currently 0.9842488765716553\n",
      "iteration:  15050\n",
      "Iteration 15050 , the loss is currently 0.9993712306022644\n",
      "iteration:  15060\n",
      "Iteration 15060 , the loss is currently 0.9856942892074585\n",
      "iteration:  15070\n",
      "Iteration 15070 , the loss is currently 0.9871148467063904\n",
      "iteration:  15080\n",
      "Iteration 15080 , the loss is currently 0.9871417880058289\n",
      "iteration:  15090\n",
      "Iteration 15090 , the loss is currently 0.9977045059204102\n",
      "iteration:  15100\n",
      "Iteration 15100 , the loss is currently 0.9868296980857849\n",
      "iteration:  15110\n",
      "Iteration 15110 , the loss is currently 1.00136137008667\n",
      "iteration:  15120\n",
      "Iteration 15120 , the loss is currently 0.9808538556098938\n",
      "iteration:  15130\n",
      "Iteration 15130 , the loss is currently 0.9949380159378052\n",
      "iteration:  15140\n",
      "Iteration 15140 , the loss is currently 1.008542537689209\n",
      "iteration:  15150\n",
      "Iteration 15150 , the loss is currently 0.9698873162269592\n",
      "iteration:  15160\n",
      "Iteration 15160 , the loss is currently 0.9825933575630188\n",
      "iteration:  15170\n",
      "Iteration 15170 , the loss is currently 1.0059456825256348\n",
      "iteration:  15180\n",
      "Iteration 15180 , the loss is currently 0.9971374869346619\n",
      "iteration:  15190\n",
      "Iteration 15190 , the loss is currently 1.0196800231933594\n",
      "iteration:  15200\n",
      "Iteration 15200 , the loss is currently 0.9931584000587463\n",
      "iteration:  15210\n",
      "Iteration 15210 , the loss is currently 0.9933407306671143\n",
      "iteration:  15220\n",
      "Iteration 15220 , the loss is currently 0.9922926425933838\n",
      "iteration:  15230\n",
      "Iteration 15230 , the loss is currently 0.9881533980369568\n",
      "iteration:  15240\n",
      "Iteration 15240 , the loss is currently 1.0047283172607422\n",
      "iteration:  15250\n",
      "Iteration 15250 , the loss is currently 0.9975776076316833\n",
      "iteration:  15260\n",
      "Iteration 15260 , the loss is currently 0.9917481541633606\n",
      "iteration:  15270\n",
      "Iteration 15270 , the loss is currently 1.0002156496047974\n",
      "iteration:  15280\n",
      "Iteration 15280 , the loss is currently 0.9850282073020935\n",
      "iteration:  15290\n",
      "Iteration 15290 , the loss is currently 0.9884616136550903\n",
      "iteration:  15300\n",
      "Iteration 15300 , the loss is currently 0.9690215587615967\n",
      "iteration:  15310\n",
      "Iteration 15310 , the loss is currently 1.0093038082122803\n",
      "iteration:  15320\n",
      "Iteration 15320 , the loss is currently 0.9929611086845398\n",
      "iteration:  15330\n",
      "Iteration 15330 , the loss is currently 0.9969390034675598\n",
      "iteration:  15340\n",
      "Iteration 15340 , the loss is currently 1.0005236864089966\n",
      "iteration:  15350\n",
      "Iteration 15350 , the loss is currently 0.974559485912323\n",
      "iteration:  15360\n",
      "Iteration 15360 , the loss is currently 0.9760973453521729\n",
      "iteration:  15370\n",
      "Iteration 15370 , the loss is currently 0.9799437522888184\n",
      "iteration:  15380\n",
      "Iteration 15380 , the loss is currently 0.9748803973197937\n",
      "iteration:  15390\n",
      "Iteration 15390 , the loss is currently 1.0073007345199585\n",
      "iteration:  15400\n",
      "Iteration 15400 , the loss is currently 1.0036753416061401\n",
      "iteration:  15410\n",
      "Iteration 15410 , the loss is currently 1.0000231266021729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  15420\n",
      "Iteration 15420 , the loss is currently 0.9711156487464905\n",
      "iteration:  15430\n",
      "Iteration 15430 , the loss is currently 0.9842302799224854\n",
      "iteration:  15440\n",
      "Iteration 15440 , the loss is currently 1.0150657892227173\n",
      "iteration:  15450\n",
      "Iteration 15450 , the loss is currently 0.987668514251709\n",
      "iteration:  15460\n",
      "Iteration 15460 , the loss is currently 0.9742649793624878\n",
      "iteration:  15470\n",
      "Iteration 15470 , the loss is currently 0.999855637550354\n",
      "iteration:  15480\n",
      "Iteration 15480 , the loss is currently 0.9847425222396851\n",
      "iteration:  15490\n",
      "Iteration 15490 , the loss is currently 0.976605236530304\n",
      "iteration:  15500\n",
      "Iteration 15500 , the loss is currently 0.9899902939796448\n",
      "iteration:  15510\n",
      "Iteration 15510 , the loss is currently 0.9950124621391296\n",
      "iteration:  15520\n",
      "Iteration 15520 , the loss is currently 0.996405839920044\n",
      "iteration:  15530\n",
      "Iteration 15530 , the loss is currently 0.9855204224586487\n",
      "iteration:  15540\n",
      "Iteration 15540 , the loss is currently 0.9989834427833557\n",
      "iteration:  15550\n",
      "Iteration 15550 , the loss is currently 0.9815514087677002\n",
      "iteration:  15560\n",
      "Iteration 15560 , the loss is currently 0.9972417950630188\n",
      "iteration:  15570\n",
      "Iteration 15570 , the loss is currently 1.0020825862884521\n",
      "iteration:  15580\n",
      "Iteration 15580 , the loss is currently 0.9947916269302368\n",
      "iteration:  15590\n",
      "Iteration 15590 , the loss is currently 0.9628205895423889\n",
      "iteration:  15600\n",
      "Iteration 15600 , the loss is currently 0.9912400245666504\n",
      "iteration:  15610\n",
      "Iteration 15610 , the loss is currently 1.0150833129882812\n",
      "iteration:  15620\n",
      "Iteration 15620 , the loss is currently 0.981529951095581\n",
      "iteration:  15630\n",
      "Iteration 15630 , the loss is currently 0.9987150430679321\n",
      "iteration:  15640\n",
      "Iteration 15640 , the loss is currently 0.9933303594589233\n",
      "iteration:  15650\n",
      "Iteration 15650 , the loss is currently 1.0153388977050781\n",
      "iteration:  15660\n",
      "Iteration 15660 , the loss is currently 1.0163654088974\n",
      "iteration:  15670\n",
      "Iteration 15670 , the loss is currently 0.989944338798523\n",
      "iteration:  15680\n",
      "Iteration 15680 , the loss is currently 1.0020036697387695\n",
      "iteration:  15690\n",
      "Iteration 15690 , the loss is currently 1.0037318468093872\n",
      "iteration:  15700\n",
      "Iteration 15700 , the loss is currently 0.9694710373878479\n",
      "iteration:  15710\n",
      "Iteration 15710 , the loss is currently 0.9993494749069214\n",
      "iteration:  15720\n",
      "Iteration 15720 , the loss is currently 0.9934693574905396\n",
      "iteration:  15730\n",
      "Iteration 15730 , the loss is currently 0.9786857962608337\n",
      "iteration:  15740\n",
      "Iteration 15740 , the loss is currently 0.9811376929283142\n",
      "iteration:  15750\n",
      "Iteration 15750 , the loss is currently 1.004159688949585\n",
      "iteration:  15760\n",
      "Iteration 15760 , the loss is currently 0.9877631664276123\n",
      "iteration:  15770\n",
      "Iteration 15770 , the loss is currently 0.983866274356842\n",
      "iteration:  15780\n",
      "Iteration 15780 , the loss is currently 0.9944076538085938\n",
      "iteration:  15790\n",
      "Iteration 15790 , the loss is currently 1.005414366722107\n",
      "iteration:  15800\n",
      "Iteration 15800 , the loss is currently 0.978603184223175\n",
      "iteration:  15810\n",
      "Iteration 15810 , the loss is currently 1.0066783428192139\n",
      "iteration:  15820\n",
      "Iteration 15820 , the loss is currently 0.9898123145103455\n",
      "iteration:  15830\n",
      "Iteration 15830 , the loss is currently 0.9778148531913757\n",
      "iteration:  15840\n",
      "Iteration 15840 , the loss is currently 0.969414472579956\n",
      "iteration:  15850\n",
      "Iteration 15850 , the loss is currently 0.9818804264068604\n",
      "iteration:  15860\n",
      "Iteration 15860 , the loss is currently 0.9981446862220764\n",
      "iteration:  15870\n",
      "Iteration 15870 , the loss is currently 0.9835739731788635\n",
      "iteration:  15880\n",
      "Iteration 15880 , the loss is currently 0.9958003163337708\n",
      "iteration:  15890\n",
      "Iteration 15890 , the loss is currently 0.9952065944671631\n",
      "iteration:  15900\n",
      "Iteration 15900 , the loss is currently 1.0086560249328613\n",
      "iteration:  15910\n",
      "Iteration 15910 , the loss is currently 0.9902064800262451\n",
      "iteration:  15920\n",
      "Iteration 15920 , the loss is currently 0.9889247417449951\n",
      "iteration:  15930\n",
      "Iteration 15930 , the loss is currently 1.0005757808685303\n",
      "iteration:  15940\n",
      "Iteration 15940 , the loss is currently 0.9617565274238586\n",
      "iteration:  15950\n",
      "Iteration 15950 , the loss is currently 0.9974406361579895\n",
      "iteration:  15960\n",
      "Iteration 15960 , the loss is currently 0.9968453645706177\n",
      "iteration:  15970\n",
      "Iteration 15970 , the loss is currently 0.9957438707351685\n",
      "iteration:  15980\n",
      "Iteration 15980 , the loss is currently 0.985633134841919\n",
      "iteration:  15990\n",
      "Iteration 15990 , the loss is currently 0.996822714805603\n",
      "iteration:  16000\n",
      "Iteration 16000 , the loss is currently 0.9987732172012329\n",
      "iteration:  16010\n",
      "Iteration 16010 , the loss is currently 1.0105364322662354\n",
      "iteration:  16020\n",
      "Iteration 16020 , the loss is currently 0.9912694692611694\n",
      "iteration:  16030\n",
      "Iteration 16030 , the loss is currently 0.9677114486694336\n",
      "iteration:  16040\n",
      "Iteration 16040 , the loss is currently 0.9906145334243774\n",
      "iteration:  16050\n",
      "Iteration 16050 , the loss is currently 0.9740602970123291\n",
      "iteration:  16060\n",
      "Iteration 16060 , the loss is currently 0.9836568236351013\n",
      "iteration:  16070\n",
      "Iteration 16070 , the loss is currently 0.9914935827255249\n",
      "iteration:  16080\n",
      "Iteration 16080 , the loss is currently 0.996204137802124\n",
      "iteration:  16090\n",
      "Iteration 16090 , the loss is currently 0.9777293801307678\n",
      "iteration:  16100\n",
      "Iteration 16100 , the loss is currently 0.9649911522865295\n",
      "iteration:  16110\n",
      "Iteration 16110 , the loss is currently 0.984379231929779\n",
      "iteration:  16120\n",
      "Iteration 16120 , the loss is currently 0.9797046780586243\n",
      "iteration:  16130\n",
      "Iteration 16130 , the loss is currently 1.0136821269989014\n",
      "iteration:  16140\n",
      "Iteration 16140 , the loss is currently 0.9778094291687012\n",
      "iteration:  16150\n",
      "Iteration 16150 , the loss is currently 0.9838298559188843\n",
      "iteration:  16160\n",
      "Iteration 16160 , the loss is currently 0.989721417427063\n",
      "iteration:  16170\n",
      "Iteration 16170 , the loss is currently 0.9744806885719299\n",
      "iteration:  16180\n",
      "Iteration 16180 , the loss is currently 1.0045206546783447\n",
      "iteration:  16190\n",
      "Iteration 16190 , the loss is currently 0.9949045777320862\n",
      "iteration:  16200\n",
      "Iteration 16200 , the loss is currently 0.9846047759056091\n",
      "iteration:  16210\n",
      "Iteration 16210 , the loss is currently 0.9882755279541016\n",
      "iteration:  16220\n",
      "Iteration 16220 , the loss is currently 0.9937044382095337\n",
      "iteration:  16230\n",
      "Iteration 16230 , the loss is currently 1.0022364854812622\n",
      "iteration:  16240\n",
      "Iteration 16240 , the loss is currently 0.9856486916542053\n",
      "iteration:  16250\n",
      "Iteration 16250 , the loss is currently 0.9976803064346313\n",
      "iteration:  16260\n",
      "Iteration 16260 , the loss is currently 0.9862905144691467\n",
      "iteration:  16270\n",
      "Iteration 16270 , the loss is currently 0.9962741732597351\n",
      "iteration:  16280\n",
      "Iteration 16280 , the loss is currently 0.9959629774093628\n",
      "iteration:  16290\n",
      "Iteration 16290 , the loss is currently 0.9964777827262878\n",
      "iteration:  16300\n",
      "Iteration 16300 , the loss is currently 0.9987910985946655\n",
      "iteration:  16310\n",
      "Iteration 16310 , the loss is currently 0.9998630881309509\n",
      "iteration:  16320\n",
      "Iteration 16320 , the loss is currently 0.9942986369132996\n",
      "iteration:  16330\n",
      "Iteration 16330 , the loss is currently 0.9767253398895264\n",
      "iteration:  16340\n",
      "Iteration 16340 , the loss is currently 1.000741958618164\n",
      "iteration:  16350\n",
      "Iteration 16350 , the loss is currently 1.0098308324813843\n",
      "iteration:  16360\n",
      "Iteration 16360 , the loss is currently 0.9728441834449768\n",
      "iteration:  16370\n",
      "Iteration 16370 , the loss is currently 0.9980031847953796\n",
      "iteration:  16380\n",
      "Iteration 16380 , the loss is currently 0.9954551458358765\n",
      "iteration:  16390\n",
      "Iteration 16390 , the loss is currently 0.9980920553207397\n",
      "iteration:  16400\n",
      "Iteration 16400 , the loss is currently 0.9967688322067261\n",
      "iteration:  16410\n",
      "Iteration 16410 , the loss is currently 0.9892457127571106\n",
      "iteration:  16420\n",
      "Iteration 16420 , the loss is currently 0.9942379593849182\n",
      "iteration:  16430\n",
      "Iteration 16430 , the loss is currently 0.9735812544822693\n",
      "iteration:  16440\n",
      "Iteration 16440 , the loss is currently 0.9836194515228271\n",
      "iteration:  16450\n",
      "Iteration 16450 , the loss is currently 0.9833129644393921\n",
      "iteration:  16460\n",
      "Iteration 16460 , the loss is currently 0.9738105535507202\n",
      "iteration:  16470\n",
      "Iteration 16470 , the loss is currently 0.9916825890541077\n",
      "iteration:  16480\n",
      "Iteration 16480 , the loss is currently 0.9966289401054382\n",
      "iteration:  16490\n",
      "Iteration 16490 , the loss is currently 0.9988768100738525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  16500\n",
      "Iteration 16500 , the loss is currently 0.9915052652359009\n",
      "iteration:  16510\n",
      "Iteration 16510 , the loss is currently 0.9886942505836487\n",
      "iteration:  16520\n",
      "Iteration 16520 , the loss is currently 1.0021309852600098\n",
      "iteration:  16530\n",
      "Iteration 16530 , the loss is currently 0.990723729133606\n",
      "iteration:  16540\n",
      "Iteration 16540 , the loss is currently 0.994973361492157\n",
      "iteration:  16550\n",
      "Iteration 16550 , the loss is currently 0.9829904437065125\n",
      "iteration:  16560\n",
      "Iteration 16560 , the loss is currently 0.9905364513397217\n",
      "iteration:  16570\n",
      "Iteration 16570 , the loss is currently 0.9805038571357727\n",
      "iteration:  16580\n",
      "Iteration 16580 , the loss is currently 0.9977240562438965\n",
      "iteration:  16590\n",
      "Iteration 16590 , the loss is currently 0.9895977973937988\n",
      "iteration:  16600\n",
      "Iteration 16600 , the loss is currently 0.9938777685165405\n",
      "iteration:  16610\n",
      "Iteration 16610 , the loss is currently 1.0064867734909058\n",
      "iteration:  16620\n",
      "Iteration 16620 , the loss is currently 0.9919673204421997\n",
      "iteration:  16630\n",
      "Iteration 16630 , the loss is currently 1.0017244815826416\n",
      "iteration:  16640\n",
      "Iteration 16640 , the loss is currently 0.9963361024856567\n",
      "iteration:  16650\n",
      "Iteration 16650 , the loss is currently 0.9937507510185242\n",
      "iteration:  16660\n",
      "Iteration 16660 , the loss is currently 0.9614807963371277\n",
      "iteration:  16670\n",
      "Iteration 16670 , the loss is currently 1.0112743377685547\n",
      "iteration:  16680\n",
      "Iteration 16680 , the loss is currently 1.000440001487732\n",
      "iteration:  16690\n",
      "Iteration 16690 , the loss is currently 0.9884623885154724\n",
      "iteration:  16700\n",
      "Iteration 16700 , the loss is currently 0.9923269748687744\n",
      "iteration:  16710\n",
      "Iteration 16710 , the loss is currently 0.9766452312469482\n",
      "iteration:  16720\n",
      "Iteration 16720 , the loss is currently 0.9940199255943298\n",
      "iteration:  16730\n",
      "Iteration 16730 , the loss is currently 0.9982169270515442\n",
      "iteration:  16740\n",
      "Iteration 16740 , the loss is currently 0.995227575302124\n",
      "iteration:  16750\n",
      "Iteration 16750 , the loss is currently 0.983932375907898\n",
      "iteration:  16760\n",
      "Iteration 16760 , the loss is currently 1.0096819400787354\n",
      "iteration:  16770\n",
      "Iteration 16770 , the loss is currently 0.9902560114860535\n",
      "iteration:  16780\n",
      "Iteration 16780 , the loss is currently 0.9956550598144531\n",
      "iteration:  16790\n",
      "Iteration 16790 , the loss is currently 0.975886344909668\n",
      "iteration:  16800\n",
      "Iteration 16800 , the loss is currently 0.9914497137069702\n",
      "iteration:  16810\n",
      "Iteration 16810 , the loss is currently 0.9788013100624084\n",
      "iteration:  16820\n",
      "Iteration 16820 , the loss is currently 0.9833713173866272\n",
      "iteration:  16830\n",
      "Iteration 16830 , the loss is currently 0.9908088445663452\n",
      "iteration:  16840\n",
      "Iteration 16840 , the loss is currently 0.9687331914901733\n",
      "iteration:  16850\n",
      "Iteration 16850 , the loss is currently 0.9883049130439758\n",
      "iteration:  16860\n",
      "Iteration 16860 , the loss is currently 0.9989542365074158\n",
      "iteration:  16870\n",
      "Iteration 16870 , the loss is currently 1.0012637376785278\n",
      "iteration:  16880\n",
      "Iteration 16880 , the loss is currently 0.9840250015258789\n",
      "iteration:  16890\n",
      "Iteration 16890 , the loss is currently 1.0090889930725098\n",
      "iteration:  16900\n",
      "Iteration 16900 , the loss is currently 0.9872039556503296\n",
      "iteration:  16910\n",
      "Iteration 16910 , the loss is currently 0.9534565210342407\n",
      "iteration:  16920\n",
      "Iteration 16920 , the loss is currently 0.9904155731201172\n",
      "iteration:  16930\n",
      "Iteration 16930 , the loss is currently 0.9919162392616272\n",
      "iteration:  16940\n",
      "Iteration 16940 , the loss is currently 1.0165287256240845\n",
      "iteration:  16950\n",
      "Iteration 16950 , the loss is currently 1.0048335790634155\n",
      "iteration:  16960\n",
      "Iteration 16960 , the loss is currently 1.0044105052947998\n",
      "iteration:  16970\n",
      "Iteration 16970 , the loss is currently 0.9873532652854919\n",
      "iteration:  16980\n",
      "Iteration 16980 , the loss is currently 0.9969869256019592\n",
      "iteration:  16990\n",
      "Iteration 16990 , the loss is currently 0.9777691960334778\n",
      "iteration:  17000\n",
      "Iteration 17000 , the loss is currently 0.9798530340194702\n",
      "iteration:  17010\n",
      "Iteration 17010 , the loss is currently 0.9859751462936401\n",
      "iteration:  17020\n",
      "Iteration 17020 , the loss is currently 0.9769189953804016\n",
      "iteration:  17030\n",
      "Iteration 17030 , the loss is currently 0.9807913899421692\n",
      "iteration:  17040\n",
      "Iteration 17040 , the loss is currently 0.9907159209251404\n",
      "iteration:  17050\n",
      "Iteration 17050 , the loss is currently 1.0138908624649048\n",
      "iteration:  17060\n",
      "Iteration 17060 , the loss is currently 0.968003511428833\n",
      "iteration:  17070\n",
      "Iteration 17070 , the loss is currently 0.9823617339134216\n",
      "iteration:  17080\n",
      "Iteration 17080 , the loss is currently 0.9853284358978271\n",
      "iteration:  17090\n",
      "Iteration 17090 , the loss is currently 0.9698753356933594\n",
      "iteration:  17100\n",
      "Iteration 17100 , the loss is currently 0.9825066328048706\n",
      "iteration:  17110\n",
      "Iteration 17110 , the loss is currently 0.9859888553619385\n",
      "iteration:  17120\n",
      "Iteration 17120 , the loss is currently 0.9679506421089172\n",
      "iteration:  17130\n",
      "Iteration 17130 , the loss is currently 0.9758678674697876\n",
      "iteration:  17140\n",
      "Iteration 17140 , the loss is currently 0.9616617560386658\n",
      "iteration:  17150\n",
      "Iteration 17150 , the loss is currently 1.002939224243164\n",
      "iteration:  17160\n",
      "Iteration 17160 , the loss is currently 0.9972246885299683\n",
      "iteration:  17170\n",
      "Iteration 17170 , the loss is currently 0.9772278666496277\n",
      "iteration:  17180\n",
      "Iteration 17180 , the loss is currently 0.9929021000862122\n",
      "iteration:  17190\n",
      "Iteration 17190 , the loss is currently 0.9967605471611023\n",
      "iteration:  17200\n",
      "Iteration 17200 , the loss is currently 0.9816334247589111\n",
      "iteration:  17210\n",
      "Iteration 17210 , the loss is currently 1.000779390335083\n",
      "iteration:  17220\n",
      "Iteration 17220 , the loss is currently 0.991631269454956\n",
      "iteration:  17230\n",
      "Iteration 17230 , the loss is currently 0.9934965968132019\n",
      "iteration:  17240\n",
      "Iteration 17240 , the loss is currently 0.9975681304931641\n",
      "iteration:  17250\n",
      "Iteration 17250 , the loss is currently 1.0024605989456177\n",
      "iteration:  17260\n",
      "Iteration 17260 , the loss is currently 0.9865441918373108\n",
      "iteration:  17270\n",
      "Iteration 17270 , the loss is currently 0.996692955493927\n",
      "iteration:  17280\n",
      "Iteration 17280 , the loss is currently 0.9763966798782349\n",
      "iteration:  17290\n",
      "Iteration 17290 , the loss is currently 1.0011472702026367\n",
      "iteration:  17300\n",
      "Iteration 17300 , the loss is currently 1.0008113384246826\n",
      "iteration:  17310\n",
      "Iteration 17310 , the loss is currently 1.0029772520065308\n",
      "iteration:  17320\n",
      "Iteration 17320 , the loss is currently 1.0003958940505981\n",
      "iteration:  17330\n",
      "Iteration 17330 , the loss is currently 0.961122453212738\n",
      "iteration:  17340\n",
      "Iteration 17340 , the loss is currently 0.9915753602981567\n",
      "iteration:  17350\n",
      "Iteration 17350 , the loss is currently 1.0087823867797852\n",
      "iteration:  17360\n",
      "Iteration 17360 , the loss is currently 0.983195960521698\n",
      "iteration:  17370\n",
      "Iteration 17370 , the loss is currently 0.9791670441627502\n",
      "iteration:  17380\n",
      "Iteration 17380 , the loss is currently 0.9898389577865601\n",
      "iteration:  17390\n",
      "Iteration 17390 , the loss is currently 1.0006130933761597\n",
      "iteration:  17400\n",
      "Iteration 17400 , the loss is currently 0.9779175519943237\n",
      "iteration:  17410\n",
      "Iteration 17410 , the loss is currently 0.9913696646690369\n",
      "iteration:  17420\n",
      "Iteration 17420 , the loss is currently 1.003355860710144\n",
      "iteration:  17430\n",
      "Iteration 17430 , the loss is currently 0.9845424294471741\n",
      "iteration:  17440\n",
      "Iteration 17440 , the loss is currently 0.9770964980125427\n",
      "iteration:  17450\n",
      "Iteration 17450 , the loss is currently 0.9873495697975159\n",
      "iteration:  17460\n",
      "Iteration 17460 , the loss is currently 1.0163730382919312\n",
      "iteration:  17470\n",
      "Iteration 17470 , the loss is currently 0.9795990586280823\n",
      "iteration:  17480\n",
      "Iteration 17480 , the loss is currently 0.9980841875076294\n",
      "iteration:  17490\n",
      "Iteration 17490 , the loss is currently 1.008100152015686\n",
      "iteration:  17500\n",
      "Iteration 17500 , the loss is currently 0.9909395575523376\n",
      "iteration:  17510\n",
      "Iteration 17510 , the loss is currently 0.9948140978813171\n",
      "iteration:  17520\n",
      "Iteration 17520 , the loss is currently 1.001313328742981\n",
      "iteration:  17530\n",
      "Iteration 17530 , the loss is currently 1.0046197175979614\n",
      "iteration:  17540\n",
      "Iteration 17540 , the loss is currently 0.9644250869750977\n",
      "iteration:  17550\n",
      "Iteration 17550 , the loss is currently 1.0034286975860596\n",
      "iteration:  17560\n",
      "Iteration 17560 , the loss is currently 0.9824554920196533\n",
      "iteration:  17570\n",
      "Iteration 17570 , the loss is currently 0.9793994426727295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  17580\n",
      "Iteration 17580 , the loss is currently 1.0013272762298584\n",
      "iteration:  17590\n",
      "Iteration 17590 , the loss is currently 1.0137308835983276\n",
      "iteration:  17600\n",
      "Iteration 17600 , the loss is currently 0.9986933469772339\n",
      "iteration:  17610\n",
      "Iteration 17610 , the loss is currently 0.989320695400238\n",
      "iteration:  17620\n",
      "Iteration 17620 , the loss is currently 0.9832044243812561\n",
      "iteration:  17630\n",
      "Iteration 17630 , the loss is currently 0.9716085195541382\n",
      "iteration:  17640\n",
      "Iteration 17640 , the loss is currently 0.9749566316604614\n",
      "iteration:  17650\n",
      "Iteration 17650 , the loss is currently 0.9925912022590637\n",
      "iteration:  17660\n",
      "Iteration 17660 , the loss is currently 0.9887502789497375\n",
      "iteration:  17670\n",
      "Iteration 17670 , the loss is currently 0.9663140773773193\n",
      "iteration:  17680\n",
      "Iteration 17680 , the loss is currently 1.005567193031311\n",
      "iteration:  17690\n",
      "Iteration 17690 , the loss is currently 0.9854892492294312\n",
      "iteration:  17700\n",
      "Iteration 17700 , the loss is currently 0.9780148267745972\n",
      "iteration:  17710\n",
      "Iteration 17710 , the loss is currently 0.9675931930541992\n",
      "iteration:  17720\n",
      "Iteration 17720 , the loss is currently 0.9681283235549927\n",
      "iteration:  17730\n",
      "Iteration 17730 , the loss is currently 0.9762284159660339\n",
      "iteration:  17740\n",
      "Iteration 17740 , the loss is currently 1.0088496208190918\n",
      "iteration:  17750\n",
      "Iteration 17750 , the loss is currently 0.9999575614929199\n",
      "iteration:  17760\n",
      "Iteration 17760 , the loss is currently 0.9858668446540833\n",
      "iteration:  17770\n",
      "Iteration 17770 , the loss is currently 0.9929938912391663\n",
      "iteration:  17780\n",
      "Iteration 17780 , the loss is currently 0.9873521327972412\n",
      "iteration:  17790\n",
      "Iteration 17790 , the loss is currently 0.9983428716659546\n",
      "iteration:  17800\n",
      "Iteration 17800 , the loss is currently 0.9828846454620361\n",
      "iteration:  17810\n",
      "Iteration 17810 , the loss is currently 0.9722542762756348\n",
      "iteration:  17820\n",
      "Iteration 17820 , the loss is currently 0.9797011017799377\n",
      "iteration:  17830\n",
      "Iteration 17830 , the loss is currently 0.9922312498092651\n",
      "iteration:  17840\n",
      "Iteration 17840 , the loss is currently 0.9852501153945923\n",
      "iteration:  17850\n",
      "Iteration 17850 , the loss is currently 0.974122166633606\n",
      "iteration:  17860\n",
      "Iteration 17860 , the loss is currently 0.9949861168861389\n",
      "iteration:  17870\n",
      "Iteration 17870 , the loss is currently 0.9923185110092163\n",
      "iteration:  17880\n",
      "Iteration 17880 , the loss is currently 0.9962663054466248\n",
      "iteration:  17890\n",
      "Iteration 17890 , the loss is currently 0.9873301982879639\n",
      "iteration:  17900\n",
      "Iteration 17900 , the loss is currently 0.9866886138916016\n",
      "iteration:  17910\n",
      "Iteration 17910 , the loss is currently 0.9979288578033447\n",
      "iteration:  17920\n",
      "Iteration 17920 , the loss is currently 0.9963853359222412\n",
      "iteration:  17930\n",
      "Iteration 17930 , the loss is currently 0.9685250520706177\n",
      "iteration:  17940\n",
      "Iteration 17940 , the loss is currently 0.9940426349639893\n",
      "iteration:  17950\n",
      "Iteration 17950 , the loss is currently 0.9675979018211365\n",
      "iteration:  17960\n",
      "Iteration 17960 , the loss is currently 0.9850099086761475\n",
      "iteration:  17970\n",
      "Iteration 17970 , the loss is currently 0.9780449271202087\n",
      "iteration:  17980\n",
      "Iteration 17980 , the loss is currently 0.997179388999939\n",
      "iteration:  17990\n",
      "Iteration 17990 , the loss is currently 0.9955980777740479\n",
      "iteration:  18000\n",
      "Iteration 18000 , the loss is currently 0.9951221942901611\n",
      "iteration:  18010\n",
      "Iteration 18010 , the loss is currently 0.9980942010879517\n",
      "iteration:  18020\n",
      "Iteration 18020 , the loss is currently 0.9930678009986877\n",
      "iteration:  18030\n",
      "Iteration 18030 , the loss is currently 0.9833290576934814\n",
      "iteration:  18040\n",
      "Iteration 18040 , the loss is currently 0.9991888999938965\n",
      "iteration:  18050\n",
      "Iteration 18050 , the loss is currently 0.9919121861457825\n",
      "iteration:  18060\n",
      "Iteration 18060 , the loss is currently 0.99027419090271\n",
      "iteration:  18070\n",
      "Iteration 18070 , the loss is currently 0.9859746694564819\n",
      "iteration:  18080\n",
      "Iteration 18080 , the loss is currently 0.9924486875534058\n",
      "iteration:  18090\n",
      "Iteration 18090 , the loss is currently 1.0091145038604736\n",
      "iteration:  18100\n",
      "Iteration 18100 , the loss is currently 0.9687543511390686\n",
      "iteration:  18110\n",
      "Iteration 18110 , the loss is currently 0.9765335917472839\n",
      "iteration:  18120\n",
      "Iteration 18120 , the loss is currently 0.9942308068275452\n",
      "iteration:  18130\n",
      "Iteration 18130 , the loss is currently 1.003683090209961\n",
      "iteration:  18140\n",
      "Iteration 18140 , the loss is currently 0.9944989681243896\n",
      "iteration:  18150\n",
      "Iteration 18150 , the loss is currently 0.9829021692276001\n",
      "iteration:  18160\n",
      "Iteration 18160 , the loss is currently 1.0121091604232788\n",
      "iteration:  18170\n",
      "Iteration 18170 , the loss is currently 0.9846541881561279\n",
      "iteration:  18180\n",
      "Iteration 18180 , the loss is currently 1.0033272504806519\n",
      "iteration:  18190\n",
      "Iteration 18190 , the loss is currently 0.9969508647918701\n",
      "iteration:  18200\n",
      "Iteration 18200 , the loss is currently 0.9905192852020264\n",
      "iteration:  18210\n",
      "Iteration 18210 , the loss is currently 0.9912810921669006\n",
      "iteration:  18220\n",
      "Iteration 18220 , the loss is currently 0.9904473423957825\n",
      "iteration:  18230\n",
      "Iteration 18230 , the loss is currently 0.9965681433677673\n",
      "iteration:  18240\n",
      "Iteration 18240 , the loss is currently 0.9850324392318726\n",
      "iteration:  18250\n",
      "Iteration 18250 , the loss is currently 0.9926412105560303\n",
      "iteration:  18260\n",
      "Iteration 18260 , the loss is currently 1.0008022785186768\n",
      "iteration:  18270\n",
      "Iteration 18270 , the loss is currently 1.00411057472229\n",
      "iteration:  18280\n",
      "Iteration 18280 , the loss is currently 0.9944844245910645\n",
      "iteration:  18290\n",
      "Iteration 18290 , the loss is currently 0.9989131689071655\n",
      "iteration:  18300\n",
      "Iteration 18300 , the loss is currently 1.0017125606536865\n",
      "iteration:  18310\n",
      "Iteration 18310 , the loss is currently 0.9944170713424683\n",
      "iteration:  18320\n",
      "Iteration 18320 , the loss is currently 0.9893370270729065\n",
      "iteration:  18330\n",
      "Iteration 18330 , the loss is currently 0.9847612977027893\n",
      "iteration:  18340\n",
      "Iteration 18340 , the loss is currently 0.9874829649925232\n",
      "iteration:  18350\n",
      "Iteration 18350 , the loss is currently 1.0015066862106323\n",
      "iteration:  18360\n",
      "Iteration 18360 , the loss is currently 0.9913821816444397\n",
      "iteration:  18370\n",
      "Iteration 18370 , the loss is currently 0.9896041750907898\n",
      "iteration:  18380\n",
      "Iteration 18380 , the loss is currently 0.9851918816566467\n",
      "iteration:  18390\n",
      "Iteration 18390 , the loss is currently 0.9971434473991394\n",
      "iteration:  18400\n",
      "Iteration 18400 , the loss is currently 0.9958716630935669\n",
      "iteration:  18410\n",
      "Iteration 18410 , the loss is currently 0.9864329695701599\n",
      "iteration:  18420\n",
      "Iteration 18420 , the loss is currently 1.0025664567947388\n",
      "iteration:  18430\n",
      "Iteration 18430 , the loss is currently 0.9910557270050049\n",
      "iteration:  18440\n",
      "Iteration 18440 , the loss is currently 0.9886409640312195\n",
      "iteration:  18450\n",
      "Iteration 18450 , the loss is currently 0.9917529821395874\n",
      "iteration:  18460\n",
      "Iteration 18460 , the loss is currently 0.9760051369667053\n",
      "iteration:  18470\n",
      "Iteration 18470 , the loss is currently 0.9828376173973083\n",
      "iteration:  18480\n",
      "Iteration 18480 , the loss is currently 0.9810965657234192\n",
      "iteration:  18490\n",
      "Iteration 18490 , the loss is currently 0.9643806219100952\n",
      "iteration:  18500\n",
      "Iteration 18500 , the loss is currently 0.9650782942771912\n",
      "iteration:  18510\n",
      "Iteration 18510 , the loss is currently 0.9794554114341736\n",
      "iteration:  18520\n",
      "Iteration 18520 , the loss is currently 0.9939877986907959\n",
      "iteration:  18530\n",
      "Iteration 18530 , the loss is currently 0.9854860901832581\n",
      "iteration:  18540\n",
      "Iteration 18540 , the loss is currently 1.0125796794891357\n",
      "iteration:  18550\n",
      "Iteration 18550 , the loss is currently 0.9814627766609192\n",
      "iteration:  18560\n",
      "Iteration 18560 , the loss is currently 0.9885364770889282\n",
      "iteration:  18570\n",
      "Iteration 18570 , the loss is currently 0.9882950782775879\n",
      "iteration:  18580\n",
      "Iteration 18580 , the loss is currently 1.0071752071380615\n",
      "iteration:  18590\n",
      "Iteration 18590 , the loss is currently 0.9925400614738464\n",
      "iteration:  18600\n",
      "Iteration 18600 , the loss is currently 0.9812135696411133\n",
      "iteration:  18610\n",
      "Iteration 18610 , the loss is currently 0.9761385917663574\n",
      "iteration:  18620\n",
      "Iteration 18620 , the loss is currently 0.9963994026184082\n",
      "iteration:  18630\n",
      "Iteration 18630 , the loss is currently 1.0050859451293945\n",
      "iteration:  18640\n",
      "Iteration 18640 , the loss is currently 1.0011426210403442\n",
      "iteration:  18650\n",
      "Iteration 18650 , the loss is currently 0.9779987335205078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  18660\n",
      "Iteration 18660 , the loss is currently 1.027497410774231\n",
      "iteration:  18670\n",
      "Iteration 18670 , the loss is currently 0.9680482149124146\n",
      "iteration:  18680\n",
      "Iteration 18680 , the loss is currently 0.9893553256988525\n",
      "iteration:  18690\n",
      "Iteration 18690 , the loss is currently 0.9779942035675049\n",
      "iteration:  18700\n",
      "Iteration 18700 , the loss is currently 0.9983576536178589\n",
      "iteration:  18710\n",
      "Iteration 18710 , the loss is currently 0.9840375781059265\n",
      "iteration:  18720\n",
      "Iteration 18720 , the loss is currently 0.9965671896934509\n",
      "iteration:  18730\n",
      "Iteration 18730 , the loss is currently 0.996973991394043\n",
      "iteration:  18740\n",
      "Iteration 18740 , the loss is currently 0.9904482364654541\n",
      "iteration:  18750\n",
      "Iteration 18750 , the loss is currently 0.9989230036735535\n",
      "iteration:  18760\n",
      "Iteration 18760 , the loss is currently 0.9830607175827026\n",
      "iteration:  18770\n",
      "Iteration 18770 , the loss is currently 0.9883025288581848\n",
      "iteration:  18780\n",
      "Iteration 18780 , the loss is currently 0.9943391680717468\n",
      "iteration:  18790\n",
      "Iteration 18790 , the loss is currently 0.9905135035514832\n",
      "iteration:  18800\n",
      "Iteration 18800 , the loss is currently 0.9850054979324341\n",
      "iteration:  18810\n",
      "Iteration 18810 , the loss is currently 0.9615605473518372\n",
      "iteration:  18820\n",
      "Iteration 18820 , the loss is currently 1.0135618448257446\n",
      "iteration:  18830\n",
      "Iteration 18830 , the loss is currently 0.9810603857040405\n",
      "iteration:  18840\n",
      "Iteration 18840 , the loss is currently 1.0113015174865723\n",
      "iteration:  18850\n",
      "Iteration 18850 , the loss is currently 0.9785327315330505\n",
      "iteration:  18860\n",
      "Iteration 18860 , the loss is currently 0.9751929640769958\n",
      "iteration:  18870\n",
      "Iteration 18870 , the loss is currently 0.9832717776298523\n",
      "iteration:  18880\n",
      "Iteration 18880 , the loss is currently 0.984901487827301\n",
      "iteration:  18890\n",
      "Iteration 18890 , the loss is currently 0.9605093598365784\n",
      "iteration:  18900\n",
      "Iteration 18900 , the loss is currently 0.9778074622154236\n",
      "iteration:  18910\n",
      "Iteration 18910 , the loss is currently 1.005153775215149\n",
      "iteration:  18920\n",
      "Iteration 18920 , the loss is currently 0.9793949723243713\n",
      "iteration:  18930\n",
      "Iteration 18930 , the loss is currently 1.0137587785720825\n",
      "iteration:  18940\n",
      "Iteration 18940 , the loss is currently 1.0134871006011963\n",
      "iteration:  18950\n",
      "Iteration 18950 , the loss is currently 0.9748843312263489\n",
      "iteration:  18960\n",
      "Iteration 18960 , the loss is currently 0.9848217964172363\n",
      "iteration:  18970\n",
      "Iteration 18970 , the loss is currently 0.9789639115333557\n",
      "iteration:  18980\n",
      "Iteration 18980 , the loss is currently 0.9887256026268005\n",
      "iteration:  18990\n",
      "Iteration 18990 , the loss is currently 1.0088785886764526\n",
      "iteration:  19000\n",
      "Iteration 19000 , the loss is currently 0.988206684589386\n",
      "iteration:  19010\n",
      "Iteration 19010 , the loss is currently 0.9799183011054993\n",
      "iteration:  19020\n",
      "Iteration 19020 , the loss is currently 0.9973652958869934\n",
      "iteration:  19030\n",
      "Iteration 19030 , the loss is currently 0.9850731492042542\n",
      "iteration:  19040\n",
      "Iteration 19040 , the loss is currently 1.0096564292907715\n",
      "iteration:  19050\n",
      "Iteration 19050 , the loss is currently 0.9684093594551086\n",
      "iteration:  19060\n",
      "Iteration 19060 , the loss is currently 0.9949275851249695\n",
      "iteration:  19070\n",
      "Iteration 19070 , the loss is currently 1.0054720640182495\n",
      "iteration:  19080\n",
      "Iteration 19080 , the loss is currently 0.9911453127861023\n",
      "iteration:  19090\n",
      "Iteration 19090 , the loss is currently 0.9849570989608765\n",
      "iteration:  19100\n",
      "Iteration 19100 , the loss is currently 1.0006330013275146\n",
      "iteration:  19110\n",
      "Iteration 19110 , the loss is currently 1.0095821619033813\n",
      "iteration:  19120\n",
      "Iteration 19120 , the loss is currently 0.9960809946060181\n",
      "iteration:  19130\n",
      "Iteration 19130 , the loss is currently 0.9903894066810608\n",
      "iteration:  19140\n",
      "Iteration 19140 , the loss is currently 0.9814625978469849\n",
      "iteration:  19150\n",
      "Iteration 19150 , the loss is currently 0.995802640914917\n",
      "iteration:  19160\n",
      "Iteration 19160 , the loss is currently 0.9943028092384338\n",
      "iteration:  19170\n",
      "Iteration 19170 , the loss is currently 1.0064127445220947\n",
      "iteration:  19180\n",
      "Iteration 19180 , the loss is currently 0.9751504063606262\n",
      "iteration:  19190\n",
      "Iteration 19190 , the loss is currently 0.990608274936676\n",
      "iteration:  19200\n",
      "Iteration 19200 , the loss is currently 1.0132694244384766\n",
      "iteration:  19210\n",
      "Iteration 19210 , the loss is currently 0.9963870644569397\n",
      "iteration:  19220\n",
      "Iteration 19220 , the loss is currently 0.9822665452957153\n",
      "iteration:  19230\n",
      "Iteration 19230 , the loss is currently 1.0057222843170166\n",
      "iteration:  19240\n",
      "Iteration 19240 , the loss is currently 0.9871158003807068\n",
      "iteration:  19250\n",
      "Iteration 19250 , the loss is currently 0.9928513169288635\n",
      "iteration:  19260\n",
      "Iteration 19260 , the loss is currently 0.9897726774215698\n",
      "iteration:  19270\n",
      "Iteration 19270 , the loss is currently 1.012618064880371\n",
      "iteration:  19280\n",
      "Iteration 19280 , the loss is currently 0.9688882231712341\n",
      "iteration:  19290\n",
      "Iteration 19290 , the loss is currently 0.9934023022651672\n",
      "iteration:  19300\n",
      "Iteration 19300 , the loss is currently 0.9977520704269409\n",
      "iteration:  19310\n",
      "Iteration 19310 , the loss is currently 1.003784418106079\n",
      "iteration:  19320\n",
      "Iteration 19320 , the loss is currently 0.9710897207260132\n",
      "iteration:  19330\n",
      "Iteration 19330 , the loss is currently 1.0088763236999512\n",
      "iteration:  19340\n",
      "Iteration 19340 , the loss is currently 0.9839435815811157\n",
      "iteration:  19350\n",
      "Iteration 19350 , the loss is currently 0.9958468079566956\n",
      "iteration:  19360\n",
      "Iteration 19360 , the loss is currently 0.9884518384933472\n",
      "iteration:  19370\n",
      "Iteration 19370 , the loss is currently 0.9922145009040833\n",
      "iteration:  19380\n",
      "Iteration 19380 , the loss is currently 1.0029792785644531\n",
      "iteration:  19390\n",
      "Iteration 19390 , the loss is currently 1.0004677772521973\n",
      "iteration:  19400\n",
      "Iteration 19400 , the loss is currently 0.9994065761566162\n",
      "iteration:  19410\n",
      "Iteration 19410 , the loss is currently 0.9657828211784363\n",
      "iteration:  19420\n",
      "Iteration 19420 , the loss is currently 0.9928550720214844\n",
      "iteration:  19430\n",
      "Iteration 19430 , the loss is currently 0.9972298741340637\n",
      "iteration:  19440\n",
      "Iteration 19440 , the loss is currently 0.9853137135505676\n",
      "iteration:  19450\n",
      "Iteration 19450 , the loss is currently 0.9897544384002686\n",
      "iteration:  19460\n",
      "Iteration 19460 , the loss is currently 0.992134690284729\n",
      "iteration:  19470\n",
      "Iteration 19470 , the loss is currently 0.9971708059310913\n",
      "iteration:  19480\n",
      "Iteration 19480 , the loss is currently 1.0128238201141357\n",
      "iteration:  19490\n",
      "Iteration 19490 , the loss is currently 0.9970528483390808\n",
      "iteration:  19500\n",
      "Iteration 19500 , the loss is currently 0.9801331162452698\n",
      "iteration:  19510\n",
      "Iteration 19510 , the loss is currently 1.0003458261489868\n",
      "iteration:  19520\n",
      "Iteration 19520 , the loss is currently 0.9883731007575989\n",
      "iteration:  19530\n",
      "Iteration 19530 , the loss is currently 0.9766800403594971\n",
      "iteration:  19540\n",
      "Iteration 19540 , the loss is currently 1.0069142580032349\n",
      "iteration:  19550\n",
      "Iteration 19550 , the loss is currently 0.9834024310112\n",
      "iteration:  19560\n",
      "Iteration 19560 , the loss is currently 0.9654310941696167\n",
      "iteration:  19570\n",
      "Iteration 19570 , the loss is currently 0.9966837763786316\n",
      "iteration:  19580\n",
      "Iteration 19580 , the loss is currently 1.0028531551361084\n",
      "iteration:  19590\n",
      "Iteration 19590 , the loss is currently 1.016567349433899\n",
      "iteration:  19600\n",
      "Iteration 19600 , the loss is currently 0.9804990291595459\n",
      "iteration:  19610\n",
      "Iteration 19610 , the loss is currently 1.011997103691101\n",
      "iteration:  19620\n",
      "Iteration 19620 , the loss is currently 1.0076994895935059\n",
      "iteration:  19630\n",
      "Iteration 19630 , the loss is currently 0.9908434152603149\n",
      "iteration:  19640\n",
      "Iteration 19640 , the loss is currently 0.9762658476829529\n",
      "iteration:  19650\n",
      "Iteration 19650 , the loss is currently 0.9721644520759583\n",
      "iteration:  19660\n",
      "Iteration 19660 , the loss is currently 0.9980714917182922\n",
      "iteration:  19670\n",
      "Iteration 19670 , the loss is currently 0.9942299127578735\n",
      "iteration:  19680\n",
      "Iteration 19680 , the loss is currently 0.995392918586731\n",
      "iteration:  19690\n",
      "Iteration 19690 , the loss is currently 0.983352780342102\n",
      "iteration:  19700\n",
      "Iteration 19700 , the loss is currently 0.9798817038536072\n",
      "iteration:  19710\n",
      "Iteration 19710 , the loss is currently 0.9983746409416199\n",
      "iteration:  19720\n",
      "Iteration 19720 , the loss is currently 0.9968308210372925\n",
      "iteration:  19730\n",
      "Iteration 19730 , the loss is currently 0.9930391907691956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  19740\n",
      "Iteration 19740 , the loss is currently 0.9920167326927185\n",
      "iteration:  19750\n",
      "Iteration 19750 , the loss is currently 1.0000993013381958\n",
      "iteration:  19760\n",
      "Iteration 19760 , the loss is currently 1.0095094442367554\n",
      "iteration:  19770\n",
      "Iteration 19770 , the loss is currently 0.9978790283203125\n",
      "iteration:  19780\n",
      "Iteration 19780 , the loss is currently 1.0032709836959839\n",
      "iteration:  19790\n",
      "Iteration 19790 , the loss is currently 1.0015290975570679\n",
      "iteration:  19800\n",
      "Iteration 19800 , the loss is currently 0.9785874485969543\n",
      "iteration:  19810\n",
      "Iteration 19810 , the loss is currently 0.9890326261520386\n",
      "iteration:  19820\n",
      "Iteration 19820 , the loss is currently 1.0049214363098145\n",
      "iteration:  19830\n",
      "Iteration 19830 , the loss is currently 0.9956891536712646\n",
      "iteration:  19840\n",
      "Iteration 19840 , the loss is currently 0.9907717108726501\n",
      "iteration:  19850\n",
      "Iteration 19850 , the loss is currently 0.9978715181350708\n",
      "iteration:  19860\n",
      "Iteration 19860 , the loss is currently 0.9768799543380737\n",
      "iteration:  19870\n",
      "Iteration 19870 , the loss is currently 0.9836331605911255\n",
      "iteration:  19880\n",
      "Iteration 19880 , the loss is currently 1.0059740543365479\n",
      "iteration:  19890\n",
      "Iteration 19890 , the loss is currently 1.0054163932800293\n",
      "iteration:  19900\n",
      "Iteration 19900 , the loss is currently 0.9890360832214355\n",
      "iteration:  19910\n",
      "Iteration 19910 , the loss is currently 0.9981964230537415\n",
      "iteration:  19920\n",
      "Iteration 19920 , the loss is currently 0.9953858256340027\n",
      "iteration:  19930\n",
      "Iteration 19930 , the loss is currently 0.9825100302696228\n",
      "iteration:  19940\n",
      "Iteration 19940 , the loss is currently 0.9808971285820007\n",
      "iteration:  19950\n",
      "Iteration 19950 , the loss is currently 0.9914762377738953\n",
      "iteration:  19960\n",
      "Iteration 19960 , the loss is currently 0.9767910838127136\n",
      "iteration:  19970\n",
      "Iteration 19970 , the loss is currently 0.9770949482917786\n",
      "iteration:  19980\n",
      "Iteration 19980 , the loss is currently 1.0014086961746216\n",
      "iteration:  19990\n",
      "Iteration 19990 , the loss is currently 0.9851592183113098\n",
      "iteration:  20000\n",
      "Iteration 20000 , the loss is currently 1.0206514596939087\n",
      "iteration:  20010\n",
      "Iteration 20010 , the loss is currently 0.9829926490783691\n",
      "iteration:  20020\n",
      "Iteration 20020 , the loss is currently 0.9781935214996338\n",
      "iteration:  20030\n",
      "Iteration 20030 , the loss is currently 0.9996792674064636\n",
      "iteration:  20040\n",
      "Iteration 20040 , the loss is currently 0.9863736033439636\n",
      "iteration:  20050\n",
      "Iteration 20050 , the loss is currently 0.9763026237487793\n",
      "iteration:  20060\n",
      "Iteration 20060 , the loss is currently 1.002803087234497\n",
      "iteration:  20070\n",
      "Iteration 20070 , the loss is currently 0.9917922019958496\n",
      "iteration:  20080\n",
      "Iteration 20080 , the loss is currently 0.9903197884559631\n",
      "iteration:  20090\n",
      "Iteration 20090 , the loss is currently 0.9690038561820984\n",
      "iteration:  20100\n",
      "Iteration 20100 , the loss is currently 0.9850594401359558\n",
      "iteration:  20110\n",
      "Iteration 20110 , the loss is currently 0.9904201030731201\n",
      "iteration:  20120\n",
      "Iteration 20120 , the loss is currently 0.9692570567131042\n",
      "iteration:  20130\n",
      "Iteration 20130 , the loss is currently 0.9956064224243164\n",
      "iteration:  20140\n",
      "Iteration 20140 , the loss is currently 0.9758663773536682\n",
      "iteration:  20150\n",
      "Iteration 20150 , the loss is currently 0.9877424240112305\n",
      "iteration:  20160\n",
      "Iteration 20160 , the loss is currently 0.9973586201667786\n",
      "iteration:  20170\n",
      "Iteration 20170 , the loss is currently 0.9756003022193909\n",
      "iteration:  20180\n",
      "Iteration 20180 , the loss is currently 0.9792256355285645\n",
      "iteration:  20190\n",
      "Iteration 20190 , the loss is currently 0.9854711294174194\n",
      "iteration:  20200\n",
      "Iteration 20200 , the loss is currently 0.9926132559776306\n",
      "iteration:  20210\n",
      "Iteration 20210 , the loss is currently 1.000220775604248\n",
      "iteration:  20220\n",
      "Iteration 20220 , the loss is currently 0.9830977320671082\n",
      "iteration:  20230\n",
      "Iteration 20230 , the loss is currently 0.9755586981773376\n",
      "iteration:  20240\n",
      "Iteration 20240 , the loss is currently 1.0017759799957275\n",
      "iteration:  20250\n",
      "Iteration 20250 , the loss is currently 0.9906896352767944\n",
      "iteration:  20260\n",
      "Iteration 20260 , the loss is currently 0.9908927083015442\n",
      "iteration:  20270\n",
      "Iteration 20270 , the loss is currently 0.997478187084198\n",
      "iteration:  20280\n",
      "Iteration 20280 , the loss is currently 1.0068304538726807\n",
      "iteration:  20290\n",
      "Iteration 20290 , the loss is currently 0.98405921459198\n",
      "iteration:  20300\n",
      "Iteration 20300 , the loss is currently 0.9715185165405273\n",
      "iteration:  20310\n",
      "Iteration 20310 , the loss is currently 0.9755155444145203\n",
      "iteration:  20320\n",
      "Iteration 20320 , the loss is currently 0.9979795217514038\n",
      "iteration:  20330\n",
      "Iteration 20330 , the loss is currently 0.9959296584129333\n",
      "iteration:  20340\n",
      "Iteration 20340 , the loss is currently 1.0146875381469727\n",
      "iteration:  20350\n",
      "Iteration 20350 , the loss is currently 0.9885980486869812\n",
      "iteration:  20360\n",
      "Iteration 20360 , the loss is currently 0.9910997748374939\n",
      "iteration:  20370\n",
      "Iteration 20370 , the loss is currently 0.9868289232254028\n",
      "iteration:  20380\n",
      "Iteration 20380 , the loss is currently 0.9773880243301392\n",
      "iteration:  20390\n",
      "Iteration 20390 , the loss is currently 0.9885649681091309\n",
      "iteration:  20400\n",
      "Iteration 20400 , the loss is currently 0.9985141754150391\n",
      "iteration:  20410\n",
      "Iteration 20410 , the loss is currently 0.9851385951042175\n",
      "iteration:  20420\n",
      "Iteration 20420 , the loss is currently 1.007977843284607\n",
      "iteration:  20430\n",
      "Iteration 20430 , the loss is currently 0.9949048161506653\n",
      "iteration:  20440\n",
      "Iteration 20440 , the loss is currently 1.015899419784546\n",
      "iteration:  20450\n",
      "Iteration 20450 , the loss is currently 0.9772812724113464\n",
      "iteration:  20460\n",
      "Iteration 20460 , the loss is currently 0.9853102564811707\n",
      "iteration:  20470\n",
      "Iteration 20470 , the loss is currently 0.9940732717514038\n",
      "iteration:  20480\n",
      "Iteration 20480 , the loss is currently 0.9910771250724792\n",
      "iteration:  20490\n",
      "Iteration 20490 , the loss is currently 0.9733265042304993\n",
      "iteration:  20500\n",
      "Iteration 20500 , the loss is currently 0.9695537090301514\n",
      "iteration:  20510\n",
      "Iteration 20510 , the loss is currently 0.9914247989654541\n",
      "iteration:  20520\n",
      "Iteration 20520 , the loss is currently 0.975351870059967\n",
      "iteration:  20530\n",
      "Iteration 20530 , the loss is currently 0.9778017997741699\n",
      "iteration:  20540\n",
      "Iteration 20540 , the loss is currently 0.9934067130088806\n",
      "iteration:  20550\n",
      "Iteration 20550 , the loss is currently 1.0012109279632568\n",
      "iteration:  20560\n",
      "Iteration 20560 , the loss is currently 0.9864612221717834\n",
      "iteration:  20570\n",
      "Iteration 20570 , the loss is currently 1.0129681825637817\n",
      "iteration:  20580\n",
      "Iteration 20580 , the loss is currently 0.9968754649162292\n",
      "iteration:  20590\n",
      "Iteration 20590 , the loss is currently 0.9907123446464539\n",
      "iteration:  20600\n",
      "Iteration 20600 , the loss is currently 1.0087990760803223\n",
      "iteration:  20610\n",
      "Iteration 20610 , the loss is currently 0.9872236251831055\n",
      "iteration:  20620\n",
      "Iteration 20620 , the loss is currently 0.9872034192085266\n",
      "iteration:  20630\n",
      "Iteration 20630 , the loss is currently 0.9852016568183899\n",
      "iteration:  20640\n",
      "Iteration 20640 , the loss is currently 1.0171318054199219\n",
      "iteration:  20650\n",
      "Iteration 20650 , the loss is currently 1.0082107782363892\n",
      "iteration:  20660\n",
      "Iteration 20660 , the loss is currently 1.017622947692871\n",
      "iteration:  20670\n",
      "Iteration 20670 , the loss is currently 0.9840103983879089\n",
      "iteration:  20680\n",
      "Iteration 20680 , the loss is currently 0.9919981956481934\n",
      "iteration:  20690\n",
      "Iteration 20690 , the loss is currently 1.0165783166885376\n",
      "iteration:  20700\n",
      "Iteration 20700 , the loss is currently 0.9907454252243042\n",
      "iteration:  20710\n",
      "Iteration 20710 , the loss is currently 1.010353922843933\n",
      "iteration:  20720\n",
      "Iteration 20720 , the loss is currently 0.9940888285636902\n",
      "iteration:  20730\n",
      "Iteration 20730 , the loss is currently 1.0009022951126099\n",
      "iteration:  20740\n",
      "Iteration 20740 , the loss is currently 0.984981119632721\n",
      "iteration:  20750\n",
      "Iteration 20750 , the loss is currently 0.9875748157501221\n",
      "iteration:  20760\n",
      "Iteration 20760 , the loss is currently 0.9821925759315491\n",
      "iteration:  20770\n",
      "Iteration 20770 , the loss is currently 0.9839089512825012\n",
      "iteration:  20780\n",
      "Iteration 20780 , the loss is currently 0.9996048212051392\n",
      "iteration:  20790\n",
      "Iteration 20790 , the loss is currently 0.9680400490760803\n",
      "iteration:  20800\n",
      "Iteration 20800 , the loss is currently 1.0134384632110596\n",
      "iteration:  20810\n",
      "Iteration 20810 , the loss is currently 0.9851212501525879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  20820\n",
      "Iteration 20820 , the loss is currently 0.9747292995452881\n",
      "iteration:  20830\n",
      "Iteration 20830 , the loss is currently 0.9927077889442444\n",
      "iteration:  20840\n",
      "Iteration 20840 , the loss is currently 0.9994415640830994\n",
      "iteration:  20850\n",
      "Iteration 20850 , the loss is currently 0.9967254400253296\n",
      "iteration:  20860\n",
      "Iteration 20860 , the loss is currently 1.0054774284362793\n",
      "iteration:  20870\n",
      "Iteration 20870 , the loss is currently 0.9871771335601807\n",
      "iteration:  20880\n",
      "Iteration 20880 , the loss is currently 0.9890139102935791\n",
      "iteration:  20890\n",
      "Iteration 20890 , the loss is currently 0.9846139550209045\n",
      "iteration:  20900\n",
      "Iteration 20900 , the loss is currently 0.9970303177833557\n",
      "iteration:  20910\n",
      "Iteration 20910 , the loss is currently 0.9765793681144714\n",
      "iteration:  20920\n",
      "Iteration 20920 , the loss is currently 1.017295479774475\n",
      "iteration:  20930\n",
      "Iteration 20930 , the loss is currently 0.9728692770004272\n",
      "iteration:  20940\n",
      "Iteration 20940 , the loss is currently 0.9865672588348389\n",
      "iteration:  20950\n",
      "Iteration 20950 , the loss is currently 0.9860826730728149\n",
      "iteration:  20960\n",
      "Iteration 20960 , the loss is currently 1.0066977739334106\n",
      "iteration:  20970\n",
      "Iteration 20970 , the loss is currently 1.014125943183899\n",
      "iteration:  20980\n",
      "Iteration 20980 , the loss is currently 0.9873049259185791\n",
      "iteration:  20990\n",
      "Iteration 20990 , the loss is currently 0.9928120374679565\n",
      "iteration:  21000\n",
      "Iteration 21000 , the loss is currently 0.973749041557312\n",
      "iteration:  21010\n",
      "Iteration 21010 , the loss is currently 0.992163360118866\n",
      "iteration:  21020\n",
      "Iteration 21020 , the loss is currently 0.9826653003692627\n",
      "iteration:  21030\n",
      "Iteration 21030 , the loss is currently 1.0046056509017944\n",
      "iteration:  21040\n",
      "Iteration 21040 , the loss is currently 1.0035614967346191\n",
      "iteration:  21050\n",
      "Iteration 21050 , the loss is currently 0.9812688827514648\n",
      "iteration:  21060\n",
      "Iteration 21060 , the loss is currently 0.9970194697380066\n",
      "iteration:  21070\n",
      "Iteration 21070 , the loss is currently 0.9948980212211609\n",
      "iteration:  21080\n",
      "Iteration 21080 , the loss is currently 0.9871224761009216\n",
      "iteration:  21090\n",
      "Iteration 21090 , the loss is currently 1.0080738067626953\n",
      "iteration:  21100\n",
      "Iteration 21100 , the loss is currently 0.986271858215332\n",
      "iteration:  21110\n",
      "Iteration 21110 , the loss is currently 1.002569317817688\n",
      "iteration:  21120\n",
      "Iteration 21120 , the loss is currently 0.9922681450843811\n",
      "iteration:  21130\n",
      "Iteration 21130 , the loss is currently 0.9865143299102783\n",
      "iteration:  21140\n",
      "Iteration 21140 , the loss is currently 0.9966238141059875\n",
      "iteration:  21150\n",
      "Iteration 21150 , the loss is currently 0.9925387501716614\n",
      "iteration:  21160\n",
      "Iteration 21160 , the loss is currently 1.0072249174118042\n",
      "iteration:  21170\n",
      "Iteration 21170 , the loss is currently 0.9966174960136414\n",
      "iteration:  21180\n",
      "Iteration 21180 , the loss is currently 1.0107402801513672\n",
      "iteration:  21190\n",
      "Iteration 21190 , the loss is currently 0.997076153755188\n",
      "iteration:  21200\n",
      "Iteration 21200 , the loss is currently 0.9898001551628113\n",
      "iteration:  21210\n",
      "Iteration 21210 , the loss is currently 0.9930419325828552\n",
      "iteration:  21220\n",
      "Iteration 21220 , the loss is currently 0.9949860572814941\n",
      "iteration:  21230\n",
      "Iteration 21230 , the loss is currently 0.9706627130508423\n",
      "iteration:  21240\n",
      "Iteration 21240 , the loss is currently 0.9816622138023376\n",
      "iteration:  21250\n",
      "Iteration 21250 , the loss is currently 0.9821163415908813\n",
      "iteration:  21260\n",
      "Iteration 21260 , the loss is currently 1.005384087562561\n",
      "iteration:  21270\n",
      "Iteration 21270 , the loss is currently 0.9862800240516663\n",
      "iteration:  21280\n",
      "Iteration 21280 , the loss is currently 0.9649357795715332\n",
      "iteration:  21290\n",
      "Iteration 21290 , the loss is currently 0.9852719902992249\n",
      "iteration:  21300\n",
      "Iteration 21300 , the loss is currently 0.9853175282478333\n",
      "iteration:  21310\n",
      "Iteration 21310 , the loss is currently 1.0008437633514404\n",
      "iteration:  21320\n",
      "Iteration 21320 , the loss is currently 1.0056627988815308\n",
      "iteration:  21330\n",
      "Iteration 21330 , the loss is currently 1.0047253370285034\n",
      "iteration:  21340\n",
      "Iteration 21340 , the loss is currently 0.9854902029037476\n",
      "iteration:  21350\n",
      "Iteration 21350 , the loss is currently 0.9971814155578613\n",
      "iteration:  21360\n",
      "Iteration 21360 , the loss is currently 0.9650416374206543\n",
      "iteration:  21370\n",
      "Iteration 21370 , the loss is currently 0.9878561496734619\n",
      "iteration:  21380\n",
      "Iteration 21380 , the loss is currently 0.9919759035110474\n",
      "iteration:  21390\n",
      "Iteration 21390 , the loss is currently 0.9873887896537781\n",
      "iteration:  21400\n",
      "Iteration 21400 , the loss is currently 0.9848050475120544\n",
      "iteration:  21410\n",
      "Iteration 21410 , the loss is currently 0.9965999722480774\n",
      "iteration:  21420\n",
      "Iteration 21420 , the loss is currently 1.006332278251648\n",
      "iteration:  21430\n",
      "Iteration 21430 , the loss is currently 0.9991318583488464\n",
      "iteration:  21440\n",
      "Iteration 21440 , the loss is currently 1.0023682117462158\n",
      "iteration:  21450\n",
      "Iteration 21450 , the loss is currently 0.9935761094093323\n",
      "iteration:  21460\n",
      "Iteration 21460 , the loss is currently 0.9985173344612122\n",
      "iteration:  21470\n",
      "Iteration 21470 , the loss is currently 1.0034840106964111\n",
      "iteration:  21480\n",
      "Iteration 21480 , the loss is currently 0.9898310899734497\n",
      "iteration:  21490\n",
      "Iteration 21490 , the loss is currently 0.9745997190475464\n",
      "iteration:  21500\n",
      "Iteration 21500 , the loss is currently 0.9914276003837585\n",
      "iteration:  21510\n",
      "Iteration 21510 , the loss is currently 1.000300407409668\n",
      "iteration:  21520\n",
      "Iteration 21520 , the loss is currently 0.9958768486976624\n",
      "iteration:  21530\n",
      "Iteration 21530 , the loss is currently 0.9851768612861633\n",
      "iteration:  21540\n",
      "Iteration 21540 , the loss is currently 1.009010910987854\n",
      "iteration:  21550\n",
      "Iteration 21550 , the loss is currently 0.9881089925765991\n",
      "iteration:  21560\n",
      "Iteration 21560 , the loss is currently 0.9969902634620667\n",
      "iteration:  21570\n",
      "Iteration 21570 , the loss is currently 0.9917666912078857\n",
      "iteration:  21580\n",
      "Iteration 21580 , the loss is currently 0.9916535019874573\n",
      "iteration:  21590\n",
      "Iteration 21590 , the loss is currently 0.9757157564163208\n",
      "iteration:  21600\n",
      "Iteration 21600 , the loss is currently 0.9863204956054688\n",
      "iteration:  21610\n",
      "Iteration 21610 , the loss is currently 0.9693940877914429\n",
      "iteration:  21620\n",
      "Iteration 21620 , the loss is currently 1.0006635189056396\n",
      "iteration:  21630\n",
      "Iteration 21630 , the loss is currently 1.0062214136123657\n",
      "iteration:  21640\n",
      "Iteration 21640 , the loss is currently 1.000846266746521\n",
      "iteration:  21650\n",
      "Iteration 21650 , the loss is currently 0.9972008466720581\n",
      "iteration:  21660\n",
      "Iteration 21660 , the loss is currently 1.0013307332992554\n",
      "iteration:  21670\n",
      "Iteration 21670 , the loss is currently 0.9696073532104492\n",
      "iteration:  21680\n",
      "Iteration 21680 , the loss is currently 0.9853732585906982\n",
      "iteration:  21690\n",
      "Iteration 21690 , the loss is currently 0.9619885683059692\n",
      "iteration:  21700\n",
      "Iteration 21700 , the loss is currently 1.0003266334533691\n",
      "iteration:  21710\n",
      "Iteration 21710 , the loss is currently 0.9913148880004883\n",
      "iteration:  21720\n",
      "Iteration 21720 , the loss is currently 0.9870874881744385\n",
      "iteration:  21730\n",
      "Iteration 21730 , the loss is currently 0.9823762774467468\n",
      "iteration:  21740\n",
      "Iteration 21740 , the loss is currently 0.9698302745819092\n",
      "iteration:  21750\n",
      "Iteration 21750 , the loss is currently 0.9812750220298767\n",
      "iteration:  21760\n",
      "Iteration 21760 , the loss is currently 0.9932205080986023\n",
      "iteration:  21770\n",
      "Iteration 21770 , the loss is currently 0.9756994247436523\n",
      "iteration:  21780\n",
      "Iteration 21780 , the loss is currently 0.9931144714355469\n",
      "iteration:  21790\n",
      "Iteration 21790 , the loss is currently 1.0003153085708618\n",
      "iteration:  21800\n",
      "Iteration 21800 , the loss is currently 0.996601939201355\n",
      "iteration:  21810\n",
      "Iteration 21810 , the loss is currently 0.9723118543624878\n",
      "iteration:  21820\n",
      "Iteration 21820 , the loss is currently 0.9740726351737976\n",
      "iteration:  21830\n",
      "Iteration 21830 , the loss is currently 0.9728187322616577\n",
      "iteration:  21840\n",
      "Iteration 21840 , the loss is currently 0.9900466799736023\n",
      "iteration:  21850\n",
      "Iteration 21850 , the loss is currently 0.9989752769470215\n",
      "iteration:  21860\n",
      "Iteration 21860 , the loss is currently 0.9796730875968933\n",
      "iteration:  21870\n",
      "Iteration 21870 , the loss is currently 0.9951391220092773\n",
      "iteration:  21880\n",
      "Iteration 21880 , the loss is currently 0.9947130084037781\n",
      "iteration:  21890\n",
      "Iteration 21890 , the loss is currently 0.9774635434150696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  21900\n",
      "Iteration 21900 , the loss is currently 0.9812270402908325\n",
      "iteration:  21910\n",
      "Iteration 21910 , the loss is currently 0.9998965263366699\n",
      "iteration:  21920\n",
      "Iteration 21920 , the loss is currently 0.998857855796814\n",
      "iteration:  21930\n",
      "Iteration 21930 , the loss is currently 0.9837761521339417\n",
      "iteration:  21940\n",
      "Iteration 21940 , the loss is currently 1.0084295272827148\n",
      "iteration:  21950\n",
      "Iteration 21950 , the loss is currently 0.9769117832183838\n",
      "iteration:  21960\n",
      "Iteration 21960 , the loss is currently 0.9796057939529419\n",
      "iteration:  21970\n",
      "Iteration 21970 , the loss is currently 0.9872303605079651\n",
      "iteration:  21980\n",
      "Iteration 21980 , the loss is currently 0.9917846918106079\n",
      "iteration:  21990\n",
      "Iteration 21990 , the loss is currently 0.9971393942832947\n",
      "iteration:  22000\n",
      "Iteration 22000 , the loss is currently 1.0098016262054443\n",
      "iteration:  22010\n",
      "Iteration 22010 , the loss is currently 0.9882063865661621\n",
      "iteration:  22020\n",
      "Iteration 22020 , the loss is currently 0.9894012808799744\n",
      "iteration:  22030\n",
      "Iteration 22030 , the loss is currently 0.9907384514808655\n",
      "iteration:  22040\n",
      "Iteration 22040 , the loss is currently 0.9896193742752075\n",
      "iteration:  22050\n",
      "Iteration 22050 , the loss is currently 0.9767200350761414\n",
      "iteration:  22060\n",
      "Iteration 22060 , the loss is currently 0.9955318570137024\n",
      "iteration:  22070\n",
      "Iteration 22070 , the loss is currently 0.9950804710388184\n",
      "iteration:  22080\n",
      "Iteration 22080 , the loss is currently 0.9920698404312134\n",
      "iteration:  22090\n",
      "Iteration 22090 , the loss is currently 0.9850863814353943\n",
      "iteration:  22100\n",
      "Iteration 22100 , the loss is currently 0.9721091985702515\n",
      "iteration:  22110\n",
      "Iteration 22110 , the loss is currently 0.9750998616218567\n",
      "iteration:  22120\n",
      "Iteration 22120 , the loss is currently 0.9948166012763977\n",
      "iteration:  22130\n",
      "Iteration 22130 , the loss is currently 0.9894859194755554\n",
      "iteration:  22140\n",
      "Iteration 22140 , the loss is currently 0.9734647870063782\n",
      "iteration:  22150\n",
      "Iteration 22150 , the loss is currently 0.989820659160614\n",
      "iteration:  22160\n",
      "Iteration 22160 , the loss is currently 0.9597312808036804\n",
      "iteration:  22170\n",
      "Iteration 22170 , the loss is currently 1.0056025981903076\n",
      "iteration:  22180\n",
      "Iteration 22180 , the loss is currently 0.9762516617774963\n",
      "iteration:  22190\n",
      "Iteration 22190 , the loss is currently 0.9897355437278748\n",
      "iteration:  22200\n",
      "Iteration 22200 , the loss is currently 1.0057882070541382\n",
      "iteration:  22210\n",
      "Iteration 22210 , the loss is currently 1.003443956375122\n",
      "iteration:  22220\n",
      "Iteration 22220 , the loss is currently 1.0050452947616577\n",
      "iteration:  22230\n",
      "Iteration 22230 , the loss is currently 0.9812074303627014\n",
      "iteration:  22240\n",
      "Iteration 22240 , the loss is currently 0.9908587336540222\n",
      "iteration:  22250\n",
      "Iteration 22250 , the loss is currently 1.0168898105621338\n",
      "iteration:  22260\n",
      "Iteration 22260 , the loss is currently 0.9917897582054138\n",
      "iteration:  22270\n",
      "Iteration 22270 , the loss is currently 0.9765183925628662\n",
      "iteration:  22280\n",
      "Iteration 22280 , the loss is currently 0.9948474168777466\n",
      "iteration:  22290\n",
      "Iteration 22290 , the loss is currently 0.9745775461196899\n",
      "iteration:  22300\n",
      "Iteration 22300 , the loss is currently 1.0033725500106812\n",
      "iteration:  22310\n",
      "Iteration 22310 , the loss is currently 0.9815413951873779\n",
      "iteration:  22320\n",
      "Iteration 22320 , the loss is currently 0.9789502620697021\n",
      "iteration:  22330\n",
      "Iteration 22330 , the loss is currently 0.9854668378829956\n",
      "iteration:  22340\n",
      "Iteration 22340 , the loss is currently 0.9951068162918091\n",
      "iteration:  22350\n",
      "Iteration 22350 , the loss is currently 0.9743301272392273\n",
      "iteration:  22360\n",
      "Iteration 22360 , the loss is currently 0.9910988807678223\n",
      "iteration:  22370\n",
      "Iteration 22370 , the loss is currently 0.9923989772796631\n",
      "iteration:  22380\n",
      "Iteration 22380 , the loss is currently 0.9885496497154236\n",
      "iteration:  22390\n",
      "Iteration 22390 , the loss is currently 0.9825495481491089\n",
      "iteration:  22400\n",
      "Iteration 22400 , the loss is currently 0.9795737266540527\n",
      "iteration:  22410\n",
      "Iteration 22410 , the loss is currently 1.0005238056182861\n",
      "iteration:  22420\n",
      "Iteration 22420 , the loss is currently 0.9837083220481873\n",
      "iteration:  22430\n",
      "Iteration 22430 , the loss is currently 1.0130115747451782\n",
      "iteration:  22440\n",
      "Iteration 22440 , the loss is currently 0.9672525525093079\n",
      "iteration:  22450\n",
      "Iteration 22450 , the loss is currently 0.9943721294403076\n",
      "iteration:  22460\n",
      "Iteration 22460 , the loss is currently 0.9950142502784729\n",
      "iteration:  22470\n",
      "Iteration 22470 , the loss is currently 0.9931986927986145\n",
      "iteration:  22480\n",
      "Iteration 22480 , the loss is currently 0.9982024431228638\n",
      "iteration:  22490\n",
      "Iteration 22490 , the loss is currently 0.976782500743866\n",
      "iteration:  22500\n",
      "Iteration 22500 , the loss is currently 0.9910044074058533\n",
      "iteration:  22510\n",
      "Iteration 22510 , the loss is currently 1.0014759302139282\n",
      "iteration:  22520\n",
      "Iteration 22520 , the loss is currently 0.9859415292739868\n",
      "iteration:  22530\n",
      "Iteration 22530 , the loss is currently 1.0029300451278687\n",
      "iteration:  22540\n",
      "Iteration 22540 , the loss is currently 0.9725635051727295\n",
      "iteration:  22550\n",
      "Iteration 22550 , the loss is currently 1.008968472480774\n",
      "iteration:  22560\n",
      "Iteration 22560 , the loss is currently 1.0156258344650269\n",
      "iteration:  22570\n",
      "Iteration 22570 , the loss is currently 0.9701408743858337\n",
      "iteration:  22580\n",
      "Iteration 22580 , the loss is currently 1.0100582838058472\n",
      "iteration:  22590\n",
      "Iteration 22590 , the loss is currently 0.9861916303634644\n",
      "iteration:  22600\n",
      "Iteration 22600 , the loss is currently 1.0087124109268188\n",
      "iteration:  22610\n",
      "Iteration 22610 , the loss is currently 0.9943074584007263\n",
      "iteration:  22620\n",
      "Iteration 22620 , the loss is currently 0.9686051607131958\n",
      "iteration:  22630\n",
      "Iteration 22630 , the loss is currently 0.9817917346954346\n",
      "iteration:  22640\n",
      "Iteration 22640 , the loss is currently 0.9953392744064331\n",
      "iteration:  22650\n",
      "Iteration 22650 , the loss is currently 1.0021286010742188\n",
      "iteration:  22660\n",
      "Iteration 22660 , the loss is currently 0.986847996711731\n",
      "iteration:  22670\n",
      "Iteration 22670 , the loss is currently 0.9770845174789429\n",
      "iteration:  22680\n",
      "Iteration 22680 , the loss is currently 0.9673224687576294\n",
      "iteration:  22690\n",
      "Iteration 22690 , the loss is currently 1.0013036727905273\n",
      "iteration:  22700\n",
      "Iteration 22700 , the loss is currently 1.0054292678833008\n",
      "iteration:  22710\n",
      "Iteration 22710 , the loss is currently 0.9934548139572144\n",
      "iteration:  22720\n",
      "Iteration 22720 , the loss is currently 0.9923000335693359\n",
      "iteration:  22730\n",
      "Iteration 22730 , the loss is currently 0.9942967295646667\n",
      "iteration:  22740\n",
      "Iteration 22740 , the loss is currently 0.9898157119750977\n",
      "iteration:  22750\n",
      "Iteration 22750 , the loss is currently 0.9904575347900391\n",
      "iteration:  22760\n",
      "Iteration 22760 , the loss is currently 0.9910539388656616\n",
      "iteration:  22770\n",
      "Iteration 22770 , the loss is currently 0.9948912262916565\n",
      "iteration:  22780\n",
      "Iteration 22780 , the loss is currently 1.0034661293029785\n",
      "iteration:  22790\n",
      "Iteration 22790 , the loss is currently 0.9854910373687744\n",
      "iteration:  22800\n",
      "Iteration 22800 , the loss is currently 0.9835023880004883\n",
      "iteration:  22810\n",
      "Iteration 22810 , the loss is currently 0.993107795715332\n",
      "iteration:  22820\n",
      "Iteration 22820 , the loss is currently 1.0148218870162964\n",
      "iteration:  22830\n",
      "Iteration 22830 , the loss is currently 0.9921924471855164\n",
      "iteration:  22840\n",
      "Iteration 22840 , the loss is currently 0.9846080541610718\n",
      "iteration:  22850\n",
      "Iteration 22850 , the loss is currently 1.0183018445968628\n",
      "iteration:  22860\n",
      "Iteration 22860 , the loss is currently 1.0025455951690674\n",
      "iteration:  22870\n",
      "Iteration 22870 , the loss is currently 0.9691109657287598\n",
      "iteration:  22880\n",
      "Iteration 22880 , the loss is currently 0.9821669459342957\n",
      "iteration:  22890\n",
      "Iteration 22890 , the loss is currently 0.9758469462394714\n",
      "iteration:  22900\n",
      "Iteration 22900 , the loss is currently 1.0030643939971924\n",
      "iteration:  22910\n",
      "Iteration 22910 , the loss is currently 0.9845836162567139\n",
      "iteration:  22920\n",
      "Iteration 22920 , the loss is currently 0.9773946404457092\n",
      "iteration:  22930\n",
      "Iteration 22930 , the loss is currently 0.97022545337677\n",
      "iteration:  22940\n",
      "Iteration 22940 , the loss is currently 0.9866325259208679\n",
      "iteration:  22950\n",
      "Iteration 22950 , the loss is currently 1.001229166984558\n",
      "iteration:  22960\n",
      "Iteration 22960 , the loss is currently 0.9953203201293945\n",
      "iteration:  22970\n",
      "Iteration 22970 , the loss is currently 1.023174524307251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  22980\n",
      "Iteration 22980 , the loss is currently 1.0136096477508545\n",
      "iteration:  22990\n",
      "Iteration 22990 , the loss is currently 1.00276780128479\n",
      "iteration:  23000\n",
      "Iteration 23000 , the loss is currently 0.9824845194816589\n",
      "iteration:  23010\n",
      "Iteration 23010 , the loss is currently 0.9836524128913879\n",
      "iteration:  23020\n",
      "Iteration 23020 , the loss is currently 0.9988936185836792\n",
      "iteration:  23030\n",
      "Iteration 23030 , the loss is currently 0.9970113635063171\n",
      "iteration:  23040\n",
      "Iteration 23040 , the loss is currently 0.9791132807731628\n",
      "iteration:  23050\n",
      "Iteration 23050 , the loss is currently 0.9930678606033325\n",
      "iteration:  23060\n",
      "Iteration 23060 , the loss is currently 1.0050262212753296\n",
      "iteration:  23070\n",
      "Iteration 23070 , the loss is currently 0.992952823638916\n",
      "iteration:  23080\n",
      "Iteration 23080 , the loss is currently 0.9903762936592102\n",
      "iteration:  23090\n",
      "Iteration 23090 , the loss is currently 1.0131757259368896\n",
      "iteration:  23100\n",
      "Iteration 23100 , the loss is currently 0.9815166592597961\n",
      "iteration:  23110\n",
      "Iteration 23110 , the loss is currently 1.0134001970291138\n",
      "iteration:  23120\n",
      "Iteration 23120 , the loss is currently 0.9785562753677368\n",
      "iteration:  23130\n",
      "Iteration 23130 , the loss is currently 0.9987887144088745\n",
      "iteration:  23140\n",
      "Iteration 23140 , the loss is currently 0.9885128736495972\n",
      "iteration:  23150\n",
      "Iteration 23150 , the loss is currently 0.9796446561813354\n",
      "iteration:  23160\n",
      "Iteration 23160 , the loss is currently 1.0062191486358643\n",
      "iteration:  23170\n",
      "Iteration 23170 , the loss is currently 0.9752134680747986\n",
      "iteration:  23180\n",
      "Iteration 23180 , the loss is currently 0.9824808239936829\n",
      "iteration:  23190\n",
      "Iteration 23190 , the loss is currently 0.972847580909729\n",
      "iteration:  23200\n",
      "Iteration 23200 , the loss is currently 0.9922798871994019\n",
      "iteration:  23210\n",
      "Iteration 23210 , the loss is currently 0.9969943165779114\n",
      "iteration:  23220\n",
      "Iteration 23220 , the loss is currently 1.0221974849700928\n",
      "iteration:  23230\n",
      "Iteration 23230 , the loss is currently 1.0250853300094604\n",
      "iteration:  23240\n",
      "Iteration 23240 , the loss is currently 0.9972553849220276\n",
      "iteration:  23250\n",
      "Iteration 23250 , the loss is currently 0.9948404431343079\n",
      "iteration:  23260\n",
      "Iteration 23260 , the loss is currently 0.9951497316360474\n",
      "iteration:  23270\n",
      "Iteration 23270 , the loss is currently 0.994710385799408\n",
      "iteration:  23280\n",
      "Iteration 23280 , the loss is currently 0.9876286387443542\n",
      "iteration:  23290\n",
      "Iteration 23290 , the loss is currently 0.9833503365516663\n",
      "iteration:  23300\n",
      "Iteration 23300 , the loss is currently 0.9867462515830994\n",
      "iteration:  23310\n",
      "Iteration 23310 , the loss is currently 0.9981120228767395\n",
      "iteration:  23320\n",
      "Iteration 23320 , the loss is currently 0.9872125387191772\n",
      "iteration:  23330\n",
      "Iteration 23330 , the loss is currently 1.005486249923706\n",
      "iteration:  23340\n",
      "Iteration 23340 , the loss is currently 0.9738844037055969\n",
      "iteration:  23350\n",
      "Iteration 23350 , the loss is currently 0.9923827052116394\n",
      "iteration:  23360\n",
      "Iteration 23360 , the loss is currently 1.0031741857528687\n",
      "iteration:  23370\n",
      "Iteration 23370 , the loss is currently 1.005236029624939\n",
      "iteration:  23380\n",
      "Iteration 23380 , the loss is currently 0.9901531934738159\n",
      "iteration:  23390\n",
      "Iteration 23390 , the loss is currently 0.9798301458358765\n",
      "iteration:  23400\n",
      "Iteration 23400 , the loss is currently 1.0072788000106812\n",
      "iteration:  23410\n",
      "Iteration 23410 , the loss is currently 0.9994782209396362\n",
      "iteration:  23420\n",
      "Iteration 23420 , the loss is currently 0.9959797859191895\n",
      "iteration:  23430\n",
      "Iteration 23430 , the loss is currently 0.9878031611442566\n",
      "iteration:  23440\n",
      "Iteration 23440 , the loss is currently 0.9839024543762207\n",
      "iteration:  23450\n",
      "Iteration 23450 , the loss is currently 1.01959228515625\n",
      "iteration:  23460\n",
      "Iteration 23460 , the loss is currently 0.9875102639198303\n",
      "iteration:  23470\n",
      "Iteration 23470 , the loss is currently 0.9872836470603943\n",
      "iteration:  23480\n",
      "Iteration 23480 , the loss is currently 1.0041389465332031\n",
      "iteration:  23490\n",
      "Iteration 23490 , the loss is currently 0.9909377098083496\n",
      "iteration:  23500\n",
      "Iteration 23500 , the loss is currently 0.9824947118759155\n",
      "iteration:  23510\n",
      "Iteration 23510 , the loss is currently 0.980190634727478\n",
      "iteration:  23520\n",
      "Iteration 23520 , the loss is currently 0.9907673597335815\n",
      "iteration:  23530\n",
      "Iteration 23530 , the loss is currently 0.9565411806106567\n",
      "iteration:  23540\n",
      "Iteration 23540 , the loss is currently 0.9710818529129028\n",
      "iteration:  23550\n",
      "Iteration 23550 , the loss is currently 0.987755537033081\n",
      "iteration:  23560\n",
      "Iteration 23560 , the loss is currently 1.0017989873886108\n",
      "iteration:  23570\n",
      "Iteration 23570 , the loss is currently 0.9869961738586426\n",
      "iteration:  23580\n",
      "Iteration 23580 , the loss is currently 1.0066988468170166\n",
      "iteration:  23590\n",
      "Iteration 23590 , the loss is currently 0.9864461421966553\n",
      "iteration:  23600\n",
      "Iteration 23600 , the loss is currently 1.0125712156295776\n",
      "iteration:  23610\n",
      "Iteration 23610 , the loss is currently 0.9765246510505676\n",
      "iteration:  23620\n",
      "Iteration 23620 , the loss is currently 0.9794785976409912\n",
      "iteration:  23630\n",
      "Iteration 23630 , the loss is currently 0.9873522520065308\n",
      "iteration:  23640\n",
      "Iteration 23640 , the loss is currently 0.9942489266395569\n",
      "iteration:  23650\n",
      "Iteration 23650 , the loss is currently 0.9866479635238647\n",
      "iteration:  23660\n",
      "Iteration 23660 , the loss is currently 0.9971053600311279\n",
      "iteration:  23670\n",
      "Iteration 23670 , the loss is currently 0.9925680756568909\n",
      "iteration:  23680\n",
      "Iteration 23680 , the loss is currently 0.9848055243492126\n",
      "iteration:  23690\n",
      "Iteration 23690 , the loss is currently 0.9925128221511841\n",
      "iteration:  23700\n",
      "Iteration 23700 , the loss is currently 0.9953835010528564\n",
      "iteration:  23710\n",
      "Iteration 23710 , the loss is currently 0.9799898266792297\n",
      "iteration:  23720\n",
      "Iteration 23720 , the loss is currently 0.9981804490089417\n",
      "iteration:  23730\n",
      "Iteration 23730 , the loss is currently 1.0120724439620972\n",
      "iteration:  23740\n",
      "Iteration 23740 , the loss is currently 0.9961597323417664\n",
      "iteration:  23750\n",
      "Iteration 23750 , the loss is currently 1.004990816116333\n",
      "iteration:  23760\n",
      "Iteration 23760 , the loss is currently 0.9746890664100647\n",
      "iteration:  23770\n",
      "Iteration 23770 , the loss is currently 0.9814625382423401\n",
      "iteration:  23780\n",
      "Iteration 23780 , the loss is currently 0.9986632466316223\n",
      "iteration:  23790\n",
      "Iteration 23790 , the loss is currently 0.9958851337432861\n",
      "iteration:  23800\n",
      "Iteration 23800 , the loss is currently 0.9857587814331055\n",
      "iteration:  23810\n",
      "Iteration 23810 , the loss is currently 0.9843147397041321\n",
      "iteration:  23820\n",
      "Iteration 23820 , the loss is currently 0.9851738810539246\n",
      "iteration:  23830\n",
      "Iteration 23830 , the loss is currently 0.977790117263794\n",
      "iteration:  23840\n",
      "Iteration 23840 , the loss is currently 0.9731258153915405\n",
      "iteration:  23850\n",
      "Iteration 23850 , the loss is currently 0.9675354361534119\n",
      "iteration:  23860\n",
      "Iteration 23860 , the loss is currently 1.001792550086975\n",
      "iteration:  23870\n",
      "Iteration 23870 , the loss is currently 0.9758408665657043\n",
      "iteration:  23880\n",
      "Iteration 23880 , the loss is currently 0.9827240109443665\n",
      "iteration:  23890\n",
      "Iteration 23890 , the loss is currently 1.0008755922317505\n",
      "iteration:  23900\n",
      "Iteration 23900 , the loss is currently 0.9892089366912842\n",
      "iteration:  23910\n",
      "Iteration 23910 , the loss is currently 0.9889179468154907\n",
      "iteration:  23920\n",
      "Iteration 23920 , the loss is currently 0.9997296333312988\n",
      "iteration:  23930\n",
      "Iteration 23930 , the loss is currently 0.9873905777931213\n",
      "iteration:  23940\n",
      "Iteration 23940 , the loss is currently 0.992091715335846\n",
      "iteration:  23950\n",
      "Iteration 23950 , the loss is currently 0.9701588749885559\n",
      "iteration:  23960\n",
      "Iteration 23960 , the loss is currently 0.9860251545906067\n",
      "iteration:  23970\n",
      "Iteration 23970 , the loss is currently 0.9882271885871887\n",
      "iteration:  23980\n",
      "Iteration 23980 , the loss is currently 0.9815241098403931\n",
      "iteration:  23990\n",
      "Iteration 23990 , the loss is currently 1.01313316822052\n",
      "iteration:  24000\n",
      "Iteration 24000 , the loss is currently 0.9685181975364685\n",
      "iteration:  24010\n",
      "Iteration 24010 , the loss is currently 0.9910114407539368\n",
      "iteration:  24020\n",
      "Iteration 24020 , the loss is currently 1.006849765777588\n",
      "iteration:  24030\n",
      "Iteration 24030 , the loss is currently 1.0020768642425537\n",
      "iteration:  24040\n",
      "Iteration 24040 , the loss is currently 0.9843456745147705\n",
      "iteration:  24050\n",
      "Iteration 24050 , the loss is currently 0.9879142045974731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  24060\n",
      "Iteration 24060 , the loss is currently 0.9902696013450623\n",
      "iteration:  24070\n",
      "Iteration 24070 , the loss is currently 0.9778307676315308\n",
      "iteration:  24080\n",
      "Iteration 24080 , the loss is currently 0.9932267665863037\n",
      "iteration:  24090\n",
      "Iteration 24090 , the loss is currently 1.0046112537384033\n",
      "iteration:  24100\n",
      "Iteration 24100 , the loss is currently 0.9765614867210388\n",
      "iteration:  24110\n",
      "Iteration 24110 , the loss is currently 1.0101029872894287\n",
      "iteration:  24120\n",
      "Iteration 24120 , the loss is currently 0.9828639626502991\n",
      "iteration:  24130\n",
      "Iteration 24130 , the loss is currently 0.9923657774925232\n",
      "iteration:  24140\n",
      "Iteration 24140 , the loss is currently 0.985520601272583\n",
      "iteration:  24150\n",
      "Iteration 24150 , the loss is currently 0.9741178154945374\n",
      "iteration:  24160\n",
      "Iteration 24160 , the loss is currently 0.983437716960907\n",
      "iteration:  24170\n",
      "Iteration 24170 , the loss is currently 0.9864318370819092\n",
      "iteration:  24180\n",
      "Iteration 24180 , the loss is currently 0.9896134734153748\n",
      "iteration:  24190\n",
      "Iteration 24190 , the loss is currently 0.9770184755325317\n",
      "iteration:  24200\n",
      "Iteration 24200 , the loss is currently 0.9948315024375916\n",
      "iteration:  24210\n",
      "Iteration 24210 , the loss is currently 0.9894649386405945\n",
      "iteration:  24220\n",
      "Iteration 24220 , the loss is currently 0.9749614596366882\n",
      "iteration:  24230\n",
      "Iteration 24230 , the loss is currently 0.993175745010376\n",
      "iteration:  24240\n",
      "Iteration 24240 , the loss is currently 0.9915528297424316\n",
      "iteration:  24250\n",
      "Iteration 24250 , the loss is currently 0.9799106121063232\n",
      "iteration:  24260\n",
      "Iteration 24260 , the loss is currently 0.9811985492706299\n",
      "iteration:  24270\n",
      "Iteration 24270 , the loss is currently 0.9870437383651733\n",
      "iteration:  24280\n",
      "Iteration 24280 , the loss is currently 0.997226893901825\n",
      "iteration:  24290\n",
      "Iteration 24290 , the loss is currently 0.9848842620849609\n",
      "iteration:  24300\n",
      "Iteration 24300 , the loss is currently 0.9775198101997375\n",
      "iteration:  24310\n",
      "Iteration 24310 , the loss is currently 0.9962518215179443\n",
      "iteration:  24320\n",
      "Iteration 24320 , the loss is currently 0.9936753511428833\n",
      "iteration:  24330\n",
      "Iteration 24330 , the loss is currently 0.9836901426315308\n",
      "iteration:  24340\n",
      "Iteration 24340 , the loss is currently 0.9732285141944885\n",
      "iteration:  24350\n",
      "Iteration 24350 , the loss is currently 0.9948024749755859\n",
      "iteration:  24360\n",
      "Iteration 24360 , the loss is currently 0.9903297424316406\n",
      "iteration:  24370\n",
      "Iteration 24370 , the loss is currently 0.9716601967811584\n",
      "iteration:  24380\n",
      "Iteration 24380 , the loss is currently 0.9827806353569031\n",
      "iteration:  24390\n",
      "Iteration 24390 , the loss is currently 0.9795008301734924\n",
      "iteration:  24400\n",
      "Iteration 24400 , the loss is currently 0.982980489730835\n",
      "iteration:  24410\n",
      "Iteration 24410 , the loss is currently 1.005316138267517\n",
      "iteration:  24420\n",
      "Iteration 24420 , the loss is currently 1.0028395652770996\n",
      "iteration:  24430\n",
      "Iteration 24430 , the loss is currently 1.0122849941253662\n",
      "iteration:  24440\n",
      "Iteration 24440 , the loss is currently 0.9924057126045227\n",
      "iteration:  24450\n",
      "Iteration 24450 , the loss is currently 0.9825416207313538\n",
      "iteration:  24460\n",
      "Iteration 24460 , the loss is currently 0.9927278161048889\n",
      "iteration:  24470\n",
      "Iteration 24470 , the loss is currently 0.9990458488464355\n",
      "iteration:  24480\n",
      "Iteration 24480 , the loss is currently 0.9949001669883728\n",
      "iteration:  24490\n",
      "Iteration 24490 , the loss is currently 0.9791672825813293\n",
      "iteration:  24500\n",
      "Iteration 24500 , the loss is currently 0.985762894153595\n",
      "iteration:  24510\n",
      "Iteration 24510 , the loss is currently 1.0027810335159302\n",
      "iteration:  24520\n",
      "Iteration 24520 , the loss is currently 0.9802406430244446\n",
      "iteration:  24530\n",
      "Iteration 24530 , the loss is currently 1.0011122226715088\n",
      "iteration:  24540\n",
      "Iteration 24540 , the loss is currently 1.0006695985794067\n",
      "iteration:  24550\n",
      "Iteration 24550 , the loss is currently 0.9989938735961914\n",
      "iteration:  24560\n",
      "Iteration 24560 , the loss is currently 0.9687663316726685\n",
      "iteration:  24570\n",
      "Iteration 24570 , the loss is currently 0.995227038860321\n",
      "iteration:  24580\n",
      "Iteration 24580 , the loss is currently 1.000150203704834\n",
      "iteration:  24590\n",
      "Iteration 24590 , the loss is currently 0.9975343942642212\n",
      "iteration:  24600\n",
      "Iteration 24600 , the loss is currently 0.9721876978874207\n",
      "iteration:  24610\n",
      "Iteration 24610 , the loss is currently 0.9988871216773987\n",
      "iteration:  24620\n",
      "Iteration 24620 , the loss is currently 0.964270830154419\n",
      "iteration:  24630\n",
      "Iteration 24630 , the loss is currently 1.002963662147522\n",
      "iteration:  24640\n",
      "Iteration 24640 , the loss is currently 0.9890730977058411\n",
      "iteration:  24650\n",
      "Iteration 24650 , the loss is currently 0.9879409074783325\n",
      "iteration:  24660\n",
      "Iteration 24660 , the loss is currently 0.998347818851471\n",
      "iteration:  24670\n",
      "Iteration 24670 , the loss is currently 0.9879973530769348\n",
      "iteration:  24680\n",
      "Iteration 24680 , the loss is currently 0.9943294525146484\n",
      "iteration:  24690\n",
      "Iteration 24690 , the loss is currently 1.0028448104858398\n",
      "iteration:  24700\n",
      "Iteration 24700 , the loss is currently 0.9924365878105164\n",
      "iteration:  24710\n",
      "Iteration 24710 , the loss is currently 0.9799447655677795\n",
      "iteration:  24720\n",
      "Iteration 24720 , the loss is currently 0.979236900806427\n",
      "iteration:  24730\n",
      "Iteration 24730 , the loss is currently 1.0149364471435547\n",
      "iteration:  24740\n",
      "Iteration 24740 , the loss is currently 0.9966232180595398\n",
      "iteration:  24750\n",
      "Iteration 24750 , the loss is currently 0.9808481335639954\n",
      "iteration:  24760\n",
      "Iteration 24760 , the loss is currently 0.9826642870903015\n",
      "iteration:  24770\n",
      "Iteration 24770 , the loss is currently 0.9692038893699646\n",
      "iteration:  24780\n",
      "Iteration 24780 , the loss is currently 0.9966657161712646\n",
      "iteration:  24790\n",
      "Iteration 24790 , the loss is currently 0.9773277044296265\n",
      "iteration:  24800\n",
      "Iteration 24800 , the loss is currently 1.002296805381775\n",
      "iteration:  24810\n",
      "Iteration 24810 , the loss is currently 0.990803599357605\n",
      "iteration:  24820\n",
      "Iteration 24820 , the loss is currently 0.9862807989120483\n",
      "iteration:  24830\n",
      "Iteration 24830 , the loss is currently 0.9967347979545593\n",
      "iteration:  24840\n",
      "Iteration 24840 , the loss is currently 0.9947284460067749\n",
      "iteration:  24850\n",
      "Iteration 24850 , the loss is currently 0.9709317684173584\n",
      "iteration:  24860\n",
      "Iteration 24860 , the loss is currently 0.9979148507118225\n",
      "iteration:  24870\n",
      "Iteration 24870 , the loss is currently 0.9870532155036926\n",
      "iteration:  24880\n",
      "Iteration 24880 , the loss is currently 0.9955323934555054\n",
      "iteration:  24890\n",
      "Iteration 24890 , the loss is currently 0.9900064468383789\n",
      "iteration:  24900\n",
      "Iteration 24900 , the loss is currently 0.991719663143158\n",
      "iteration:  24910\n",
      "Iteration 24910 , the loss is currently 0.9579556584358215\n",
      "iteration:  24920\n",
      "Iteration 24920 , the loss is currently 0.988074541091919\n",
      "iteration:  24930\n",
      "Iteration 24930 , the loss is currently 0.9874960780143738\n",
      "iteration:  24940\n",
      "Iteration 24940 , the loss is currently 0.9926896095275879\n",
      "iteration:  24950\n",
      "Iteration 24950 , the loss is currently 0.9999182820320129\n",
      "iteration:  24960\n",
      "Iteration 24960 , the loss is currently 0.9844857454299927\n",
      "iteration:  24970\n",
      "Iteration 24970 , the loss is currently 1.0141669511795044\n",
      "iteration:  24980\n",
      "Iteration 24980 , the loss is currently 0.9855398535728455\n",
      "iteration:  24990\n",
      "Iteration 24990 , the loss is currently 1.0044703483581543\n",
      "iteration:  25000\n",
      "Iteration 25000 , the loss is currently 0.9931730031967163\n",
      "iteration:  25010\n",
      "Iteration 25010 , the loss is currently 0.9871248006820679\n",
      "iteration:  25020\n",
      "Iteration 25020 , the loss is currently 1.0100959539413452\n",
      "iteration:  25030\n",
      "Iteration 25030 , the loss is currently 0.9900103211402893\n",
      "iteration:  25040\n",
      "Iteration 25040 , the loss is currently 0.9930654764175415\n",
      "iteration:  25050\n",
      "Iteration 25050 , the loss is currently 0.9828316569328308\n",
      "iteration:  25060\n",
      "Iteration 25060 , the loss is currently 0.9923049807548523\n",
      "iteration:  25070\n",
      "Iteration 25070 , the loss is currently 0.9815525412559509\n",
      "iteration:  25080\n",
      "Iteration 25080 , the loss is currently 1.0184226036071777\n",
      "iteration:  25090\n",
      "Iteration 25090 , the loss is currently 0.9781453013420105\n",
      "iteration:  25100\n",
      "Iteration 25100 , the loss is currently 0.9912068247795105\n",
      "iteration:  25110\n",
      "Iteration 25110 , the loss is currently 0.9881154298782349\n",
      "iteration:  25120\n",
      "Iteration 25120 , the loss is currently 0.9897070527076721\n",
      "iteration:  25130\n",
      "Iteration 25130 , the loss is currently 0.987457811832428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  25140\n",
      "Iteration 25140 , the loss is currently 0.9725174307823181\n",
      "iteration:  25150\n",
      "Iteration 25150 , the loss is currently 1.009662389755249\n",
      "iteration:  25160\n",
      "Iteration 25160 , the loss is currently 1.0090049505233765\n",
      "iteration:  25170\n",
      "Iteration 25170 , the loss is currently 0.9961330890655518\n",
      "iteration:  25180\n",
      "Iteration 25180 , the loss is currently 0.9922797679901123\n",
      "iteration:  25190\n",
      "Iteration 25190 , the loss is currently 1.0038747787475586\n",
      "iteration:  25200\n",
      "Iteration 25200 , the loss is currently 0.9941167235374451\n",
      "iteration:  25210\n",
      "Iteration 25210 , the loss is currently 0.9965454936027527\n",
      "iteration:  25220\n",
      "Iteration 25220 , the loss is currently 1.005096435546875\n",
      "iteration:  25230\n",
      "Iteration 25230 , the loss is currently 0.9851616621017456\n",
      "iteration:  25240\n",
      "Iteration 25240 , the loss is currently 0.986116886138916\n",
      "iteration:  25250\n",
      "Iteration 25250 , the loss is currently 0.9888976812362671\n",
      "iteration:  25260\n",
      "Iteration 25260 , the loss is currently 0.9836450815200806\n",
      "iteration:  25270\n",
      "Iteration 25270 , the loss is currently 0.9950488805770874\n",
      "iteration:  25280\n",
      "Iteration 25280 , the loss is currently 1.0095330476760864\n",
      "iteration:  25290\n",
      "Iteration 25290 , the loss is currently 0.9869539737701416\n",
      "iteration:  25300\n",
      "Iteration 25300 , the loss is currently 0.9933741092681885\n",
      "iteration:  25310\n",
      "Iteration 25310 , the loss is currently 0.9930078387260437\n",
      "iteration:  25320\n",
      "Iteration 25320 , the loss is currently 0.9662036895751953\n",
      "iteration:  25330\n",
      "Iteration 25330 , the loss is currently 1.0257452726364136\n",
      "iteration:  25340\n",
      "Iteration 25340 , the loss is currently 0.9870607852935791\n",
      "iteration:  25350\n",
      "Iteration 25350 , the loss is currently 1.0231211185455322\n",
      "iteration:  25360\n",
      "Iteration 25360 , the loss is currently 0.9928032159805298\n",
      "iteration:  25370\n",
      "Iteration 25370 , the loss is currently 1.002217173576355\n",
      "iteration:  25380\n",
      "Iteration 25380 , the loss is currently 0.993757426738739\n",
      "iteration:  25390\n",
      "Iteration 25390 , the loss is currently 0.9848474264144897\n",
      "iteration:  25400\n",
      "Iteration 25400 , the loss is currently 0.994278073310852\n",
      "iteration:  25410\n",
      "Iteration 25410 , the loss is currently 0.9900091886520386\n",
      "iteration:  25420\n",
      "Iteration 25420 , the loss is currently 0.9937469363212585\n",
      "iteration:  25430\n",
      "Iteration 25430 , the loss is currently 1.0090124607086182\n",
      "iteration:  25440\n",
      "Iteration 25440 , the loss is currently 1.005995512008667\n",
      "iteration:  25450\n",
      "Iteration 25450 , the loss is currently 0.9777293801307678\n",
      "iteration:  25460\n",
      "Iteration 25460 , the loss is currently 1.0002329349517822\n",
      "iteration:  25470\n",
      "Iteration 25470 , the loss is currently 0.9823198914527893\n",
      "iteration:  25480\n",
      "Iteration 25480 , the loss is currently 0.9611231684684753\n",
      "iteration:  25490\n",
      "Iteration 25490 , the loss is currently 0.9987753033638\n",
      "iteration:  25500\n",
      "Iteration 25500 , the loss is currently 0.9893021583557129\n",
      "iteration:  25510\n",
      "Iteration 25510 , the loss is currently 0.9964142441749573\n",
      "iteration:  25520\n",
      "Iteration 25520 , the loss is currently 0.9931899905204773\n",
      "iteration:  25530\n",
      "Iteration 25530 , the loss is currently 0.9933307766914368\n",
      "iteration:  25540\n",
      "Iteration 25540 , the loss is currently 0.9843417406082153\n",
      "iteration:  25550\n",
      "Iteration 25550 , the loss is currently 1.0177937746047974\n",
      "iteration:  25560\n",
      "Iteration 25560 , the loss is currently 0.9906864166259766\n",
      "iteration:  25570\n",
      "Iteration 25570 , the loss is currently 0.9793890118598938\n",
      "iteration:  25580\n",
      "Iteration 25580 , the loss is currently 1.0139001607894897\n",
      "iteration:  25590\n",
      "Iteration 25590 , the loss is currently 0.9864564538002014\n",
      "iteration:  25600\n",
      "Iteration 25600 , the loss is currently 0.9864224195480347\n",
      "iteration:  25610\n",
      "Iteration 25610 , the loss is currently 0.9800878167152405\n",
      "iteration:  25620\n",
      "Iteration 25620 , the loss is currently 0.9903518557548523\n",
      "iteration:  25630\n",
      "Iteration 25630 , the loss is currently 0.9718764424324036\n",
      "iteration:  25640\n",
      "Iteration 25640 , the loss is currently 0.9869492650032043\n",
      "iteration:  25650\n",
      "Iteration 25650 , the loss is currently 0.9928892254829407\n",
      "iteration:  25660\n",
      "Iteration 25660 , the loss is currently 1.0081337690353394\n",
      "iteration:  25670\n",
      "Iteration 25670 , the loss is currently 0.9828309416770935\n",
      "iteration:  25680\n",
      "Iteration 25680 , the loss is currently 0.9974891543388367\n",
      "iteration:  25690\n",
      "Iteration 25690 , the loss is currently 0.991493821144104\n",
      "iteration:  25700\n",
      "Iteration 25700 , the loss is currently 0.976701021194458\n",
      "iteration:  25710\n",
      "Iteration 25710 , the loss is currently 0.977750837802887\n",
      "iteration:  25720\n",
      "Iteration 25720 , the loss is currently 1.0055913925170898\n",
      "iteration:  25730\n",
      "Iteration 25730 , the loss is currently 0.9879968166351318\n",
      "iteration:  25740\n",
      "Iteration 25740 , the loss is currently 1.0031511783599854\n",
      "iteration:  25750\n",
      "Iteration 25750 , the loss is currently 1.0224170684814453\n",
      "iteration:  25760\n",
      "Iteration 25760 , the loss is currently 1.0010889768600464\n",
      "iteration:  25770\n",
      "Iteration 25770 , the loss is currently 0.9719291925430298\n",
      "iteration:  25780\n",
      "Iteration 25780 , the loss is currently 0.9788908362388611\n",
      "iteration:  25790\n",
      "Iteration 25790 , the loss is currently 1.001823902130127\n",
      "iteration:  25800\n",
      "Iteration 25800 , the loss is currently 0.9909267425537109\n",
      "iteration:  25810\n",
      "Iteration 25810 , the loss is currently 0.9881839752197266\n",
      "iteration:  25820\n",
      "Iteration 25820 , the loss is currently 0.9793717265129089\n",
      "iteration:  25830\n",
      "Iteration 25830 , the loss is currently 1.0046637058258057\n",
      "iteration:  25840\n",
      "Iteration 25840 , the loss is currently 1.0005502700805664\n",
      "iteration:  25850\n",
      "Iteration 25850 , the loss is currently 0.9912116527557373\n",
      "iteration:  25860\n",
      "Iteration 25860 , the loss is currently 0.9908980131149292\n",
      "iteration:  25870\n",
      "Iteration 25870 , the loss is currently 0.981218695640564\n",
      "iteration:  25880\n",
      "Iteration 25880 , the loss is currently 0.9951278567314148\n",
      "iteration:  25890\n",
      "Iteration 25890 , the loss is currently 0.9923096299171448\n",
      "iteration:  25900\n",
      "Iteration 25900 , the loss is currently 0.9856199026107788\n",
      "iteration:  25910\n",
      "Iteration 25910 , the loss is currently 0.9935497045516968\n",
      "iteration:  25920\n",
      "Iteration 25920 , the loss is currently 0.9824357032775879\n",
      "iteration:  25930\n",
      "Iteration 25930 , the loss is currently 0.9865379333496094\n",
      "iteration:  25940\n",
      "Iteration 25940 , the loss is currently 0.9710385203361511\n",
      "iteration:  25950\n",
      "Iteration 25950 , the loss is currently 0.9959179162979126\n",
      "iteration:  25960\n",
      "Iteration 25960 , the loss is currently 1.0120692253112793\n",
      "iteration:  25970\n",
      "Iteration 25970 , the loss is currently 0.994668185710907\n",
      "iteration:  25980\n",
      "Iteration 25980 , the loss is currently 1.013020396232605\n",
      "iteration:  25990\n",
      "Iteration 25990 , the loss is currently 0.9903245568275452\n",
      "iteration:  26000\n",
      "Iteration 26000 , the loss is currently 0.9818298816680908\n",
      "iteration:  26010\n",
      "Iteration 26010 , the loss is currently 0.984343945980072\n",
      "iteration:  26020\n",
      "Iteration 26020 , the loss is currently 0.9964914917945862\n",
      "iteration:  26030\n",
      "Iteration 26030 , the loss is currently 0.9730050563812256\n",
      "iteration:  26040\n",
      "Iteration 26040 , the loss is currently 0.9856011867523193\n",
      "iteration:  26050\n",
      "Iteration 26050 , the loss is currently 0.9870758652687073\n",
      "iteration:  26060\n",
      "Iteration 26060 , the loss is currently 0.9897441267967224\n",
      "iteration:  26070\n",
      "Iteration 26070 , the loss is currently 0.974739134311676\n",
      "iteration:  26080\n",
      "Iteration 26080 , the loss is currently 0.9993228316307068\n",
      "iteration:  26090\n",
      "Iteration 26090 , the loss is currently 1.0134024620056152\n",
      "iteration:  26100\n",
      "Iteration 26100 , the loss is currently 0.9910314083099365\n",
      "iteration:  26110\n",
      "Iteration 26110 , the loss is currently 0.9966198801994324\n",
      "iteration:  26120\n",
      "Iteration 26120 , the loss is currently 0.9972723126411438\n",
      "iteration:  26130\n",
      "Iteration 26130 , the loss is currently 0.994830846786499\n",
      "iteration:  26140\n",
      "Iteration 26140 , the loss is currently 0.9810742139816284\n",
      "iteration:  26150\n",
      "Iteration 26150 , the loss is currently 0.9889675378799438\n",
      "iteration:  26160\n",
      "Iteration 26160 , the loss is currently 0.9877660274505615\n",
      "iteration:  26170\n",
      "Iteration 26170 , the loss is currently 0.9923405647277832\n",
      "iteration:  26180\n",
      "Iteration 26180 , the loss is currently 0.9912766814231873\n",
      "iteration:  26190\n",
      "Iteration 26190 , the loss is currently 1.009798526763916\n",
      "iteration:  26200\n",
      "Iteration 26200 , the loss is currently 0.9877241253852844\n",
      "iteration:  26210\n",
      "Iteration 26210 , the loss is currently 1.0037150382995605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  26220\n",
      "Iteration 26220 , the loss is currently 0.9644482135772705\n",
      "iteration:  26230\n",
      "Iteration 26230 , the loss is currently 0.978066086769104\n",
      "iteration:  26240\n",
      "Iteration 26240 , the loss is currently 1.0037972927093506\n",
      "iteration:  26250\n",
      "Iteration 26250 , the loss is currently 0.98975670337677\n",
      "iteration:  26260\n",
      "Iteration 26260 , the loss is currently 1.0033060312271118\n",
      "iteration:  26270\n",
      "Iteration 26270 , the loss is currently 0.9817700982093811\n",
      "iteration:  26280\n",
      "Iteration 26280 , the loss is currently 0.9944252967834473\n",
      "iteration:  26290\n",
      "Iteration 26290 , the loss is currently 0.9970530271530151\n",
      "iteration:  26300\n",
      "Iteration 26300 , the loss is currently 0.9894599914550781\n",
      "iteration:  26310\n",
      "Iteration 26310 , the loss is currently 0.9933525919914246\n",
      "iteration:  26320\n",
      "Iteration 26320 , the loss is currently 0.9817519187927246\n",
      "iteration:  26330\n",
      "Iteration 26330 , the loss is currently 0.9889774322509766\n",
      "iteration:  26340\n",
      "Iteration 26340 , the loss is currently 0.987904965877533\n",
      "iteration:  26350\n",
      "Iteration 26350 , the loss is currently 0.9839280247688293\n",
      "iteration:  26360\n",
      "Iteration 26360 , the loss is currently 1.0014290809631348\n",
      "iteration:  26370\n",
      "Iteration 26370 , the loss is currently 1.0208525657653809\n",
      "iteration:  26380\n",
      "Iteration 26380 , the loss is currently 0.9985080361366272\n",
      "iteration:  26390\n",
      "Iteration 26390 , the loss is currently 0.9714158177375793\n",
      "iteration:  26400\n",
      "Iteration 26400 , the loss is currently 0.9906655550003052\n",
      "iteration:  26410\n",
      "Iteration 26410 , the loss is currently 0.9984546303749084\n",
      "iteration:  26420\n",
      "Iteration 26420 , the loss is currently 0.9852789640426636\n",
      "iteration:  26430\n",
      "Iteration 26430 , the loss is currently 1.0087753534317017\n",
      "iteration:  26440\n",
      "Iteration 26440 , the loss is currently 1.0002307891845703\n",
      "iteration:  26450\n",
      "Iteration 26450 , the loss is currently 1.0033133029937744\n",
      "iteration:  26460\n",
      "Iteration 26460 , the loss is currently 0.9822187423706055\n",
      "iteration:  26470\n",
      "Iteration 26470 , the loss is currently 0.9903439283370972\n",
      "iteration:  26480\n",
      "Iteration 26480 , the loss is currently 0.9870027899742126\n",
      "iteration:  26490\n",
      "Iteration 26490 , the loss is currently 0.970784604549408\n",
      "iteration:  26500\n",
      "Iteration 26500 , the loss is currently 0.9640390872955322\n",
      "iteration:  26510\n",
      "Iteration 26510 , the loss is currently 1.0008227825164795\n",
      "iteration:  26520\n",
      "Iteration 26520 , the loss is currently 0.993489682674408\n",
      "iteration:  26530\n",
      "Iteration 26530 , the loss is currently 0.9799368381500244\n",
      "iteration:  26540\n",
      "Iteration 26540 , the loss is currently 0.9965351223945618\n",
      "iteration:  26550\n",
      "Iteration 26550 , the loss is currently 0.9908549785614014\n",
      "iteration:  26560\n",
      "Iteration 26560 , the loss is currently 1.000352144241333\n",
      "iteration:  26570\n",
      "Iteration 26570 , the loss is currently 0.9958260655403137\n",
      "iteration:  26580\n",
      "Iteration 26580 , the loss is currently 1.0003355741500854\n",
      "iteration:  26590\n",
      "Iteration 26590 , the loss is currently 0.990753173828125\n",
      "iteration:  26600\n",
      "Iteration 26600 , the loss is currently 0.9950466156005859\n",
      "iteration:  26610\n",
      "Iteration 26610 , the loss is currently 0.984738290309906\n",
      "iteration:  26620\n",
      "Iteration 26620 , the loss is currently 0.9846440553665161\n",
      "iteration:  26630\n",
      "Iteration 26630 , the loss is currently 1.013144612312317\n",
      "iteration:  26640\n",
      "Iteration 26640 , the loss is currently 0.974720299243927\n",
      "iteration:  26650\n",
      "Iteration 26650 , the loss is currently 0.9739243984222412\n",
      "iteration:  26660\n",
      "Iteration 26660 , the loss is currently 0.9865342974662781\n",
      "iteration:  26670\n",
      "Iteration 26670 , the loss is currently 0.9889764189720154\n",
      "iteration:  26680\n",
      "Iteration 26680 , the loss is currently 1.0043498277664185\n",
      "iteration:  26690\n",
      "Iteration 26690 , the loss is currently 0.9910408854484558\n",
      "iteration:  26700\n",
      "Iteration 26700 , the loss is currently 1.0132098197937012\n",
      "iteration:  26710\n",
      "Iteration 26710 , the loss is currently 0.9937373399734497\n",
      "iteration:  26720\n",
      "Iteration 26720 , the loss is currently 0.9882556200027466\n",
      "iteration:  26730\n",
      "Iteration 26730 , the loss is currently 0.9922642707824707\n",
      "iteration:  26740\n",
      "Iteration 26740 , the loss is currently 1.000056266784668\n",
      "iteration:  26750\n",
      "Iteration 26750 , the loss is currently 0.9764803051948547\n",
      "iteration:  26760\n",
      "Iteration 26760 , the loss is currently 0.9931900501251221\n",
      "iteration:  26770\n",
      "Iteration 26770 , the loss is currently 1.0201970338821411\n",
      "iteration:  26780\n",
      "Iteration 26780 , the loss is currently 1.0182126760482788\n",
      "iteration:  26790\n",
      "Iteration 26790 , the loss is currently 0.9800369739532471\n",
      "iteration:  26800\n",
      "Iteration 26800 , the loss is currently 0.9982743859291077\n",
      "iteration:  26810\n",
      "Iteration 26810 , the loss is currently 0.994871973991394\n",
      "iteration:  26820\n",
      "Iteration 26820 , the loss is currently 0.9604537487030029\n",
      "iteration:  26830\n",
      "Iteration 26830 , the loss is currently 0.990545392036438\n",
      "iteration:  26840\n",
      "Iteration 26840 , the loss is currently 0.9825941324234009\n",
      "iteration:  26850\n",
      "Iteration 26850 , the loss is currently 1.0121312141418457\n",
      "iteration:  26860\n",
      "Iteration 26860 , the loss is currently 0.9815378189086914\n",
      "iteration:  26870\n",
      "Iteration 26870 , the loss is currently 0.9970906376838684\n",
      "iteration:  26880\n",
      "Iteration 26880 , the loss is currently 0.989342987537384\n",
      "iteration:  26890\n",
      "Iteration 26890 , the loss is currently 0.9844905138015747\n",
      "iteration:  26900\n",
      "Iteration 26900 , the loss is currently 0.9833343029022217\n",
      "iteration:  26910\n",
      "Iteration 26910 , the loss is currently 0.965175986289978\n",
      "iteration:  26920\n",
      "Iteration 26920 , the loss is currently 0.9893664121627808\n",
      "iteration:  26930\n",
      "Iteration 26930 , the loss is currently 0.9811429977416992\n",
      "iteration:  26940\n",
      "Iteration 26940 , the loss is currently 0.9801201224327087\n",
      "iteration:  26950\n",
      "Iteration 26950 , the loss is currently 0.9951502084732056\n",
      "iteration:  26960\n",
      "Iteration 26960 , the loss is currently 0.9678940773010254\n",
      "iteration:  26970\n",
      "Iteration 26970 , the loss is currently 1.0165281295776367\n",
      "iteration:  26980\n",
      "Iteration 26980 , the loss is currently 0.9757063984870911\n",
      "iteration:  26990\n",
      "Iteration 26990 , the loss is currently 0.9743326902389526\n",
      "iteration:  27000\n",
      "Iteration 27000 , the loss is currently 0.994622528553009\n",
      "iteration:  27010\n",
      "Iteration 27010 , the loss is currently 0.9973170757293701\n",
      "iteration:  27020\n",
      "Iteration 27020 , the loss is currently 1.0031418800354004\n",
      "iteration:  27030\n",
      "Iteration 27030 , the loss is currently 0.9938597679138184\n",
      "iteration:  27040\n",
      "Iteration 27040 , the loss is currently 0.9850839972496033\n",
      "iteration:  27050\n",
      "Iteration 27050 , the loss is currently 0.9756965041160583\n",
      "iteration:  27060\n",
      "Iteration 27060 , the loss is currently 0.9911723732948303\n",
      "iteration:  27070\n",
      "Iteration 27070 , the loss is currently 0.9823717474937439\n",
      "iteration:  27080\n",
      "Iteration 27080 , the loss is currently 0.9803978204727173\n",
      "iteration:  27090\n",
      "Iteration 27090 , the loss is currently 0.9745299220085144\n",
      "iteration:  27100\n",
      "Iteration 27100 , the loss is currently 0.9974790215492249\n",
      "iteration:  27110\n",
      "Iteration 27110 , the loss is currently 1.0139822959899902\n",
      "iteration:  27120\n",
      "Iteration 27120 , the loss is currently 0.9932358264923096\n",
      "iteration:  27130\n",
      "Iteration 27130 , the loss is currently 0.9824334383010864\n",
      "iteration:  27140\n",
      "Iteration 27140 , the loss is currently 0.9802118539810181\n",
      "iteration:  27150\n",
      "Iteration 27150 , the loss is currently 1.0219106674194336\n",
      "iteration:  27160\n",
      "Iteration 27160 , the loss is currently 0.9917908310890198\n",
      "iteration:  27170\n",
      "Iteration 27170 , the loss is currently 0.9749500155448914\n",
      "iteration:  27180\n",
      "Iteration 27180 , the loss is currently 0.9703990817070007\n",
      "iteration:  27190\n",
      "Iteration 27190 , the loss is currently 0.9757887721061707\n",
      "iteration:  27200\n",
      "Iteration 27200 , the loss is currently 1.0082552433013916\n",
      "iteration:  27210\n",
      "Iteration 27210 , the loss is currently 0.9910245537757874\n",
      "iteration:  27220\n",
      "Iteration 27220 , the loss is currently 0.9897838234901428\n",
      "iteration:  27230\n",
      "Iteration 27230 , the loss is currently 0.9724223017692566\n",
      "iteration:  27240\n",
      "Iteration 27240 , the loss is currently 0.9788442254066467\n",
      "iteration:  27250\n",
      "Iteration 27250 , the loss is currently 0.995624303817749\n",
      "iteration:  27260\n",
      "Iteration 27260 , the loss is currently 1.0048893690109253\n",
      "iteration:  27270\n",
      "Iteration 27270 , the loss is currently 0.9790335297584534\n",
      "iteration:  27280\n",
      "Iteration 27280 , the loss is currently 1.0048413276672363\n",
      "iteration:  27290\n",
      "Iteration 27290 , the loss is currently 1.0045629739761353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  27300\n",
      "Iteration 27300 , the loss is currently 0.9924706220626831\n",
      "iteration:  27310\n",
      "Iteration 27310 , the loss is currently 0.9809175133705139\n",
      "iteration:  27320\n",
      "Iteration 27320 , the loss is currently 0.9852770566940308\n",
      "iteration:  27330\n",
      "Iteration 27330 , the loss is currently 0.997734546661377\n",
      "iteration:  27340\n",
      "Iteration 27340 , the loss is currently 0.993989109992981\n",
      "iteration:  27350\n",
      "Iteration 27350 , the loss is currently 0.9727087616920471\n",
      "iteration:  27360\n",
      "Iteration 27360 , the loss is currently 0.9816675186157227\n",
      "iteration:  27370\n",
      "Iteration 27370 , the loss is currently 0.9995225667953491\n",
      "iteration:  27380\n",
      "Iteration 27380 , the loss is currently 0.9930263161659241\n",
      "iteration:  27390\n",
      "Iteration 27390 , the loss is currently 0.9899560809135437\n",
      "iteration:  27400\n",
      "Iteration 27400 , the loss is currently 0.9927341938018799\n",
      "iteration:  27410\n",
      "Iteration 27410 , the loss is currently 0.987888753414154\n",
      "iteration:  27420\n",
      "Iteration 27420 , the loss is currently 0.9980634450912476\n",
      "iteration:  27430\n",
      "Iteration 27430 , the loss is currently 0.9994891881942749\n",
      "iteration:  27440\n",
      "Iteration 27440 , the loss is currently 0.977971076965332\n",
      "iteration:  27450\n",
      "Iteration 27450 , the loss is currently 1.0097403526306152\n",
      "iteration:  27460\n",
      "Iteration 27460 , the loss is currently 0.9908074140548706\n",
      "iteration:  27470\n",
      "Iteration 27470 , the loss is currently 0.9947797656059265\n",
      "iteration:  27480\n",
      "Iteration 27480 , the loss is currently 1.0080173015594482\n",
      "iteration:  27490\n",
      "Iteration 27490 , the loss is currently 0.9766198396682739\n",
      "iteration:  27500\n",
      "Iteration 27500 , the loss is currently 0.9990647435188293\n",
      "iteration:  27510\n",
      "Iteration 27510 , the loss is currently 0.9833540320396423\n",
      "iteration:  27520\n",
      "Iteration 27520 , the loss is currently 0.9845533967018127\n",
      "iteration:  27530\n",
      "Iteration 27530 , the loss is currently 0.9839003682136536\n",
      "iteration:  27540\n",
      "Iteration 27540 , the loss is currently 1.0060226917266846\n",
      "iteration:  27550\n",
      "Iteration 27550 , the loss is currently 0.9994930028915405\n",
      "iteration:  27560\n",
      "Iteration 27560 , the loss is currently 0.978600025177002\n",
      "iteration:  27570\n",
      "Iteration 27570 , the loss is currently 0.9931080341339111\n",
      "iteration:  27580\n",
      "Iteration 27580 , the loss is currently 0.994937539100647\n",
      "iteration:  27590\n",
      "Iteration 27590 , the loss is currently 0.9910589456558228\n",
      "iteration:  27600\n",
      "Iteration 27600 , the loss is currently 1.0004023313522339\n",
      "iteration:  27610\n",
      "Iteration 27610 , the loss is currently 0.9788674116134644\n",
      "iteration:  27620\n",
      "Iteration 27620 , the loss is currently 0.9932916164398193\n",
      "iteration:  27630\n",
      "Iteration 27630 , the loss is currently 1.005252480506897\n",
      "iteration:  27640\n",
      "Iteration 27640 , the loss is currently 0.989170253276825\n",
      "iteration:  27650\n",
      "Iteration 27650 , the loss is currently 0.9749916791915894\n",
      "iteration:  27660\n",
      "Iteration 27660 , the loss is currently 0.985438883304596\n",
      "iteration:  27670\n",
      "Iteration 27670 , the loss is currently 0.9786905646324158\n",
      "iteration:  27680\n",
      "Iteration 27680 , the loss is currently 0.9992172718048096\n",
      "iteration:  27690\n",
      "Iteration 27690 , the loss is currently 0.9871158003807068\n",
      "iteration:  27700\n",
      "Iteration 27700 , the loss is currently 0.9981022477149963\n",
      "iteration:  27710\n",
      "Iteration 27710 , the loss is currently 0.9914319515228271\n",
      "iteration:  27720\n",
      "Iteration 27720 , the loss is currently 1.0064988136291504\n",
      "iteration:  27730\n",
      "Iteration 27730 , the loss is currently 0.9869423508644104\n",
      "iteration:  27740\n",
      "Iteration 27740 , the loss is currently 0.9974074959754944\n",
      "iteration:  27750\n",
      "Iteration 27750 , the loss is currently 1.003445029258728\n",
      "iteration:  27760\n",
      "Iteration 27760 , the loss is currently 0.9952653050422668\n",
      "iteration:  27770\n",
      "Iteration 27770 , the loss is currently 0.989593505859375\n",
      "iteration:  27780\n",
      "Iteration 27780 , the loss is currently 0.9940246343612671\n",
      "iteration:  27790\n",
      "Iteration 27790 , the loss is currently 0.9768945574760437\n",
      "iteration:  27800\n",
      "Iteration 27800 , the loss is currently 0.9946686625480652\n",
      "iteration:  27810\n",
      "Iteration 27810 , the loss is currently 0.9812135100364685\n",
      "iteration:  27820\n",
      "Iteration 27820 , the loss is currently 0.9718763828277588\n",
      "iteration:  27830\n",
      "Iteration 27830 , the loss is currently 1.004286527633667\n",
      "iteration:  27840\n",
      "Iteration 27840 , the loss is currently 0.9843696355819702\n",
      "iteration:  27850\n",
      "Iteration 27850 , the loss is currently 1.0057204961776733\n",
      "iteration:  27860\n",
      "Iteration 27860 , the loss is currently 0.974017322063446\n",
      "iteration:  27870\n",
      "Iteration 27870 , the loss is currently 0.987448513507843\n",
      "iteration:  27880\n",
      "Iteration 27880 , the loss is currently 0.9965046048164368\n",
      "iteration:  27890\n",
      "Iteration 27890 , the loss is currently 0.9739022254943848\n",
      "iteration:  27900\n",
      "Iteration 27900 , the loss is currently 0.9897830486297607\n",
      "iteration:  27910\n",
      "Iteration 27910 , the loss is currently 0.9946901798248291\n",
      "iteration:  27920\n",
      "Iteration 27920 , the loss is currently 0.9849797487258911\n",
      "iteration:  27930\n",
      "Iteration 27930 , the loss is currently 0.992620050907135\n",
      "iteration:  27940\n",
      "Iteration 27940 , the loss is currently 0.9956570267677307\n",
      "iteration:  27950\n",
      "Iteration 27950 , the loss is currently 1.0117759704589844\n",
      "iteration:  27960\n",
      "Iteration 27960 , the loss is currently 0.9944887757301331\n",
      "iteration:  27970\n",
      "Iteration 27970 , the loss is currently 0.9954941868782043\n",
      "iteration:  27980\n",
      "Iteration 27980 , the loss is currently 1.001871109008789\n",
      "iteration:  27990\n",
      "Iteration 27990 , the loss is currently 1.0003207921981812\n",
      "iteration:  28000\n",
      "Iteration 28000 , the loss is currently 0.9983541965484619\n",
      "iteration:  28010\n",
      "Iteration 28010 , the loss is currently 0.9809821248054504\n",
      "iteration:  28020\n",
      "Iteration 28020 , the loss is currently 0.980839729309082\n",
      "iteration:  28030\n",
      "Iteration 28030 , the loss is currently 1.0065724849700928\n",
      "iteration:  28040\n",
      "Iteration 28040 , the loss is currently 0.9904876947402954\n",
      "iteration:  28050\n",
      "Iteration 28050 , the loss is currently 1.008696436882019\n",
      "iteration:  28060\n",
      "Iteration 28060 , the loss is currently 0.9900491833686829\n",
      "iteration:  28070\n",
      "Iteration 28070 , the loss is currently 0.9773310422897339\n",
      "iteration:  28080\n",
      "Iteration 28080 , the loss is currently 0.9682983160018921\n",
      "iteration:  28090\n",
      "Iteration 28090 , the loss is currently 0.9831535816192627\n",
      "iteration:  28100\n",
      "Iteration 28100 , the loss is currently 0.9935128092765808\n",
      "iteration:  28110\n",
      "Iteration 28110 , the loss is currently 0.9879161715507507\n",
      "iteration:  28120\n",
      "Iteration 28120 , the loss is currently 0.988727331161499\n",
      "iteration:  28130\n",
      "Iteration 28130 , the loss is currently 0.9914745688438416\n",
      "iteration:  28140\n",
      "Iteration 28140 , the loss is currently 0.985288679599762\n",
      "iteration:  28150\n",
      "Iteration 28150 , the loss is currently 0.9964481592178345\n",
      "iteration:  28160\n",
      "Iteration 28160 , the loss is currently 1.0034711360931396\n",
      "iteration:  28170\n",
      "Iteration 28170 , the loss is currently 1.00865638256073\n",
      "iteration:  28180\n",
      "Iteration 28180 , the loss is currently 1.0069416761398315\n",
      "iteration:  28190\n",
      "Iteration 28190 , the loss is currently 0.9793717861175537\n",
      "iteration:  28200\n",
      "Iteration 28200 , the loss is currently 0.998417854309082\n",
      "iteration:  28210\n",
      "Iteration 28210 , the loss is currently 1.0001580715179443\n",
      "iteration:  28220\n",
      "Iteration 28220 , the loss is currently 0.9954252243041992\n",
      "iteration:  28230\n",
      "Iteration 28230 , the loss is currently 1.0103967189788818\n",
      "iteration:  28240\n",
      "Iteration 28240 , the loss is currently 0.991632342338562\n",
      "iteration:  28250\n",
      "Iteration 28250 , the loss is currently 0.9965011477470398\n",
      "iteration:  28260\n",
      "Iteration 28260 , the loss is currently 1.00191068649292\n",
      "iteration:  28270\n",
      "Iteration 28270 , the loss is currently 0.9956951141357422\n",
      "iteration:  28280\n",
      "Iteration 28280 , the loss is currently 0.9993222951889038\n",
      "iteration:  28290\n",
      "Iteration 28290 , the loss is currently 0.9935714602470398\n",
      "iteration:  28300\n",
      "Iteration 28300 , the loss is currently 0.9856056571006775\n",
      "iteration:  28310\n",
      "Iteration 28310 , the loss is currently 1.0006879568099976\n",
      "iteration:  28320\n",
      "Iteration 28320 , the loss is currently 0.9959233999252319\n",
      "iteration:  28330\n",
      "Iteration 28330 , the loss is currently 0.9939996004104614\n",
      "iteration:  28340\n",
      "Iteration 28340 , the loss is currently 0.9980306625366211\n",
      "iteration:  28350\n",
      "Iteration 28350 , the loss is currently 0.9842812418937683\n",
      "iteration:  28360\n",
      "Iteration 28360 , the loss is currently 0.9847171306610107\n",
      "iteration:  28370\n",
      "Iteration 28370 , the loss is currently 0.9786350131034851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  28380\n",
      "Iteration 28380 , the loss is currently 0.9775968790054321\n",
      "iteration:  28390\n",
      "Iteration 28390 , the loss is currently 0.9797887206077576\n",
      "iteration:  28400\n",
      "Iteration 28400 , the loss is currently 1.012380599975586\n",
      "iteration:  28410\n",
      "Iteration 28410 , the loss is currently 0.9885318279266357\n",
      "iteration:  28420\n",
      "Iteration 28420 , the loss is currently 1.0035251379013062\n",
      "iteration:  28430\n",
      "Iteration 28430 , the loss is currently 0.9961012005805969\n",
      "iteration:  28440\n",
      "Iteration 28440 , the loss is currently 0.9831315875053406\n",
      "iteration:  28450\n",
      "Iteration 28450 , the loss is currently 1.0010151863098145\n",
      "iteration:  28460\n",
      "Iteration 28460 , the loss is currently 0.9960935115814209\n",
      "iteration:  28470\n",
      "Iteration 28470 , the loss is currently 0.9845823645591736\n",
      "iteration:  28480\n",
      "Iteration 28480 , the loss is currently 1.0015654563903809\n",
      "iteration:  28490\n",
      "Iteration 28490 , the loss is currently 0.9703657627105713\n",
      "iteration:  28500\n",
      "Iteration 28500 , the loss is currently 0.9892054796218872\n",
      "iteration:  28510\n",
      "Iteration 28510 , the loss is currently 1.006913423538208\n",
      "iteration:  28520\n",
      "Iteration 28520 , the loss is currently 1.0240612030029297\n",
      "iteration:  28530\n",
      "Iteration 28530 , the loss is currently 0.9957118630409241\n",
      "iteration:  28540\n",
      "Iteration 28540 , the loss is currently 0.9879084825515747\n",
      "iteration:  28550\n",
      "Iteration 28550 , the loss is currently 0.9905261397361755\n",
      "iteration:  28560\n",
      "Iteration 28560 , the loss is currently 0.9982255101203918\n",
      "iteration:  28570\n",
      "Iteration 28570 , the loss is currently 0.9826673269271851\n",
      "iteration:  28580\n",
      "Iteration 28580 , the loss is currently 0.9963282346725464\n",
      "iteration:  28590\n",
      "Iteration 28590 , the loss is currently 0.985771656036377\n",
      "iteration:  28600\n",
      "Iteration 28600 , the loss is currently 0.9716504812240601\n",
      "iteration:  28610\n",
      "Iteration 28610 , the loss is currently 0.9917270541191101\n",
      "iteration:  28620\n",
      "Iteration 28620 , the loss is currently 0.9998035430908203\n",
      "iteration:  28630\n",
      "Iteration 28630 , the loss is currently 0.9845371246337891\n",
      "iteration:  28640\n",
      "Iteration 28640 , the loss is currently 0.9822309017181396\n",
      "iteration:  28650\n",
      "Iteration 28650 , the loss is currently 0.9973323941230774\n",
      "iteration:  28660\n",
      "Iteration 28660 , the loss is currently 1.019300103187561\n",
      "iteration:  28670\n",
      "Iteration 28670 , the loss is currently 0.9647035598754883\n",
      "iteration:  28680\n",
      "Iteration 28680 , the loss is currently 0.9869134426116943\n",
      "iteration:  28690\n",
      "Iteration 28690 , the loss is currently 0.9870578646659851\n",
      "iteration:  28700\n",
      "Iteration 28700 , the loss is currently 0.9894411563873291\n",
      "iteration:  28710\n",
      "Iteration 28710 , the loss is currently 1.01007080078125\n",
      "iteration:  28720\n",
      "Iteration 28720 , the loss is currently 0.9819765090942383\n",
      "iteration:  28730\n",
      "Iteration 28730 , the loss is currently 0.992522656917572\n",
      "iteration:  28740\n",
      "Iteration 28740 , the loss is currently 1.0061453580856323\n",
      "iteration:  28750\n",
      "Iteration 28750 , the loss is currently 0.9875220060348511\n",
      "iteration:  28760\n",
      "Iteration 28760 , the loss is currently 1.007350206375122\n",
      "iteration:  28770\n",
      "Iteration 28770 , the loss is currently 0.99362713098526\n",
      "iteration:  28780\n",
      "Iteration 28780 , the loss is currently 0.9687908887863159\n",
      "iteration:  28790\n",
      "Iteration 28790 , the loss is currently 0.9817773103713989\n",
      "iteration:  28800\n",
      "Iteration 28800 , the loss is currently 0.9932255744934082\n",
      "iteration:  28810\n",
      "Iteration 28810 , the loss is currently 0.9940102100372314\n",
      "iteration:  28820\n",
      "Iteration 28820 , the loss is currently 0.9905251860618591\n",
      "iteration:  28830\n",
      "Iteration 28830 , the loss is currently 0.9724152684211731\n",
      "iteration:  28840\n",
      "Iteration 28840 , the loss is currently 0.9695026278495789\n",
      "iteration:  28850\n",
      "Iteration 28850 , the loss is currently 0.9781072735786438\n",
      "iteration:  28860\n",
      "Iteration 28860 , the loss is currently 0.9824067950248718\n",
      "iteration:  28870\n",
      "Iteration 28870 , the loss is currently 0.9775003790855408\n",
      "iteration:  28880\n",
      "Iteration 28880 , the loss is currently 1.0004726648330688\n",
      "iteration:  28890\n",
      "Iteration 28890 , the loss is currently 1.007616639137268\n",
      "iteration:  28900\n",
      "Iteration 28900 , the loss is currently 1.0015636682510376\n",
      "iteration:  28910\n",
      "Iteration 28910 , the loss is currently 1.0055369138717651\n",
      "iteration:  28920\n",
      "Iteration 28920 , the loss is currently 0.9940212368965149\n",
      "iteration:  28930\n",
      "Iteration 28930 , the loss is currently 0.9903409481048584\n",
      "iteration:  28940\n",
      "Iteration 28940 , the loss is currently 0.9794469475746155\n",
      "iteration:  28950\n",
      "Iteration 28950 , the loss is currently 1.0026062726974487\n",
      "iteration:  28960\n",
      "Iteration 28960 , the loss is currently 1.0010488033294678\n",
      "iteration:  28970\n",
      "Iteration 28970 , the loss is currently 0.989679753780365\n",
      "iteration:  28980\n",
      "Iteration 28980 , the loss is currently 0.9922842979431152\n",
      "iteration:  28990\n",
      "Iteration 28990 , the loss is currently 1.0120491981506348\n",
      "iteration:  29000\n",
      "Iteration 29000 , the loss is currently 0.9878252744674683\n",
      "iteration:  29010\n",
      "Iteration 29010 , the loss is currently 0.9623448848724365\n",
      "iteration:  29020\n",
      "Iteration 29020 , the loss is currently 0.9880474805831909\n",
      "iteration:  29030\n",
      "Iteration 29030 , the loss is currently 1.0200307369232178\n",
      "iteration:  29040\n",
      "Iteration 29040 , the loss is currently 0.9930956363677979\n",
      "iteration:  29050\n",
      "Iteration 29050 , the loss is currently 0.9994825124740601\n",
      "iteration:  29060\n",
      "Iteration 29060 , the loss is currently 0.9767310619354248\n",
      "iteration:  29070\n",
      "Iteration 29070 , the loss is currently 1.0041017532348633\n",
      "iteration:  29080\n",
      "Iteration 29080 , the loss is currently 0.990687370300293\n",
      "iteration:  29090\n",
      "Iteration 29090 , the loss is currently 0.9934914112091064\n",
      "iteration:  29100\n",
      "Iteration 29100 , the loss is currently 1.0024725198745728\n",
      "iteration:  29110\n",
      "Iteration 29110 , the loss is currently 1.0020506381988525\n",
      "iteration:  29120\n",
      "Iteration 29120 , the loss is currently 0.9786748290061951\n",
      "iteration:  29130\n",
      "Iteration 29130 , the loss is currently 1.007521629333496\n",
      "iteration:  29140\n",
      "Iteration 29140 , the loss is currently 0.9866480231285095\n",
      "iteration:  29150\n",
      "Iteration 29150 , the loss is currently 0.9933333992958069\n",
      "iteration:  29160\n",
      "Iteration 29160 , the loss is currently 0.9784024953842163\n",
      "iteration:  29170\n",
      "Iteration 29170 , the loss is currently 0.9823068380355835\n",
      "iteration:  29180\n",
      "Iteration 29180 , the loss is currently 0.982804000377655\n",
      "iteration:  29190\n",
      "Iteration 29190 , the loss is currently 0.98832768201828\n",
      "iteration:  29200\n",
      "Iteration 29200 , the loss is currently 1.0226466655731201\n",
      "iteration:  29210\n",
      "Iteration 29210 , the loss is currently 0.990702211856842\n",
      "iteration:  29220\n",
      "Iteration 29220 , the loss is currently 0.9806808233261108\n",
      "iteration:  29230\n",
      "Iteration 29230 , the loss is currently 0.9829458594322205\n",
      "iteration:  29240\n",
      "Iteration 29240 , the loss is currently 0.9781224727630615\n",
      "iteration:  29250\n",
      "Iteration 29250 , the loss is currently 0.9864702820777893\n",
      "iteration:  29260\n",
      "Iteration 29260 , the loss is currently 1.0055598020553589\n",
      "iteration:  29270\n",
      "Iteration 29270 , the loss is currently 0.982036828994751\n",
      "iteration:  29280\n",
      "Iteration 29280 , the loss is currently 0.9894995093345642\n",
      "iteration:  29290\n",
      "Iteration 29290 , the loss is currently 0.9846839904785156\n",
      "iteration:  29300\n",
      "Iteration 29300 , the loss is currently 0.9724025130271912\n",
      "iteration:  29310\n",
      "Iteration 29310 , the loss is currently 0.9807284474372864\n",
      "iteration:  29320\n",
      "Iteration 29320 , the loss is currently 1.0044608116149902\n",
      "iteration:  29330\n",
      "Iteration 29330 , the loss is currently 0.9926004409790039\n",
      "iteration:  29340\n",
      "Iteration 29340 , the loss is currently 1.0037193298339844\n",
      "iteration:  29350\n",
      "Iteration 29350 , the loss is currently 1.0068234205245972\n",
      "iteration:  29360\n",
      "Iteration 29360 , the loss is currently 0.9976001381874084\n",
      "iteration:  29370\n",
      "Iteration 29370 , the loss is currently 0.9977255463600159\n",
      "iteration:  29380\n",
      "Iteration 29380 , the loss is currently 0.9824240207672119\n",
      "iteration:  29390\n",
      "Iteration 29390 , the loss is currently 0.9703314304351807\n",
      "iteration:  29400\n",
      "Iteration 29400 , the loss is currently 0.9934501051902771\n",
      "iteration:  29410\n",
      "Iteration 29410 , the loss is currently 0.9936513900756836\n",
      "iteration:  29420\n",
      "Iteration 29420 , the loss is currently 1.0077528953552246\n",
      "iteration:  29430\n",
      "Iteration 29430 , the loss is currently 0.9829646944999695\n",
      "iteration:  29440\n",
      "Iteration 29440 , the loss is currently 0.9806864857673645\n",
      "iteration:  29450\n",
      "Iteration 29450 , the loss is currently 0.9920416474342346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  29460\n",
      "Iteration 29460 , the loss is currently 0.9806605577468872\n",
      "iteration:  29470\n",
      "Iteration 29470 , the loss is currently 0.9830146431922913\n",
      "iteration:  29480\n",
      "Iteration 29480 , the loss is currently 0.9805755615234375\n",
      "iteration:  29490\n",
      "Iteration 29490 , the loss is currently 0.9895049333572388\n",
      "iteration:  29500\n",
      "Iteration 29500 , the loss is currently 0.9602532386779785\n",
      "iteration:  29510\n",
      "Iteration 29510 , the loss is currently 1.019428014755249\n",
      "iteration:  29520\n",
      "Iteration 29520 , the loss is currently 0.9868649840354919\n",
      "iteration:  29530\n",
      "Iteration 29530 , the loss is currently 0.9815470576286316\n",
      "iteration:  29540\n",
      "Iteration 29540 , the loss is currently 1.0023115873336792\n",
      "iteration:  29550\n",
      "Iteration 29550 , the loss is currently 1.023728370666504\n",
      "iteration:  29560\n",
      "Iteration 29560 , the loss is currently 1.004820704460144\n",
      "iteration:  29570\n",
      "Iteration 29570 , the loss is currently 1.0099045038223267\n",
      "iteration:  29580\n",
      "Iteration 29580 , the loss is currently 1.0027841329574585\n",
      "iteration:  29590\n",
      "Iteration 29590 , the loss is currently 0.9990522265434265\n",
      "iteration:  29600\n",
      "Iteration 29600 , the loss is currently 0.9949324727058411\n",
      "iteration:  29610\n",
      "Iteration 29610 , the loss is currently 0.9932939410209656\n",
      "iteration:  29620\n",
      "Iteration 29620 , the loss is currently 1.0054113864898682\n",
      "iteration:  29630\n",
      "Iteration 29630 , the loss is currently 0.9877603054046631\n",
      "iteration:  29640\n",
      "Iteration 29640 , the loss is currently 0.9748441576957703\n",
      "iteration:  29650\n",
      "Iteration 29650 , the loss is currently 0.9962784051895142\n",
      "iteration:  29660\n",
      "Iteration 29660 , the loss is currently 0.9830365777015686\n",
      "iteration:  29670\n",
      "Iteration 29670 , the loss is currently 0.982424259185791\n",
      "iteration:  29680\n",
      "Iteration 29680 , the loss is currently 0.9990454912185669\n",
      "iteration:  29690\n",
      "Iteration 29690 , the loss is currently 0.9801041483879089\n",
      "iteration:  29700\n",
      "Iteration 29700 , the loss is currently 0.9799085855484009\n",
      "iteration:  29710\n",
      "Iteration 29710 , the loss is currently 1.0010513067245483\n",
      "iteration:  29720\n",
      "Iteration 29720 , the loss is currently 0.9915504455566406\n",
      "iteration:  29730\n",
      "Iteration 29730 , the loss is currently 0.9926079511642456\n",
      "iteration:  29740\n",
      "Iteration 29740 , the loss is currently 0.9847086668014526\n",
      "iteration:  29750\n",
      "Iteration 29750 , the loss is currently 1.0168837308883667\n",
      "iteration:  29760\n",
      "Iteration 29760 , the loss is currently 0.9825092554092407\n",
      "iteration:  29770\n",
      "Iteration 29770 , the loss is currently 0.9998213648796082\n",
      "iteration:  29780\n",
      "Iteration 29780 , the loss is currently 0.999443769454956\n",
      "iteration:  29790\n",
      "Iteration 29790 , the loss is currently 1.0136646032333374\n",
      "iteration:  29800\n",
      "Iteration 29800 , the loss is currently 0.9973426461219788\n",
      "iteration:  29810\n",
      "Iteration 29810 , the loss is currently 1.0029819011688232\n",
      "iteration:  29820\n",
      "Iteration 29820 , the loss is currently 0.9977371096611023\n",
      "iteration:  29830\n",
      "Iteration 29830 , the loss is currently 1.0062410831451416\n",
      "iteration:  29840\n",
      "Iteration 29840 , the loss is currently 0.9812566637992859\n",
      "iteration:  29850\n",
      "Iteration 29850 , the loss is currently 0.9996985793113708\n",
      "iteration:  29860\n",
      "Iteration 29860 , the loss is currently 0.9876667857170105\n",
      "iteration:  29870\n",
      "Iteration 29870 , the loss is currently 1.00242018699646\n",
      "iteration:  29880\n",
      "Iteration 29880 , the loss is currently 0.9901005625724792\n",
      "iteration:  29890\n",
      "Iteration 29890 , the loss is currently 1.0028009414672852\n",
      "iteration:  29900\n",
      "Iteration 29900 , the loss is currently 0.9882733225822449\n",
      "iteration:  29910\n",
      "Iteration 29910 , the loss is currently 0.9782273769378662\n",
      "iteration:  29920\n",
      "Iteration 29920 , the loss is currently 0.9960479140281677\n",
      "iteration:  29930\n",
      "Iteration 29930 , the loss is currently 0.9820985794067383\n",
      "iteration:  29940\n",
      "Iteration 29940 , the loss is currently 0.9863348007202148\n",
      "iteration:  29950\n",
      "Iteration 29950 , the loss is currently 0.990029513835907\n",
      "iteration:  29960\n",
      "Iteration 29960 , the loss is currently 1.0002824068069458\n",
      "iteration:  29970\n",
      "Iteration 29970 , the loss is currently 0.9876822233200073\n",
      "iteration:  29980\n",
      "Iteration 29980 , the loss is currently 0.9991718530654907\n",
      "iteration:  29990\n",
      "Iteration 29990 , the loss is currently 0.981533944606781\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "train_step(model, data, repeat= 300)\n",
    "print('done training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(state):\n",
    "    for x in range(16):\n",
    "        if x % 4 == 0 and not x == 0:\n",
    "            print()\n",
    "        print(state[x], end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = env2048()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = e.get_state(normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-14e60e270811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'visualize' is not defined"
     ]
    }
   ],
   "source": [
    "s = memo[3000][0]\n",
    "visualize(s.reshape(16,))\n",
    "print()\n",
    "print(memo[3000][1])\n",
    "print(model(s))\n",
    "get_action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('modelWeights150E', np.array(model.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
